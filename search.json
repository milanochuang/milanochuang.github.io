[{"title":"【NLP】Day 7: 現出你的原形！tokenization、lemmatization、stemming ","url":"/2023/01/08/Untitled/","content":"<blockquote>\n<p>「隱藏著黑暗力量的鑰匙啊，在我面前顯示真正的力量！ 跟你訂下約定的小櫻命令你，封印解除！」<br>《庫洛魔法使》木之本櫻</p>\n</blockquote>\n<p>昨天我們學到了中文斷詞的方法，還有一些需要釐清的觀念，以及我個人的一些想法。中文學完了，那英文呢？英文的斷詞方法跟中文有什麼不同的地方嗎？我們今天就一起來瞧瞧世界通用的語言之一，英文，在自然語言處理中是怎麼進行的？</p>\n<p>昨天我們提到了中文這個語言其中的一個特性是，所有字都是黏在一起的，而相對於英文來說，由於英文中間都會以空格分開，所以在這個問題上就不用像中文一樣考量比較多一點，其實這也告訴我們不同語言在自然語言處理上都會有不同的處理方式，沒有單一的最好處理方式，只有最適合的方法。</p>\n<p>英文斷詞叫做<strong>tokenization</strong>，雖然說英文的斷詞並不像中文一樣全部都黏在一起，但是英文也有英文的難處。這是因爲英文的詞尾常常出現變化，比較兩種語言起來，中文卻可以說是完全沒有字尾變化。而英文這種詞尾變化在語言學裡面則稱呼為<strong>inflection</strong>，而如何處理inflection，就是英文的自然語言處理上一個常被討論的議題。</p>\n<p><img src=\"https://miro.medium.com/max/1400/1*VLjKk9NpfDBRgk6FwhfCJA.jpeg\"><br>Resource: <a href=\"https://medium.com/data-science-in-your-pocket/tokenization-algorithms-in-natural-language-processing-nlp-1fceab8454af\">https://medium.com/data-science-in-your-pocket/tokenization-algorithms-in-natural-language-processing-nlp-1fceab8454af</a></p>\n<p>我們就先來看看tokenization的做法吧！以下這段是由海賊王的英文版維基百科中所擷取下來的一段文字。我們要用的是自然語言處理很常用的函式庫，叫做nltk（Natural Language Toolkit），裡面提供了特別多的函式幫助你可以更好地進行自然語言處理。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">inputSTR = <span class=\"string\">&quot;The story follows the adventures of Monkey D Luffy , a boy whose body gained the properties of rubber after unintentionally eating a Devil Fruit. With his pirate crew, the Straw Hat Pirates , Luffy explores the Grand Line in search of the deceased King of the Pirates Gol D Roger&#x27;s ultimate treasure known as the One Piece in order to become the next King of the Pirates.&quot;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> nltk</span><br><span class=\"line\"><span class=\"keyword\">from</span> nltk.tokenize <span class=\"keyword\">import</span> (word_tokenize, MWETokenizer)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(word_tokenize(inputSTR))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">[&#x27;The&#x27;, &#x27;story&#x27;, &#x27;follows&#x27;, &#x27;the&#x27;, &#x27;adventures&#x27;, &#x27;of&#x27;, &#x27;Monkey&#x27;, &#x27;D&#x27;, &#x27;Luffy&#x27;, &#x27;,&#x27;, &#x27;a&#x27;, &#x27;boy&#x27;, &#x27;whose&#x27;, &#x27;body&#x27;, &#x27;gained&#x27;, &#x27;the&#x27;, &#x27;properties&#x27;, &#x27;of&#x27;, &#x27;rubber&#x27;, &#x27;after&#x27;, &#x27;unintentionally&#x27;, &#x27;eating&#x27;, &#x27;a&#x27;, &#x27;Devil&#x27;, &#x27;Fruit&#x27;, &#x27;.&#x27;, &#x27;With&#x27;, &#x27;his&#x27;, &#x27;pirate&#x27;, &#x27;crew&#x27;, &#x27;,&#x27;, &#x27;the&#x27;, &#x27;Straw&#x27;, &#x27;Hat&#x27;, &#x27;Pirates&#x27;, &#x27;,&#x27;, &#x27;Luffy&#x27;, &#x27;explores&#x27;, &#x27;the&#x27;, &#x27;Grand&#x27;, &#x27;Line&#x27;, &#x27;in&#x27;, &#x27;search&#x27;, &#x27;of&#x27;, &#x27;the&#x27;, &#x27;deceased&#x27;, &#x27;King&#x27;, &#x27;of&#x27;, &#x27;the&#x27;, &#x27;Pirates&#x27;, &#x27;Gol&#x27;, &#x27;D&#x27;, &#x27;Roger&#x27;, &quot;&#x27;s&quot;, &#x27;ultimate&#x27;, &#x27;treasure&#x27;, &#x27;known&#x27;, &#x27;as&#x27;, &#x27;the&#x27;, &#x27;One&#x27;, &#x27;Piece&#x27;, &#x27;in&#x27;, &#x27;order&#x27;, &#x27;to&#x27;, &#x27;become&#x27;, &#x27;the&#x27;, &#x27;next&#x27;, &#x27;King&#x27;, &#x27;of&#x27;, &#x27;the&#x27;, &#x27;Pirates&#x27;, &#x27;.&#x27;]</span><br></pre></td></tr></table></figure>\n\n<p>照慣例，我們先來看看輸出結果有什麼令人不滿意的地方。其中我們可以發現，有很多應該是要連在一起的字，由於tokenizer的緣故都被分開了，那其實這樣的結果會影響我們之後的下游任務，一個比較常見且簡單的方法是在這些應該連在一起的字中間加上底線，這樣就會被分成同一個token了。程式碼的部分可以這麼寫：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tokenizer = MWETokenizer([(<span class=\"string\">&#x27;Monkey&#x27;</span>, <span class=\"string\">&#x27;D&#x27;</span>, <span class=\"string\">&#x27;Luffy&#x27;</span>), (<span class=\"string\">&#x27;Devil&#x27;</span>, <span class=\"string\">&#x27;Fruit&#x27;</span>), (<span class=\"string\">&#x27;Straw&#x27;</span>, <span class=\"string\">&#x27;Hat&#x27;</span>, <span class=\"string\">&#x27;Pirates&#x27;</span>), (<span class=\"string\">&#x27;Grand&#x27;</span>, <span class=\"string\">&#x27;Line&#x27;</span>), (<span class=\"string\">&#x27;King&#x27;</span>, <span class=\"string\">&#x27;of&#x27;</span>, <span class=\"string\">&#x27;the&#x27;</span>, <span class=\"string\">&#x27;Pirates&#x27;</span>), (<span class=\"string\">&#x27;Gol&#x27;</span>, <span class=\"string\">&#x27;D&#x27;</span>, <span class=\"string\">&#x27;Roger&#x27;</span>), (<span class=\"string\">&#x27;One&#x27;</span>, <span class=\"string\">&#x27;Piece&#x27;</span>)])</span><br><span class=\"line\">tokenizer.tokenize(inputSTR.split())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tokenizer.tokenize (word_tokenize(inputSTR)))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">[&#x27;The&#x27;, &#x27;story&#x27;, &#x27;follows&#x27;, &#x27;the&#x27;, &#x27;adventures&#x27;, &#x27;of&#x27;, &#x27;Monkey_D_Luffy&#x27;, &#x27;,&#x27;, &#x27;a&#x27;, &#x27;boy&#x27;, &#x27;whose&#x27;, &#x27;body&#x27;, &#x27;gained&#x27;, &#x27;the&#x27;, &#x27;properties&#x27;, &#x27;of&#x27;, &#x27;rubber&#x27;, &#x27;after&#x27;, &#x27;unintentionally&#x27;, &#x27;eating&#x27;, &#x27;a&#x27;, &#x27;Devil_Fruit&#x27;, &#x27;.&#x27;, &#x27;With&#x27;, &#x27;his&#x27;, &#x27;pirate&#x27;, &#x27;crew&#x27;, &#x27;,&#x27;, &#x27;the&#x27;, &#x27;Straw_Hat_Pirates&#x27;, &#x27;,&#x27;, &#x27;Luffy&#x27;, &#x27;explores&#x27;, &#x27;the&#x27;, &#x27;Grand_Line&#x27;, &#x27;in&#x27;, &#x27;search&#x27;, &#x27;of&#x27;, &#x27;the&#x27;, &#x27;deceased&#x27;, &#x27;King_of_the_Pirates&#x27;, &#x27;Gol_D_Roger&#x27;, &quot;&#x27;s&quot;, &#x27;ultimate&#x27;, &#x27;treasure&#x27;, &#x27;known&#x27;, &#x27;as&#x27;, &#x27;the&#x27;, &#x27;One_Piece&#x27;, &#x27;in&#x27;, &#x27;order&#x27;, &#x27;to&#x27;, &#x27;become&#x27;, &#x27;the&#x27;, &#x27;next&#x27;, &#x27;King_of_the_Pirates&#x27;, &#x27;.&#x27;]</span><br></pre></td></tr></table></figure>\n\n\n<p>等我們都斷好詞之後，若有需求的話，有兩種將整個文本標準化的方法，像小櫻一樣，讓這些字現出原形，展現出其真正的力量：一種是stemming、另一種則是lemmatization。</p>\n<p><img src=\"https://miro.medium.com/max/1128/1*HLQgkMt5-g5WO5VpNuTl_g.jpeg\"><br>Resource: <a href=\"https://tr.pinterest.com/pin/706854104005417976/\">https://tr.pinterest.com/pin/706854104005417976/</a></p>\n<h2 id=\"stemming\"><a href=\"#stemming\" class=\"headerlink\" title=\"stemming\"></a>stemming</h2><p>stemming在於找出每一個字的字根。你可能也會發現這些stemming過後的字有些跟你印象中的字根不太一樣，所以要注意，這裡的字根並不一定是字典上所記載的字根，再加上這些字是以rule-based的方法打造而成的，只是為了找出每個字的較短形式。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> nltk.stem <span class=\"keyword\">import</span> PorterStemmer</span><br><span class=\"line\">ps = PorterStemmer()</span><br><span class=\"line\">word = (<span class=\"string\">&quot;adventure&quot;</span>)</span><br><span class=\"line\">ps.stem(word)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">adventur</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"lemmatization\"><a href=\"#lemmatization\" class=\"headerlink\" title=\"lemmatization\"></a>lemmatization</h2><p>跟stemming不一樣的是，lemmatization不只是把字切斷，而是透過字彙庫的建置，提供詞幹的結果。簡單來說就是比較正確的stemming啦！</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> nltk</span><br><span class=\"line\"><span class=\"keyword\">from</span> nltk.stem <span class=\"keyword\">import</span> WordNetLemmatizer </span><br><span class=\"line\">lemmatizer = WordNetLemmatizer()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(lemmatizer.lemmatize(<span class=\"string\">&quot;adventures&quot;</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(lemmatizer.lemmatize(<span class=\"string\">&quot;deceased&quot;</span>))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">adventure</span><br><span class=\"line\">deceased</span><br></pre></td></tr></table></figure>\n\n<p>把他加入句子之後就會變成這樣：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">tokenizer = MWETokenizer([(<span class=\"string\">&#x27;Monkey&#x27;</span>, <span class=\"string\">&#x27;D&#x27;</span>, <span class=\"string\">&#x27;Luffy&#x27;</span>), (<span class=\"string\">&#x27;Devil&#x27;</span>, <span class=\"string\">&#x27;Fruit&#x27;</span>), (<span class=\"string\">&#x27;Straw&#x27;</span>, <span class=\"string\">&#x27;Hat&#x27;</span>, <span class=\"string\">&#x27;Pirates&#x27;</span>), (<span class=\"string\">&#x27;Grand&#x27;</span>, <span class=\"string\">&#x27;Line&#x27;</span>), (<span class=\"string\">&#x27;King&#x27;</span>, <span class=\"string\">&#x27;of&#x27;</span>, <span class=\"string\">&#x27;the&#x27;</span>, <span class=\"string\">&#x27;Pirates&#x27;</span>), (<span class=\"string\">&#x27;Gol&#x27;</span>, <span class=\"string\">&#x27;D&#x27;</span>, <span class=\"string\">&#x27;Roger&#x27;</span>), (<span class=\"string\">&#x27;One&#x27;</span>, <span class=\"string\">&#x27;Piece&#x27;</span>)])</span><br><span class=\"line\">word_list = tokenizer.tokenize(inputSTR.split())</span><br><span class=\"line\">lemmatized_output = <span class=\"string\">&#x27; &#x27;</span>.join([lemmatizer.lemmatize(w) <span class=\"keyword\">for</span> w <span class=\"keyword\">in</span> word_list])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(lemmatized_output)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">The story follows the adventure of Monkey_D_Luffy , a boy whose body gained the property of rubber after unintentionally eating a Devil Fruit. With his pirate crew, the Straw_Hat_Pirates , Luffy explores the Grand_Line in search of the deceased King_of_the_Pirates Gol D Roger&#x27;s ultimate treasure known a the One_Piece in order to become the next King of the Pirates.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>好的，那今天就介紹到這裡囉，明天就要來介紹TF-IDF以及Bag of Words囉！我們明天再見吧！</p>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"NLP】Day 21: 手把手教你如何利用 Python 搭建 Tensorflow 的深度學習模型 ","url":"/2023/01/08/NLP%E3%80%91Day-21-%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8-Python-%E6%90%AD%E5%BB%BA-Tensorflow-%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B/","content":"<p>啊！好久沒有實作了，不曉得各位還記不記得怎麼寫 Python 呢？今天的這篇文章將主要以實作為主，簡單地介紹如何搭建出我們在過去幾天的旅程中，學到的那些形形色色的深度學習模型：LSTM、GRU，以及兩者的雙向版本，也就是BiLSTM、以及BiGRU。不過，你可能已經發現，咦？那單純的 RNN 呢？事實上，在自然語言處理的實務中，由於先前提過的那些缺失，如記性不好等問題，已經很少人使用單純的 RNN 來完成任務了。欸！別人都是一篇一個神經網路，我直接一篇全部寫完，超值吧！還不趕快再點一下我其他文章！</p>\n<p>不過在開始弄髒你的手實際上工之前，我們得先了解今天要用的工具有哪些。</p>\n<h2 id=\"今天要使用的工具箱\"><a href=\"#今天要使用的工具箱\" class=\"headerlink\" title=\"今天要使用的工具箱\"></a>今天要使用的工具箱</h2><p>對於一些可能從未寫過程式的朋友，可能會有一些疑慮，擔心說「我們該不會要從零開始搭神經網路吧？先前介紹過的那些輸出層、輸入層、隱藏層殺毀的，我們都要從頭開始做起嗎？別吧？」接著擺出了 No No No 的手勢。</p>\n<p><img src=\"https://i.imgur.com/x3YOaph.jpg\"></p>\n<p>事實上，像這種這麼常用的深度學習工具，不可能沒有前人寫過。所謂前人種樹，後人乘涼，在過去就已經有程式大神代替我們將這些深度學習以及神經網路整理成 API 了。什麼？你說什麼是API？沒事，你只需要理解因為某邪惡大神的建樹，我們只需要簡單地引入（<code>import</code>）工具箱後，這些神經網路就可以隨取隨用了。我想，最多只需要調參數、以及疊各種隱藏層吧？而我今天就會一步一步地仔細帶領你撰寫搭建模型的程式，並逐句介紹這些程式語言在做什麼。至於是哪個大神如此邪惡，擁有幾乎全世界所有網頁的資料，又有極大量的運算資源？其實也沒有別人了，就是：</p>\n<p><img src=\"https://i.redd.it/1qskl89lxzs31.png\"></p>\n<h3 id=\"Tensorflow\"><a href=\"#Tensorflow\" class=\"headerlink\" title=\"Tensorflow\"></a>Tensorflow</h3><p>Tensorflow 是一個開源的深度學習程式庫，並利用機器學習來完成各種人工智慧的下游任務，例如：圖像辨識，還有我們的重點，自然語言處理。你現在所用的幾乎所有 Google 的服務，舉凡如 Google 搜尋、Google 翻譯（乙定要有的吧！）、GOogle 語音辨識、Google 地圖、Google 相片，都是來自於利用 Tensorflow 的深度學習框架所打造而成的。換句話說，我們其實就是站在 Google 打造好的立足點來解決自然語言處理的應用。</p>\n<h2 id=\"LSTM\"><a href=\"#LSTM\" class=\"headerlink\" title=\"LSTM\"></a>LSTM</h2><p>首先，在程式檔的頂端，我們得先引用搭建神經網路所需要的工具箱，也就是 Tensorflow。今天，我們會直接使用 tensorflow 內建的 imdb 資料集。<del>資料科學的前輩對imdb資料一定又愛又恨吧！</del></p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.datasets <span class=\"keyword\">import</span> imdb <span class=\"comment\"># 引入資料集</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.models <span class=\"keyword\">import</span> Sequential <span class=\"comment\"># 引入序列模型，記得我們在先前提到的 sequence model嗎？</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.layers <span class=\"keyword\">import</span> Dense <span class=\"comment\"># 一層全連接的神經網路</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.layers <span class=\"keyword\">import</span> LSTM <span class=\"comment\"># 長短期記憶（Long Short-term memory）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.layers <span class=\"keyword\">import</span> Embedding <span class=\"comment\"># 使模型理解語意的 Embedding</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.preprocessing <span class=\"keyword\">import</span> sequence </span><br><span class=\"line\"><span class=\"comment\"># 在前處理會需要用到，將長短不一的資料補齊或截斷成相同長度（記得我們先前所說神經網路的長度可以彈性調整嗎？就是透過這裡將序列轉換成相同長度）</span></span><br><span class=\"line\">tf.random.set_seed(<span class=\"number\">7</span>)  <span class=\"comment\"># 設定隨機種子</span></span><br></pre></td></tr></table></figure>\n<p>再來得將資料先處理好，我們在這邊先將資料處理好之後，後續在搭建無論是 GRU，甚至是BiLSTM、BiGRU，都會直接用這裡的資料，就不會再重複這裡的動作啦！</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 設定資料中字頻前五千的字</span></span><br><span class=\"line\">top_words = <span class=\"number\">5000</span> </span><br><span class=\"line\"><span class=\"comment\"># 取得資料後，分成訓練資料集以及測試資料集</span></span><br><span class=\"line\">(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words) </span><br><span class=\"line\"><span class=\"comment\"># 在這裡就是要截斷或補齊輸入模型的句子，以保持所有句子長度相同。</span></span><br><span class=\"line\">max_review_length = <span class=\"number\">500</span></span><br><span class=\"line\">X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)</span><br><span class=\"line\">X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)</span><br></pre></td></tr></table></figure>\n<p>接著要先搭建模型，搭建完模型之後才會開始訓練。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">embedding_vector_length = <span class=\"number\">32</span></span><br><span class=\"line\"><span class=\"comment\"># 首先設定模型為序列模型</span></span><br><span class=\"line\">lstm_model = Sequential()</span><br><span class=\"line\"><span class=\"comment\"># 將已經訓練好的 Embedding 加入神經網路模型中</span></span><br><span class=\"line\">lstm_model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))</span><br><span class=\"line\"><span class=\"comment\"># LSTM函數中的值則是代表你要在模型中放入多少 LSTM 的神經元（有門閥的那個）</span></span><br><span class=\"line\">lstm_model.add(LSTM(<span class=\"number\">100</span>))</span><br><span class=\"line\"><span class=\"comment\"># 全連接的神經網路隱藏層，可以選擇想要的激勵函數</span></span><br><span class=\"line\">lstm_model.add(Dense(<span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>))</span><br><span class=\"line\"><span class=\"comment\"># 這裡的compile是在將模型組合起來。</span></span><br><span class=\"line\">lstm_model.<span class=\"built_in\">compile</span>(loss=<span class=\"string\">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class=\"string\">&#x27;adam&#x27;</span>, metrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 可以一起來看看模型長怎樣</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(lstm_model.summary())</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Model: &quot;sequential&quot;</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\"> Layer (type)                Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\"> embedding (Embedding)       (None, 500, 32)           160000    </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> lstm (LSTM)                 (None, 100)               53200     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense (Dense)               (None, 1)                 101       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 213,301</span><br><span class=\"line\">Trainable params: 213,301</span><br><span class=\"line\">Non-trainable params: 0</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">None</span><br></pre></td></tr></table></figure>\n<p>我們從<code>params</code>中可以看到模型總共有多少個參數。另外，在 LSTM 中的參數決定的是神經元數量，沒有一定的固定數字，但是有個模式可循。首先，RNN 的寬度代表特徵的多寡，由輸入以及篩選器的數量決定；深度代表的則是特徵的豐富度，由神經網路層數及步驟數決定的。若不需要從原資料產出大量特徵，那麼一般來說寬度會減少。若資料相對單純，那麼深度則需要調整的淺一點，這麼做可以節省訓練時間及資源。(<a href=\"https://stackoverflow.com/questions/59072728/what-is-the-rule-to-know-how-many-lstm-cells-and-how-many-units-in-each-lstm-cel\">source</a>)</p>\n<p>確認沒問題之後，就可以開始訓練模型了！這裡可能會需要跑比較久時間，像我試跑就跑了十分多鐘，所以才說深度學習模型的一個缺點就是速度比較慢一點。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">model.fit(X_train, y_train, epochs=<span class=\"number\">3</span>, batch_size=<span class=\"number\">64</span>) <span class=\"comment\"># 訓練模型</span></span><br><span class=\"line\">scores = model.evaluate(X_test, y_test, verbose=<span class=\"number\">0</span>) <span class=\"comment\"># 測試模型</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Accuracy: %.2f%%&quot;</span> % (scores[<span class=\"number\">1</span>]*<span class=\"number\">100</span>)) </span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Epoch 1/3</span><br><span class=\"line\">391/391 [==============================] - 207s 525ms/step - loss: 0.4316 - accuracy: 0.7884</span><br><span class=\"line\">Epoch 2/3</span><br><span class=\"line\">391/391 [==============================] - 205s 525ms/step - loss: 0.2653 - accuracy: 0.8937</span><br><span class=\"line\">Epoch 3/3</span><br><span class=\"line\">391/391 [==============================] - 204s 522ms/step - loss: 0.2377 - accuracy: 0.9065</span><br><span class=\"line\">Accuracy: 86.72%</span><br></pre></td></tr></table></figure>\n<h2 id=\"BiLSTM\"><a href=\"#BiLSTM\" class=\"headerlink\" title=\"BiLSTM\"></a>BiLSTM</h2><p>其實雙向的 BiLSTM 也是大同小異，我們只需要關注模型的搭建上就可以了。資料一樣沿用在上面所整理好的訓練以及測試資料集。別忘了要先引進雙向的套件，至於其他的套件，在前面就已經引進過，就不需要再引進了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.layers <span class=\"keyword\">import</span> Bidirectional</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">bilstm_model = Sequential()</span><br><span class=\"line\">bilstm_model.add(Embedding(top_words, embedding_vetcor_length, input_length=max_review_length))</span><br><span class=\"line\">bilstm_model.add(Bidirectional(LSTM(<span class=\"number\">100</span>, dropout=<span class=\"number\">0.2</span>, recurrent_dropout=<span class=\"number\">0.2</span>)))</span><br><span class=\"line\">bilstm_model.add(Dense(<span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>))</span><br><span class=\"line\">bilstm_model.<span class=\"built_in\">compile</span>(loss=<span class=\"string\">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class=\"string\">&#x27;adam&#x27;</span>, metrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(bilstm_model.summary()) <span class=\"comment\"># 同樣先來看模型參數</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Model: &quot;sequential_1&quot;</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\"> Layer (type)                Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\"> embedding_1 (Embedding)     (None, 500, 32)           160000    </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> bidirectional (Bidirectiona  (None, 200)              106400    </span><br><span class=\"line\"> l)                                                              </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense_1 (Dense)             (None, 1)                 201       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 266,601</span><br><span class=\"line\">Trainable params: 266,601</span><br><span class=\"line\">Non-trainable params: 0</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">None</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">bilstm_model.fit(X_train, y_train, epochs=<span class=\"number\">3</span>, batch_size=<span class=\"number\">64</span>)</span><br><span class=\"line\">scores = bilstm_model.evaluate(X_test, y_test, verbose=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Accuracy: %.2f%%&quot;</span> % (scores[<span class=\"number\">1</span>]*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Epoch 1/3</span><br><span class=\"line\">391/391 [==============================] - 805s 2s/step - loss: 0.4870 - accuracy: 0.7579</span><br><span class=\"line\">Epoch 2/3</span><br><span class=\"line\">391/391 [==============================] - 803s 2s/step - loss: 0.3148 - accuracy: 0.8726</span><br><span class=\"line\">Epoch 3/3</span><br><span class=\"line\">391/391 [==============================] - 799s 2s/step - loss: 0.2637 - accuracy: 0.8944</span><br><span class=\"line\">Accuracy: 87.89%</span><br></pre></td></tr></table></figure>\n<p>BiLSTM 因為參數更多，所以訓練時間又很明顯地變更久了。我在這裡花了快20多分鐘在訓練模型。</p>\n<h2 id=\"GRU\"><a href=\"#GRU\" class=\"headerlink\" title=\"GRU\"></a>GRU</h2><figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">gru_model = Sequential()</span><br><span class=\"line\">gru_model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))</span><br><span class=\"line\">gru_model.add(GRU(<span class=\"number\">32</span>))</span><br><span class=\"line\">gru_model.add(Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>))</span><br><span class=\"line\">gru_model.add(Dense(<span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>))</span><br><span class=\"line\">gru_model.<span class=\"built_in\">compile</span>(loss=<span class=\"string\">&quot;binary_crossentropy&quot;</span>, optimizer=<span class=\"string\">&#x27;adam&#x27;</span>,metrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(gru_model.summary())</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Model: &quot;sequential_2&quot;</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\"> Layer (type)                Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\"> embedding_2 (Embedding)     (None, 500, 32)           160000    </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> gru (GRU)                   (None, 32)                6336      </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense_2 (Dense)             (None, 10)                330       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense_3 (Dense)             (None, 1)                 11        </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 166,677</span><br><span class=\"line\">Trainable params: 166,677</span><br><span class=\"line\">Non-trainable params: 0</span><br><span class=\"line\">_________________________________________________________________</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">gru_model.fit(X_train, y_train, epochs=<span class=\"number\">3</span>, batch_size=<span class=\"number\">64</span>)</span><br><span class=\"line\">scores = gru_model.evaluate(X_test, y_test, verbose=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Accuracy: %.2f%%&quot;</span> % (scores[<span class=\"number\">1</span>]*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Epoch 1/3</span><br><span class=\"line\">391/391 [==============================] - 89s 222ms/step - loss: 0.4658 - accuracy: 0.7620</span><br><span class=\"line\">Epoch 2/3</span><br><span class=\"line\">391/391 [==============================] - 88s 226ms/step - loss: 0.2631 - accuracy: 0.8943</span><br><span class=\"line\">Epoch 3/3</span><br><span class=\"line\">391/391 [==============================] - 88s 224ms/step - loss: 0.2215 - accuracy: 0.9149</span><br><span class=\"line\">Accuracy: 87.74%</span><br></pre></td></tr></table></figure>\n<p>GRU 模型當初有說，其實設計架構上與 LSTM 差不了多少，但是由於 GRU 的參數相對比較少的緣故，所以運算時間還有資源都比原本的 LSTM 還要少。在準確度皆為接近 90% 的前提下，我在前面訓練 LSTM 的時間為十一分鐘左右，而這裡 GRU 我卻只需要五分多鐘就結束訓練了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">bigru_model = Sequential()</span><br><span class=\"line\">bigru_model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))</span><br><span class=\"line\">bigru_model.add(Bidirectional(GRU(<span class=\"number\">32</span>)))</span><br><span class=\"line\">bigru_model.add(Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>))</span><br><span class=\"line\">bigru_model.add(Dense(<span class=\"number\">1</span>, activation=<span class=\"string\">&#x27;sigmoid&#x27;</span>))</span><br><span class=\"line\">bigru_model.<span class=\"built_in\">compile</span>(loss=<span class=\"string\">&quot;binary_crossentropy&quot;</span>, optimizer=<span class=\"string\">&#x27;adam&#x27;</span>,metrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])</span><br><span class=\"line\">bigru_model.summary()</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Model: &quot;sequential_3&quot;</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\"> Layer (type)                Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\"> embedding_3 (Embedding)     (None, 500, 32)           160000    </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> bidirectional_1 (Bidirectio  (None, 64)               12672     </span><br><span class=\"line\"> nal)                                                            </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense_4 (Dense)             (None, 10)                650       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense_5 (Dense)             (None, 1)                 11        </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 173,333</span><br><span class=\"line\">Trainable params: 173,333</span><br><span class=\"line\">Non-trainable params: 0</span><br><span class=\"line\">_________________________________________________________________</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">bigru_model.fit(X_train, y_train, epochs=<span class=\"number\">3</span>, batch_size=<span class=\"number\">64</span>)</span><br><span class=\"line\">scores = bigru_model.evaluate(X_test, y_test, verbose=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Accuracy: %.2f%%&quot;</span> % (scores[<span class=\"number\">1</span>]*<span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">Epoch <span class=\"number\">1</span>/<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">391</span>/<span class=\"number\">391</span> [==============================] - 166s 414ms/step - loss: <span class=\"number\">0.4397</span> - accuracy: <span class=\"number\">0.7790</span></span><br><span class=\"line\">Epoch <span class=\"number\">2</span>/<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">391</span>/<span class=\"number\">391</span> [==============================] - 160s 408ms/step - loss: <span class=\"number\">0.2599</span> - accuracy: <span class=\"number\">0.8975</span></span><br><span class=\"line\">Epoch <span class=\"number\">3</span>/<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">391</span>/<span class=\"number\">391</span> [==============================] - 159s 407ms/step - loss: <span class=\"number\">0.2170</span> - accuracy: <span class=\"number\">0.9153</span></span><br><span class=\"line\">Accuracy: <span class=\"number\">87.74</span>%</span><br></pre></td></tr></table></figure>\n<p>BiGRU 的模型訓練時間跟 BiLSTM 相比就更明顯了，BiLSTM 我當初花了 40 分鐘左右才訓練完成，反觀 BiGRU，只需要 9 分鐘左右就結束了，模型表現卻也都差不多。</p>\n<h2 id=\"結語\"><a href=\"#結語\" class=\"headerlink\" title=\"結語\"></a>結語</h2><p>我們最後來看一下結果，可以發現四種表現都沒有差距太大，但訓練時間就有明顯的差異，也就是 GRU 的時間還要少於 LSTM；另外，雙向的神經網路模型表現也比單向還要好一些。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>LSTM</th>\n<th>BiLSTM</th>\n<th>GRU</th>\n<th>BiGRU</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>準確度 <strong>(%)</strong></td>\n<td>86.72</td>\n<td>87.89</td>\n<td>87.74</td>\n<td>87.74</td>\n</tr>\n<tr>\n<td>時間 <strong>(min)</strong></td>\n<td>11</td>\n<td>19</td>\n<td>5.5</td>\n<td>9</td>\n</tr>\n</tbody></table>\n<p>這也印證了在先前 <a href=\"https://ithelp.ithome.com.tw/articles/10302765\">【NLP】Day 17: 每天成為更好的自己！神經網路也是！深度學習模型 GRU</a> 的文章中所說，參數的多寡會大大影響著運算時間以及資源。好，神經網路的實作就到這邊，是不是比想像中的還要簡單很多呢？明天開始就要進入 BERT 了喔！</p>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"【Crawler】Day 27: 爬爬爬，向前爬！網路爬蟲速成班！（上）","url":"/2023/01/08/%E3%80%90Crawler%E3%80%91Day-27-%E7%88%AC%E7%88%AC%E7%88%AC%EF%BC%8C%E5%90%91%E5%89%8D%E7%88%AC%EF%BC%81%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2%E9%80%9F%E6%88%90%E7%8F%AD%EF%BC%81%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"<p>在我們過去一起經歷的旅程中，我們從一開始的正規表達式、詞頻、N-Gram，一直到機器學習，像是貝氏分類器、羅吉斯迴歸等等，接著又講到了深度學習，利用神經網路來進行自然語言處理，比如說像是循環神經網路、長短期記憶等等，後來又發展出了自注意機制，有了 Transformer 以及 BERT 還有他的芝麻街小夥伴，又學到了以語言學基礎的工具 Articut 以及 Loki。</p>\n<p>我們一起學習了好多好多的語言模型，但不知道你有沒有想過，處理這些語言資料的我們，學了那麼多的模型，那資料從哪來？我們想要學習處理語言資料，但是卻沒有語言資料，那不就像是雞蛋布丁沒有雞蛋、哆啦A夢沒有百寶袋、太陽餅沒有太陽、老婆餅沒有老婆 <del>等等，如果你有看電馭叛客：邊緣行者的話，那就可能有</del>。那其實要取得這些資料，有很大一部分是取自於網路上的資源（但也必須要守法喔！）取得這些資源的方法就是利用網路爬蟲！今天的內容都是爬蟲的先備知識，要熟悉這些技巧才有辦法駕馭爬蟲喔！</p>\n<p><img src=\"https://i.imgur.com/T8PGH8F.jpg\"></p>\n<h2 id=\"小複習\"><a href=\"#小複習\" class=\"headerlink\" title=\"小複習\"></a>小複習</h2><p>還記得我們在剛開始旅程的時候，曾經有講過串列 <code>list</code> 以及 字典<code>dict</code>嗎？我們在這邊再複習一遍。</p>\n<h3 id=\"串列-list\"><a href=\"#串列-list\" class=\"headerlink\" title=\"串列 list\"></a>串列 <code>list</code></h3><p>我們可以透過串列來儲存一系列的資料，程式碼中透過中括號將資料包起來的就是 <code>list</code> ，並透過索引值（index number）來取得串列中的資料。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">myLIST = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]</span><br><span class=\"line\">myStrLIST = [<span class=\"string\">&quot;1&quot;</span>, <span class=\"string\">&quot;2&quot;</span>, <span class=\"string\">&quot;3&quot;</span>, <span class=\"string\">&quot;4&quot;</span>, <span class=\"string\">&quot;5&quot;</span>, <span class=\"string\">&quot;6&quot;</span>, <span class=\"string\">&quot;7&quot;</span>, <span class=\"string\">&quot;8&quot;</span>, <span class=\"string\">&quot;9&quot;</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(myLIST))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(myStrLIST[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(myStrLIST[<span class=\"number\">0</span>]))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;class &#x27;list&#x27;&gt;</span><br><span class=\"line\">1</span><br><span class=\"line\">&lt;class &#x27;str&#x27;&gt;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"字典-dict\"><a href=\"#字典-dict\" class=\"headerlink\" title=\"字典 dict\"></a>字典 <code>dict</code></h3><p>至於字典就是像平常我們查字典一樣，每一個字會有一個對應的解釋，在 Python 中，我們稱呼「字」為鍵（key），「解釋」則為值（value）。<br>例如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">strawhat = &#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;船長&quot;</span>: <span class=\"string\">&quot;魯夫&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;戰鬥員&quot;</span>: <span class=\"string\">&quot;索隆&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;航海士&quot;</span>: <span class=\"string\">&quot;娜美&quot;</span>,</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"另外，我們也可以將字典加入串列中！-dict-in-list\"><a href=\"#另外，我們也可以將字典加入串列中！-dict-in-list\" class=\"headerlink\" title=\"另外，我們也可以將字典加入串列中！ dict in list\"></a>另外，我們也可以將字典加入串列中！ <code>dict in list</code></h4><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">strawhat = &#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;船長&quot;</span>: <span class=\"string\">&quot;魯夫&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;戰鬥員&quot;</span>: <span class=\"string\">&quot;索隆&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;航海士&quot;</span>: <span class=\"string\">&quot;娜美&quot;</span>,</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">heart = &#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;船長&quot;</span>: <span class=\"string\">&quot;托拉法爾加羅&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;副船長&quot;</span>: <span class=\"string\">&quot;培波&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;船員一號&quot;</span>: <span class=\"string\">&quot;佩金&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;船員二號&quot;</span>: <span class=\"string\">&quot;夏奇&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;船員三號&quot;</span>: <span class=\"string\">&quot;強帕爾&quot;</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">aliance = []</span><br><span class=\"line\">aliance.append(strawhat)</span><br><span class=\"line\">aliance.append(heart)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(aliance[<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(aliance[<span class=\"number\">1</span>][<span class=\"string\">&#x27;船長&#x27;</span>])</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;&#x27;船長&#x27;: &#x27;托拉法爾加羅&#x27;, &#x27;副船長&#x27;: &#x27;培波&#x27;, &#x27;船員&#x27;: &#x27;佩金&#x27;, &#x27;二號船員&#x27;: &#x27;夏奇&#x27;, &#x27;三號船員&#x27;: &#x27;強帕爾&#x27;&#125;</span><br><span class=\"line\">托拉法爾加羅</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"認識-JSON-檔\"><a href=\"#認識-JSON-檔\" class=\"headerlink\" title=\"認識 JSON 檔\"></a>認識 <code>JSON</code> 檔</h3><p>由於網站跟網站之間傳遞資料，最常用的資料形式就是 json 檔，而爬蟲所要做的就是要去抓取網頁中的這些 json 檔，或是在網頁上的元素。形式可以是一串 <code>list</code>，或是 <code>dict</code> in <code>list</code>，或是單一一個<code>dict</code> 都可以轉換成 json 檔。 json 檔的要點如下：</p>\n<ul>\n<li><strong>JavaScript Object Notation</strong> 的縮寫</li>\n<li>常用於網站上的<strong>資料呈現、傳輸</strong></li>\n<li>可在 JSON 之內加入<strong>相同的基本資料類型</strong></li>\n</ul>\n<h4 id=\"輸出-json-檔\"><a href=\"#輸出-json-檔\" class=\"headerlink\" title=\"輸出 json 檔\"></a>輸出 json 檔</h4><p>在這裡，我們用先前的串列<code>aliance</code>來輸出 json 檔，並存成檔名為<code>data.json</code>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;data.json&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    json.dump(aliance, f)</span><br></pre></td></tr></table></figure>\n<h4 id=\"讀取-json-檔\"><a href=\"#讀取-json-檔\" class=\"headerlink\" title=\"讀取 json 檔\"></a>讀取 json 檔</h4><p>在這裡，我們則是將 <strong>同一個檔案路徑</strong>下的<code>data.json</code>讀取進 Python，並取變數名為 pirates。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;data.json&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    pirates = json.load(f)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Try…Except-一個寫程式時眼不見為淨的好幫手（誤）\"><a href=\"#Try…Except-一個寫程式時眼不見為淨的好幫手（誤）\" class=\"headerlink\" title=\"Try…Except 一個寫程式時眼不見為淨的好幫手（誤）\"></a>Try…Except 一個寫程式時眼不見為淨的好幫手（誤）</h3><p>在寫爬蟲時，很有可能會碰到一個窘境，就是網路資料路徑可能會有改變，或著是你爬得太猖狂，爬到網頁管理者覺得你太超過了，擋你 IP 之類的，這時候你寫的爬蟲程式碼可能就會出現錯誤。那出現錯誤，程式就會立刻停止了耶！這時該怎麼辦呢？讓爬蟲繼續爬嗎？還是你要他進行什麼動作？</p>\n<p>在這個時候就會需要進行所謂的<strong>例外處理</strong>，寫法就是 <code>try...except</code>。其實你可以想像就是你跟電腦說：「你先試試看（這樣那樣） 除非（發生什麼事）你再（幹嘛幹嘛）。」來看看實作怎麼做吧！</p>\n<p>假如說你今天寫了一個相加的程式：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">a = <span class=\"built_in\">input</span>(<span class=\"string\">&#x27;輸入數字：&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a + <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<p>運行程式之後絕對會出現錯誤：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">輸入數字：1</span><br><span class=\"line\">---------------------------------------------------------------------------</span><br><span class=\"line\">TypeError                                 Traceback (most recent call last)</span><br><span class=\"line\">&lt;ipython-input-1-f0bc7ffa95ff&gt; in &lt;module&gt;</span><br><span class=\"line\">      1 a = input(&#x27;輸入數字：&#x27;)</span><br><span class=\"line\">----&gt; 2 print(a + 1)</span><br><span class=\"line\"></span><br><span class=\"line\">TypeError: can only concatenate str (not &quot;int&quot;) to str</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>這是因為你輸入的資料型態其實不是「數字」，而是「字串」。「字串」只能跟「字串」相加，不能跟數字相加，所以這時候就出現錯誤。</p>\n<p>那這時候我們可以加入 <code>try...except...</code> 。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">try:                      # 使用 try，測試內容是否正確</span><br><span class=\"line\">  a = input(&#x27;輸入數字：&#x27;)</span><br><span class=\"line\">  print(a + 1)</span><br><span class=\"line\">except:</span><br><span class=\"line\">  print(&#x27;發生錯誤&#x27;)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">發生錯誤</span><br></pre></td></tr></table></figure>\n\n<p>這邊就會直接跳過錯誤訊息，直接執行<code>except</code>後面的動作，在這邊就是印出「發生錯誤」。</p>\n<p>那你可以仔細觀察一下前面出現錯誤的訊息，你可以發現有一行：</p>\n<blockquote>\n<p>TypeError: can only concatenate str (not “int”) to str</p>\n</blockquote>\n<p>前面的 <code>TypeError</code> 指的就是這個錯誤的類型，有很多種的 Error message，例如：</p>\n<ul>\n<li><code>SyntaxError</code>: 程式不合 Python 語法。</li>\n<li><code>IndexError</code>: index 位置中沒有東西，或是超出 <code>list</code> 的長度！</li>\n<li><code>FileNotFoundError</code>: 找不到檔案</li>\n<li><code>ModuleNotFoundError</code>: 可能要檢查一下套件</li>\n<li><code>EOFError</code>: 可以換新的 code block 再寫一遍，才可能解決了。</li>\n</ul>\n<p>有時候會碰到各種不同的 error。你可以根據不同的 error 去決定你的 try…except 要怎麼寫，最後一個沒有寫的 except 則是一旦發生錯誤，而前面又沒有指定時，就會跑到這個區塊，例如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span>:                      <span class=\"comment\"># 使用 try，測試內容是否正確</span></span><br><span class=\"line\">  a = <span class=\"built_in\">input</span>(<span class=\"string\">&#x27;輸入數字：&#x27;</span>)</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(a + <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> TypeError:         <span class=\"comment\"># 如果 try 的內容發生錯誤，就執行 except 裡的內容</span></span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;發生錯誤&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span>:</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;跑來這裡&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">輸入數字：1</span><br><span class=\"line\">發生錯誤</span><br></pre></td></tr></table></figure>\n\n<p>好的，今天的內容就是這些！明天就會開始正式進入爬蟲啦！</p>\n"},{"title":"【AI】Day 30: 未竟的旅程：自然語言處理的拉乎德爾","url":"/2023/01/08/%E3%80%90AI%E3%80%91Day-30-%E6%9C%AA%E7%AB%9F%E7%9A%84%E6%97%85%E7%A8%8B%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E7%9A%84%E6%8B%89%E4%B9%8E%E5%BE%B7%E7%88%BE/","content":"<blockquote>\n<p>「終點，是旅程的一部分。」<br>東尼・史塔克《復仇者聯盟4：終局之戰》</p>\n</blockquote>\n<p>最後一天，我想來談談 <strong>「人工智慧」</strong>。</p>\n<p>在過去的旅程中，我們花了將近一半的篇幅在討論機器學習模型。從一開始的 N-Gram、BoW、樸素貝氏分類模型、羅吉斯迴歸，一直到深度學習的神經網路模型，像是循環神經網路、長短期記憶，還有利用自注意力機制的 Transformer，還有踩在巨人肩膀上，使用預訓練模型完成任何任務的 BERT，是一段旅程，而且絕對是一段縱覽從過去到現在自然語言處理以及計算語言學的朝聖之旅。</p>\n<p>不知道大家是否還記得，我在第一天的文章，也就是 <a href=\"https://ithelp.ithome.com.tw/articles/10291504\">前往NLP的偉大航道！一起成為我的夥伴吧！</a>中提到了計算語言學、自然語言處理、人工智慧還有非常大的進步空間，但我們不免還是能從過去的研究中，看出發展的脈絡。</p>\n<p>但今天，我們不再講機器學習，讓我把視角稍微拉大，在鐵人賽的最後，來簡單聊聊「人工智慧」。</p>\n<h2 id=\"關於人工智慧\"><a href=\"#關於人工智慧\" class=\"headerlink\" title=\"關於人工智慧\"></a>關於人工智慧</h2><p>在 Kate Crawford 所撰寫的《人工智慧最後的秘密》一書中，一開始提到了「聰明漢斯」。「聰明漢斯」是一匹能夠計算數學、告知時間、辨識日期以及音符，還可以拼出文法跟句子。訓練者馮・奧斯頓在一開始訓練這匹馬時，透過握住馬匹的腿，讓他學習數字，並引導馬匹用馬蹄敲出正確的答案作為回應。他發現漢斯非常能理解人類智慧才能掌握的概念，於是便聲稱「動物也能推理」。經過教育局組成的委員會，其中的心理學家斯圖姆夫以及芬斯特，再加上其他研究者的檢驗，發現不管奧斯頓是否在場，漢斯都能夠回答出正確答案；但也發現，如果提問者不知道正確答案，或是站得離馬匹很遠，那漢斯就很少給出正確答案。</p>\n<p>後來芬斯特就發現，<strong>提問者的姿勢、呼吸和臉部表情，會在提問者移動到正確答案時，出現細微的變化</strong>；漢斯感知到這些變化後，便能在正確答案時停下動作，最後解答出正確答案。而<strong>提問者通常不知道自己給了馬匹線索</strong>。</p>\n<p>有趣的是，奧斯頓的馬匹後來流入了市場，情感以及經濟上的刺激還有投資，吸引了社會在財務、文化以及科學研究上的關注</p>\n<p>當我們把這個「聰明漢斯效應」，或是「觀察者期望效應」放在機器學習上，其實就代表著今天人類，以及透過資料所訓練的機器學習模型之間的關係。我們很難得知機器學習模型從資料中學習到了什麼，透過模型，我們可以基於需求輸出理想的資料結果，但我們不知道為什麼模型會輸出這樣的結果，因為機器學習模型從資料學到了什麼，基本上無從得知。</p>\n<h3 id=\"「聰明漢斯效應」的迷思\"><a href=\"#「聰明漢斯效應」的迷思\" class=\"headerlink\" title=\"「聰明漢斯效應」的迷思\"></a>「聰明漢斯效應」的迷思</h3><p>Crawford 在書中提到，從「聰明漢斯」的案例，可以發現兩種迷思。首先，我們假設非人系統比擬作人類心智，也就是說，只要給出<strong>夠多的資料，並以足夠的資源經過適當的訓練，就能打造出與人類相似的智慧，但卻忽略了人類是以什麼形式，不管是文化、科學，亦或是歷史、政治形式，與社會互動</strong>。第二個迷思則是，<strong>智慧是獨立存在的，智慧並不會影響社會的運作形式，也不會與歷史、政治、力量，以及文化層面有任何牽扯。</strong></p>\n<h3 id=\"人工智慧的真相\"><a href=\"#人工智慧的真相\" class=\"headerlink\" title=\"人工智慧的真相\"></a>人工智慧的真相</h3><p>但在現代社會中，人工智慧的掌握者，事實上，就代表了權力所在。從 Crawford 的觀點來看，人工智慧既非<strong>人工</strong>，也不是<strong>智慧</strong>。這是因為人工智慧並非自主學習，也不是理性且無所不察的，如同先前介紹的那些語言學習模型，只要有大型的資料集，或是對這些資料集預先定義好的標籤等等，進行廣泛密集運算的訓練。要進行這些模型的訓練，需要極為大量運算資源，這也就仰賴擁有這些資源以及資本的大企業，為了保有這些資源，人工智慧也有很大一部分是基於優勢階層的利益所建置。一旦這些人工智慧大量融入我們日常生活當中，成為我們日常生活的微觀權力，便會像是拉大貧富差距等，在在影響人類社會以及文化。</p>\n<p>那要如何有意識地對人工智慧的機器學習模型進行掌控，我認為可以從以下幾點做起。</p>\n<h2 id=\"資料的合理運用\"><a href=\"#資料的合理運用\" class=\"headerlink\" title=\"資料的合理運用\"></a>資料的合理運用</h2><p>我們就該因為這樣就放棄使用人工智慧嗎？筆者我本人倒也覺得不必要因此而因噎廢食。我們反倒更應該以更多不同面向以及角度去看待人工智慧，並小心處理每一個細節。我認為留意資料的使用就是一點，且可以從兩個角度出發。</p>\n<h3 id=\"1-作爲一般公民，有意識地掌握自身資料如何為人所用\"><a href=\"#1-作爲一般公民，有意識地掌握自身資料如何為人所用\" class=\"headerlink\" title=\"1. 作爲一般公民，有意識地掌握自身資料如何為人所用\"></a>1. 作爲一般公民，有意識地掌握自身資料如何為人所用</h3><p>在這個時代，我們能做到的，就是要有意識掌握自身資料的使用方式。現在在使用各種雲端服務時，公司都會需要你加入會員方可使用。只是在註冊的過程中，資料很容易就會被拿去做別的用途。所以在輸入任何資料的場合，都要知道自己的資料可能會被拿去做什麼事情，並有意識地去保護自己的資料。</p>\n<h3 id=\"2-作為資料處理者，有意識地仔細審閱資料的合適性\"><a href=\"#2-作為資料處理者，有意識地仔細審閱資料的合適性\" class=\"headerlink\" title=\"2. 作為資料處理者，有意識地仔細審閱資料的合適性\"></a>2. 作為資料處理者，有意識地仔細審閱資料的合適性</h3><p>作為進行自然語言處理研究的我們，在處理語言資料時，也要小心資料的運用上，是否符合需求，以及這些資料是否有害，以及是否會造成模型偏誤，導致模型無法達成我們的需求。畢竟資料會影響模型的每一步決策，因此更要小心處理。</p>\n<h4 id=\"❗資料為何重要？\"><a href=\"#❗資料為何重要？\" class=\"headerlink\" title=\"❗資料為何重要？\"></a>❗資料為何重要？</h4><p>資料的選擇不當，很有可能會造成模型偏誤，甚至影響到社會架構，我們來舉以下例子，來說明資料的重要性。訓練模型的過程中，通常都會需要取得大量的資料，而這些來源通常都來自過去的工作人員所收集的，但難保這些資料的運用，都是在一個公平且透明的框架下進行。在《大數據的傲慢與偏見》一書中就舉了一個例子：假如問一名在舒適市郊社區長大的犯人「第一次與警方交手的狀況」，他可能除了導致他入獄的那件事之外，就再也沒有跟警方交手的經驗；但若是年輕男性黑人，可能就並不是這樣。紐約公民自由聯盟的研究發現，雖然 14-24 歲的黑人跟拉美裔男性僅佔紐約 4.7%，但警方臨檢的比例卻有 40% 的盤查對象卻屬於這一群人。而這項問題的結果則會納入美國犯罪的再犯模型當中，導致警察巡邏的範圍區域，就會有極大的偏差，更因為有些未成年，並不如富人區的孩子幸運，可能會喝酒或藏有大麻而因此惹上麻煩，卻也因為這個再犯模型誤了一生，導致惡性循環。所以即使問卷中並沒有加入種族問題（因為畢竟不可能問說你是黑人還是白人），但資料仍然會有隱性的偏見存在。</p>\n<p>為了要解決這項問題，模型的解釋能力，還有解釋性人工智慧（Explainable AI, XAI）就成為一個近年最重要的議題。</p>\n<h2 id=\"解釋性人工智慧\"><a href=\"#解釋性人工智慧\" class=\"headerlink\" title=\"解釋性人工智慧\"></a>解釋性人工智慧</h2><p>所謂的解釋性人工智慧，就是藉由一系列方法，將 <strong>「黑盒子模型」</strong> 打開，了解人工智慧的決策方式，讓人類更容易理解機器學習模型，以及模型進行決策的方式。首先，我們先從黑盒子開始講起：</p>\n<p><img src=\"https://i.imgur.com/oFjjuoW.jpg\"></p>\n<p>在圖中，我們可以看到哆啦A夢拿出了一台機器，並跟大雄說，只要將手塚治虫的漫畫丟進機器中，機器就如哆啦A夢所說：「變成手塚老師了」，最後，機器會產出一本類似於原子小金剛的漫畫（因為畢竟他是手塚治虫老師嘛！）我們從中可以發現一件事：</p>\n<blockquote>\n<p>我們並不知道哆啦A夢的機器是怎麼學習手塚老師的畫風、劇本能力。</p>\n</blockquote>\n<p>哆啦A夢的這個道具，就是一種黑盒子模型。而至今為止我們所學到的那些統計語言模型，循環神經網路、長短期記憶模型、GRU，乃至於 Transformer、BERT，都屬於黑盒子模型，因為我們無從得知模型如何產出當下的結果，只知道模型能夠解決任務，僅僅如此而已。但準確度不該是模型該追求的目標，應該說，人類該如何為模型制定適當的準確度標準，才是應該思考的方向。藉此預防像是在科幻電影《2001太空漫遊》中，人工智慧最後決定殺死船艦上的所有人類，僅僅是因為任務「重要到無法允許這些人類阻礙完成任務」。</p>\n<p>我們可以從三個方向，與預測結果搭配，藉此來提升模型解釋能力。</p>\n<h3 id=\"模型驗證能力（Model-Validation）\"><a href=\"#模型驗證能力（Model-Validation）\" class=\"headerlink\" title=\"模型驗證能力（Model Validation）\"></a>模型驗證能力（Model Validation）</h3><p>我們可以藉由檢查機器學習模型是否存在「偏誤」，也就是如前面所説，模型是否只針對特定族群的資料進行訓練，以及這些「毒性」資料是否影響著模型的決策。另外，敏感私人資訊也必須進行適當的去識別化，以保護個人資訊的外流（例如法律資訊、醫療隱私等）</p>\n<h3 id=\"模型偵錯能力（Model-Debugging）\"><a href=\"#模型偵錯能力（Model-Debugging）\" class=\"headerlink\" title=\"模型偵錯能力（Model Debugging）\"></a>模型偵錯能力（Model Debugging）</h3><p>為了保證模型的可靠度與精實度，模型必須要能夠偵錯。我們必須了解機器模型背後產出結果的原因以及理由。也就是說，輸入資料的細微更動，不會大幅度地改變預測結果，這麼做可以減少混淆模型的風險。因此，更需要賦予模型透明性與詮釋性，以便在模型出現不當現象或是不合理預測時進行偵錯。</p>\n<h3 id=\"了解領域知識（Knowledge-Discovery）\"><a href=\"#了解領域知識（Knowledge-Discovery）\" class=\"headerlink\" title=\"了解領域知識（Knowledge Discovery）\"></a>了解領域知識（Knowledge Discovery）</h3><p>我們必須要確認模型是否真正理解某特定過程以及事件。也就是說，僅僅進行預測是不適當的，仍要必須解釋並了解案例中的因果關係。例如過去預測肺炎致死率的模型顯示，患有氣喘疾病可以降低肺炎死亡的風險。但事實上這是因為氣喘病患可以得到更多醫療上的治療。這其實就顯示了模型解釋能力的重要性。</p>\n<h2 id=\"結語：終點，是旅程的一部分\"><a href=\"#結語：終點，是旅程的一部分\" class=\"headerlink\" title=\"結語：終點，是旅程的一部分\"></a>結語：終點，是旅程的一部分</h2><p>最後，如同引言所說，「終點，是旅程的一部分。」雖然三十天的旅程在這邊要畫下句點了，感謝各位的陪伴，但這不代表是我學習的終點。自然語言處理的旅程，對我來說還有非常長一段路要走，對於這個領域的研究，有著更多需要我去探索的新世界，希望在接下來的旅程中，能夠繼續保有我所重視的，增長我所需要的，並開拓我所感興趣的。魯夫還沒到達拉乎德爾，但曾經說過，在這片大海上，最自由的就是海賊王；我在還沒成為那個令自己滿意的自己之前，只要能夠維持初衷、維持好奇、維持求知，那這一切的辛苦也就值得了。</p>\n<p><img src=\"https://i.imgur.com/QZoGYJv.jpg\"></p>\n"},{"title":"【NLP】Day 10: 進入偉大航道！機器學習基礎知識：你需要知道的這些那些","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-10-%E9%80%B2%E5%85%A5%E5%81%89%E5%A4%A7%E8%88%AA%E9%81%93%EF%BC%81%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%9F%BA%E7%A4%8E%E7%9F%A5%E8%AD%98%EF%BC%9A%E4%BD%A0%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E9%80%99%E4%BA%9B%E9%82%A3%E4%BA%9B/","content":"<blockquote>\n<p>「孩兒，你看清楚了沒有？」「看清楚了」<br>「都記得了沒有？」「已忘記了一小半。」<br>「現下怎樣了？」「已忘記了一大半。」<br>「好，我再使一遍。」<br>「孩兒，怎樣啦？」「還有三招沒忘記。」<br>「這我可全忘了，忘得乾乾淨淨的了。」<br><strong>張無忌、張三丰《倚天屠龍記》</strong></p>\n</blockquote>\n<p>前面講了這麼多，終於要講模型了嗎？我們這十天一起度過的旅程，目的只有一個，就是將文本資料轉化成數值資料。因為機器學習模型沒有辦法透過文字資料進行學習，唯有將其轉成數值資料，模型才有辦法從這些數字中學習洞見。前面所學都是非常重要的基礎，我認為要打好這些基礎，再來學習後面的機器學習的模型才有意義。就像是武俠小說中，渾厚內力才能帶來強大武功，若沒有強大的內力，即便你會九陽神功、九陰真經、降龍十八掌，也只是空有華麗的外殼、空虛的內在；像是死神中，強大的個體都帶有很強的靈壓，若靈壓不夠強，即便你所擁有的是規則殺的能力，在強大的靈壓面前都是毫無用處的。</p>\n<p><img src=\"https://i.imgur.com/zOq0XjF.png\"></p>\n<p>今天的文章，你可以帶走這些機器學習的基本知識：</p>\n<ul>\n<li><strong>資料集＆交叉驗證（Dataset &amp; cross validation）</strong></li>\n<li><strong>分類指標（classification matrix）</strong></li>\n<li><strong>監督式學習＆非監督式學習（supervised learning vs unsupervised learning）</strong></li>\n<li><strong>生成式模型 vs 判別式模型（generative model vs discriminative model）</strong></li>\n</ul>\n<h2 id=\"資料集＆交叉驗證\"><a href=\"#資料集＆交叉驗證\" class=\"headerlink\" title=\"資料集＆交叉驗證\"></a>資料集＆交叉驗證</h2><p>想像今天你有一份已經經過完善的前處理、以及正常的正規化，也就是可以直接丟進去模型的完整資料集。一般來說，我們會將資料集依照比例分成兩份：一份訓練資料集（training set）、一份測試資料集（test set），而這個比例有時候是8:2，有時候是7:3，依照個人配置會有所不同，沒有絕對的正確答案。首先，先給予機器學習模型訓練資料集，讓模型學習資料中的特徵，接著再給訓練好的模型進行考試，以模型預測測試資料集的目標資料（例如正負面評論），並將這些答案與原始資料的「正確答案」（Golden label）進行比對。</p>\n<h3 id=\"交叉驗證\"><a href=\"#交叉驗證\" class=\"headerlink\" title=\"交叉驗證\"></a>交叉驗證</h3><p>若是碰到資料量較小的狀況，抑或是你的資料極度不平衡的情況，就要對資料進行交叉驗證。所謂的交叉驗證就是，除了前面所提到的訓練資料集、測試資料集、還會再多分出新的一部分的開發測試資料集。首先會將資料集分成k個，然後選其中一部分做為剛剛多分出來的開發測試資料集，並以另外k-1份的資料集作為訓練資料集，並計算準確率，接著再從那k份資料中選擇下一份資料作為開發測試資料集，直到所有的資料集都已經輪過後，再以這份模型去測試測試資料集。</p>\n<p><img src=\"https://i.imgur.com/YwOunUQ.png\"></p>\n<h2 id=\"分類指標\"><a href=\"#分類指標\" class=\"headerlink\" title=\"分類指標\"></a>分類指標</h2><p>我們剛剛說到，模型預測之後，要與正確答案（Golden Label）進行比對。那麼該怎麼呈現這個比對結果，來決定一個模型的表現好壞呢？我們可以先來看以下這張圖：</p>\n<p><img src=\"https://miro.medium.com/max/1302/1*WEpnGSHzAPnqAl1kAOxhcQ.png\"></p>\n<p>假如模型預測A文本是正面評價，與正確答案相符，那麼我們就會將其納入true positive；假如模型預測B文本是負面評價，正確答案也標記為負面評價，那麼就屬於true negative；假如模型預測C文本是正面評價，但正確答案卻是負面評價，那麼就屬於false positive；若模型判斷D文本是負面評價，但事實上卻是正面評價時，就屬於false negative。</p>\n<p>接著就會產生了以下四種衡量的方式：</p>\n<p><img src=\"https://i.imgur.com/IpBW9uf.png\"></p>\n<p>在做二元分類時，可以透過以上的表格來衡量模型，這矩陣也稱為confusion matrix。而我們可以這麼理解這些指標：</p>\n<ul>\n<li><strong>準確率（accuracy）</strong>：整體預測的正確率</li>\n<li><strong>召回率（recall）</strong>：真正對的答案內，有多少比例是機器模型答對的</li>\n<li><strong>精準率（precision）</strong>：機器模型認為正確的答案內，有多少比例是真正答對的</li>\n<li><strong>F1-Score</strong>：所有衡量標準的調和平均</li>\n</ul>\n<h2 id=\"監督式學習-vs-非監督式學習\"><a href=\"#監督式學習-vs-非監督式學習\" class=\"headerlink\" title=\"監督式學習 vs 非監督式學習\"></a>監督式學習 vs 非監督式學習</h2><ul>\n<li><strong>監督式學習模型</strong><br><strong>監督式學習就是將已經做好完整標記的資料丟進去機器學習模型中，模型可以邊學習，邊以標記答案對比誤差，並一步步修正模型。</strong> 由於需要標記資料，所以前置工程往往需要大量的人工標記，耗費人力較高，但相對於非監督式學習模型來說，監督式學習的解釋力也比較高（想想昨天所說的，解釋力為何重要）。常見的模型種類像是：貝氏分類器、羅吉斯回歸、決策樹。</li>\n<li><strong>非監督式學習模型</strong><br><strong>非監督式學習就是模型透過完全沒有標記的資料，依照關聯性進行歸類、找出潛在規則與模式，以及形成集群。</strong> 這種學習方式雖然不需要大量地標記資料，但卻可能找出從人類的角度來看關聯性極低的不重要特徵，使得分類變得毫無意義。</li>\n</ul>\n<h2 id=\"生成模型-vs-判別模型\"><a href=\"#生成模型-vs-判別模型\" class=\"headerlink\" title=\"生成模型 vs 判別模型\"></a>生成模型 vs 判別模型</h2><p>我們可以透過這個比喻來理解何謂生成式模型，何謂判別式模型。假如說我們今天需要模型判別圖片中的動物是貓貓還是狗狗，生成式模型會先透過訓練資料的照片學習狗與貓的 <strong>整體大致長相</strong> ，接著在測試資料的照片判斷圖中的動物比較接近貓貓還是狗狗，最後再下判斷。判別式模型則是<strong>透過照片學習貓貓有哪些特徵、狗狗有哪些特徵</strong>，例如貓貓有長鬍鬚，狗狗較常吐舌頭，接著再在新圖片中觀察是否有這些觀察到的特徵。</p>\n<p>所以我們大致上可以這麼解釋：</p>\n<ul>\n<li>生成式模型</li>\n</ul>\n<ol>\n<li>先學習訓練資料中整體文本的大致分布方式，並以機率的方式呈現，找出文本資料的機率分佈。</li>\n<li>給予新文本，並判斷新文本是否有類似分佈</li>\n<li>給予分類</li>\n</ol>\n<ul>\n<li>判別式模型</li>\n</ul>\n<ol>\n<li>從訓練文本資料中觀察不同類別的特徵</li>\n<li>從測試文本中觀察是否有相對應的特徵</li>\n<li>給予分類</li>\n</ol>\n<p>好，大致上就是這樣，我們接下來就可以進入機器學習的領域啦！剩下二十天，一起加油！</p>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"【NLP】Day 11: 什麼？妳男友有乾妹妹？那你很大機率被綠了！機器學習：貝氏分類器","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-11-%E4%BB%80%E9%BA%BC%EF%BC%9F%E5%A6%B3%E7%94%B7%E5%8F%8B%E6%9C%89%E4%B9%BE%E5%A6%B9%E5%A6%B9%EF%BC%9F%E9%82%A3%E4%BD%A0%E5%BE%88%E5%A4%A7%E6%A9%9F%E7%8E%87%E8%A2%AB%E7%B6%A0%E4%BA%86%EF%BC%81%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%EF%BC%9A%E8%B2%9D%E6%B0%8F%E5%88%86%E9%A1%9E%E5%99%A8/","content":"<blockquote>\n<p>“You know what possibly means?”<br>“Like probably.”<br>“No, probably means there’s a good chance that we’re going. Possibly means we might, we might not.”<br><strong>Chris Gardener《當幸福來敲門》</strong></p>\n</blockquote>\n<p>我們終於進入機器學習啦！就像每個初學程式的人都一定要印個<code>Hello World!</code>，每個剛入門機器學習程式的同學，一定也是要先從貝式分類器（Naive Bayes classifier）開始啦！如同前面所說，所謂的語言模型（Language modeling）就是將每個詞出現的可能化成機率，並以機率的角度去看待整個語言，那貝氏分類器同樣也是一個基於機率的分類器，不過在了解貝氏分類器之前，我們得先知道什麼是貝氏定理。</p>\n<h2 id=\"貝氏定理\"><a href=\"#貝氏定理\" class=\"headerlink\" title=\"貝氏定理\"></a>貝氏定理</h2><p>大家想必對這四個字可是熟悉又陌生，或許上次見面是高中吧，你心想。只是自己卻又好像沒有很瞭解這是什麼，就像是久久沒聯絡的同學一樣，在一次共同好友的婚禮上見到了，卻不知道要說些什麼，只能說些客套的話。<del>這是什麼JOJO的奇妙比喻</del></p>\n<p>扯遠了。<strong>其實貝氏定理就是在描述在一些已知的條件下，某件事情發生的比率。</strong></p>\n<p>比如說，我們已經知道伴侶的交友圈，以及伴侶的回家時間，跟他&#x2F;她是否有第三者有關，那麼使用貝氏定理，就可以透過得知以上資訊，來推斷你的伴侶外遇的機率。假如今天你是一位異性戀女性，你的男朋友每天晚上都說會載女同事回家，所以每天幾乎都要快十點才會到家，你說你很在意，但是男友卻說他們只是朋友關係。某天，男友說，他要載他<strong>乾妹妹</strong>回家，你問男友說他要載誰，他說就是之前那個同事，但男友一走，卻一直到隔天早上才回家，請問男友有第三者的機率多高？</p>\n<p><img src=\"https://i.imgur.com/DbjYLRq.png\"></p>\n<ul>\n<li><strong>事件A: 你男友有乾妹妹</strong></li>\n<li><strong>事件B: 有第三者介入你們感情</strong></li>\n</ul>\n<p>首先，在確定你男友有第三者之前，對你男友是否有乾妹妹這件事情會有一個基本機率判斷，因此我們稱你男友有乾妹妹的機率為P(A)，也就是事前機率。而在確定你男友有第三者之後，我們會重新判斷你男友是否有乾妹妹的機率。亦即，若你男友有第三者之後，男友有認乾妹妹的機率作為P(A|B)，也就是事後機率。</p>\n<p>相反亦然，在確定你男友是否有乾妹妹之前，會先判斷你男友有第三者的機率，也就是事前機率P(B)。但在確定你男友有乾妹妹之後，會重新判斷你男友在有乾妹妹的前提下，有第三者的機率是多少。也就是說，若你男友認了乾妹妹之後，男友有第三者的機率為P(B|A)，也就是事後機率。</p>\n<!-- $${\\displaystyle P(有乾妹妹\\mid 有第三者)={\\frac {P(有乾妹妹)P(有第三者\\mid 有乾妹妹)}{P(有第三者)}}}$$ -->\n<p><img src=\"https://i.imgur.com/u9L4w5p.png\"></p>\n<h2 id=\"貝式學習\"><a href=\"#貝式學習\" class=\"headerlink\" title=\"貝式學習\"></a>貝式學習</h2><p>至於貝式學習就是要找到一個假設，並以這個假設為前提，找到發生這個事件的最大機率。舉例來說，P(有第三者｜有乾妹妹)&#x3D;0.69，而P(沒有第三者｜有乾妹妹)&#x3D;0.31，貝氏分類器就會將有乾妹妹的特徵分類到「會有第三者」，這是因為0.69 &gt; 0.31。也就是說，在貝式分類器所求的為：<br><img src=\"https://i.imgur.com/0IOgqPN.png\"></p>\n<h2 id=\"自然語言處理上會怎麼運作？\"><a href=\"#自然語言處理上會怎麼運作？\" class=\"headerlink\" title=\"自然語言處理上會怎麼運作？\"></a>自然語言處理上會怎麼運作？</h2><p>如上所述，若今天以分類正負面電影評論為例，我們在進行文本計算時，事前機率P(+)就是所有文章中，正面文章的比例；而事前機率P(-)就是所有文章中，負面文章的比例。<br><img src=\"https://i.imgur.com/oJwaFTQ.png\"><br>假如說今天的測試文本為「電影好看！」，那麼斷詞之後會變成<code>電影/好看</code>。接著若是以詞頻作為特徵的話，就是要計算這兩個詞分別在訓練資料中正面評價出現幾次，以及在負面評價中出現幾次。另外為了避免出現機率為零的狀況，也會將公式進行smoothing。也就是像這樣。我們直接賦予這些機率一個數字。<br><img src=\"https://i.imgur.com/Oajkks2.png\"><br><img src=\"https://i.imgur.com/mN7Aiqf.png\"></p>\n<!-- $$P(+)=0.6$$ -->\n\n<!-- $$P(-)=0.4$$ -->\n\n\n<!-- $$P(「電影」\\mid 正面評價)= {\\frac{「電影」詞頻+1}{正面評價文本字數+詞類總數}} = 0.234$$ -->\n\n<!-- $$P(「好看」\\mid 正面評價)= {\\frac{「電影」詞頻+1}{正面評價文本字數+詞類總數}} = 0.888$$ -->\n\n<!-- $$P(「電影」\\mid 負面評價)= {\\frac{「電影」詞頻+1}{負面評價文本字數+詞類總數}} = 0.323$$ -->\n\n<!-- $$P(「好看」\\mid 負面評價)= {\\frac{「電影」詞頻+1}{負面評價文本字數+詞類總數}} = 0.026$$ -->\n\n<p>要注意的是，以上都是計算這些詞在訓練資料集出現的條件機率。現在要計算測試文本「電影好看！」會是正面還是負面影評的話，就會是這樣算。</p>\n<!-- $$P(+)P(S \\mid +) = 0.6\\times0.234\\times0.888 = 0.125$$ -->\n\n<!-- $$P(-)P(S \\mid -) = 0.4\\times0.323\\times0.026 = 0.003$$ -->\n<p><img src=\"https://i.imgur.com/bwOxPXc.png\"></p>\n<p>最後可以得知，正面評價的機率大於負面評價，因此機器學習模型會將文本分類到正面評價。</p>\n<h2 id=\"問題討論＆優缺點\"><a href=\"#問題討論＆優缺點\" class=\"headerlink\" title=\"問題討論＆優缺點\"></a>問題討論＆優缺點</h2><p>不免俗，文章最後一定都要來討論一下模型的優缺點。貝氏分類器作為早期的分類器，具有<strong>堅實的數學基礎</strong>，也因此<strong>較容易解釋</strong>（記得前面我們提到解釋力有多重要），即使是<strong>面對許多的缺漏值，也可以很好的進行分類</strong>。<strong>處理小資料進行分類的時候，也有很優秀的表現</strong>。但貝氏分類卻有一個致命缺點，也就是<strong>貝氏分類假設屬性間互相獨立</strong>，但往往在現實中卻不是如此，所以後續也有許多改良版的模型，如半樸素貝氏分類器。</p>\n<p>好，貝氏分類就講到這裡，明天要來講羅吉斯回歸囉！</p>\n"},{"title":"【Crawler】Day 28: 其實就是聖誕節大採購嘛！網路爬蟲速成班（下）","url":"/2023/01/08/%E3%80%90Crawler%E3%80%91Day-28-%E5%85%B6%E5%AF%A6%E5%B0%B1%E6%98%AF%E8%81%96%E8%AA%95%E7%AF%80%E5%A4%A7%E6%8E%A1%E8%B3%BC%E5%98%9B%EF%BC%81%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2%E9%80%9F%E6%88%90%E7%8F%AD%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"<blockquote>\n<p>聖誕快樂！你這骯髒的小畜生！<br>《小鬼當家2》</p>\n</blockquote>\n<p>昨天我們把所有寫爬蟲時可能會需要的先備知識都學起來了，今天就要直接進入爬蟲主題啦！以往我在教同學寫網路爬蟲的時候，我都會用一個我自己原創的比喻：<strong>聖誕節大採購</strong>！且聽我娓娓道來吧！</p>\n<h2 id=\"爬蟲出動！\"><a href=\"#爬蟲出動！\" class=\"headerlink\" title=\"爬蟲出動！\"></a>爬蟲出動！</h2><p>過去從來沒有接觸過網路爬蟲的人，可能會覺得爬蟲是一個很難理解的技術，對初學者而言就更不用說了。但其實爬蟲的概念很簡單，就是：</p>\n<blockquote>\n<p>把所有要爬網頁瀏覽一遍，把我們要的資訊整理成結構化資料，接著儲存成我們想要的檔案形式。</p>\n</blockquote>\n<p>好像還是有點抽象。沒關係，我們從了解一個網路的架構開始談起。網頁通常會分成兩種：</p>\n<ul>\n<li>動態網頁</li>\n<li>靜態網頁</li>\n</ul>\n<p>動態網頁不是我們平常會看到的那些有很精美的動畫，才能叫做動態網頁。有時候，一個靜態網頁也可能會有很厲害的動畫。決定一個網頁是靜態還是動態，取決於他是否有連接至資料庫。</p>\n<p>對爬蟲而言，抓取靜態跟動態網頁各自有不一樣的寫法。動態網頁稍微複雜一點，今天先講抓取靜態網頁的網路爬蟲寫法。</p>\n<h3 id=\"網頁架構\"><a href=\"#網頁架構\" class=\"headerlink\" title=\"網頁架構\"></a>網頁架構</h3><p>一個網站的基本架構，通常是 html + CSS (+ javascript)</p>\n<p>首先，我們在網路上看到的所有網站，都是透過一個一個的 html 檔所組成的，這些 html 檔又被稱為標籤語言，這是因為你可以看到這是由很多的 <code>&lt;label&gt;</code> 組成的語言，這些標籤中又各自帶有不同的特性。我們先來看 html 檔長什麼樣子，我通常都會把 html 檔稱為<strong>聖誕樹</strong>。</p>\n<figure class=\"highlight html\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">http-equiv</span>=<span class=\"string\">&quot;X-UA-Compatible&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;viewport&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Document<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>之後，在網頁上有時候會再加上 CSS 檔，來裝飾網頁，讓網頁變得更加繽紛。而我通常會稱這些 CSS 檔為<strong>聖誕樹裝飾品還有裝飾燈</strong>。</p>\n<p>有些網站會有 Javascript ，有些沒有。Javascript 主要是用來讓網站回應訪客的要求，並作出適當回饋；又或者是，可以透過 Javascript 來跟後端的資料庫進行溝通。因此我通常稱 Javascript 為<strong>聖誕樹裝飾燈的開關</strong>。</p>\n<p>所以說，爬蟲就是聖誕節搶購！</p>\n<h3 id=\"為什麼是聖誕節的大採購！？\"><a href=\"#為什麼是聖誕節的大採購！？\" class=\"headerlink\" title=\"為什麼是聖誕節的大採購！？\"></a>為什麼是聖誕節的大採購！？</h3><p>所以，如果要用聖誕節大採購來比喻爬蟲的話，基本上就是這樣：</p>\n<ul>\n<li>你可以將一個一個的網頁當作聖誕樹。</li>\n<li>網頁中的文字、連結，則是聖誕樹的裝飾品，也就是你需要的資料。</li>\n<li>你要做的，就是找到一棵一棵的聖誕樹，接著從一棵一棵聖誕樹的樹枝上找到你要的裝飾品。</li>\n<li>接著，將裝飾品裝到你的籃子裡，打包帶回家。</li>\n</ul>\n<p><img src=\"https://media4.giphy.com/media/l2YWiHWiFHwmbwIso/giphy.gif\"></p>\n<h3 id=\"什麼意思？我們來寫寫程式碼\"><a href=\"#什麼意思？我們來寫寫程式碼\" class=\"headerlink\" title=\"什麼意思？我們來寫寫程式碼\"></a>什麼意思？我們來寫寫程式碼</h3><p>首先，我們要下載需要的套件。請先在終端機執行 <code>pip install</code>，爬蟲會用到的套件為 <code>BeautifulSoup</code></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install BeautifulSoup</span><br></pre></td></tr></table></figure>\n\n<p>如果說今天我們想爬 PTT 電影版的資料作為簡單範例，我們先執行以下程式：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup <span class=\"keyword\">as</span> bs</span><br><span class=\"line\">r = requests.get(<span class=\"string\">&quot;https://www.ptt.cc/bbs/movie/index.html&quot;</span>)</span><br><span class=\"line\">page = bs(r.text, <span class=\"string\">&quot;html.parser&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(page)</span><br></pre></td></tr></table></figure>\n\n<p>就可以得到那個網頁的 html 檔！（注意，如果是動態網頁，很有可能就抓不到需要的內容，那時就需要另外一種寫法）以下貼一部分的輸出即可（我有特別擷取某一段，所以你的輸出會跟我長得不太一樣）：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">&lt;meta charset=&quot;utf-8&quot;/&gt;</span><br><span class=\"line\">&lt;meta content=&quot;width=device-width, initial-scale=1&quot; name=&quot;viewport&quot;/&gt;</span><br><span class=\"line\">&lt;title&gt;看板 movie 文章列表 - 批踢踢實業坊&lt;/title&gt;</span><br><span class=\"line\">&lt;link href=&quot;//images.ptt.cc/bbs/v2.27/bbs-common.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt;</span><br><span class=\"line\">&lt;link href=&quot;//images.ptt.cc/bbs/v2.27/bbs-base.css&quot; media=&quot;screen&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt;</span><br><span class=\"line\">&lt;link href=&quot;//images.ptt.cc/bbs/v2.27/bbs-custom.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt;</span><br><span class=\"line\">&lt;link href=&quot;//images.ptt.cc/bbs/v2.27/pushstream.css&quot; media=&quot;screen&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt;</span><br><span class=\"line\">&lt;link href=&quot;//images.ptt.cc/bbs/v2.27/bbs-print.css&quot; media=&quot;print&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt;</span><br><span class=\"line\">&lt;script async=&quot;&quot; src=&quot;/cdn-cgi/challenge-platform/h/b/scripts/invisible.js?ts=1652421600&quot;&gt;&lt;/script&gt;&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">&lt;div id=&quot;topbar-container&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;bbs-content&quot; id=&quot;topbar&quot;&gt;</span><br><span class=\"line\">&lt;a href=&quot;/bbs/&quot; id=&quot;logo&quot;&gt;批踢踢實業坊&lt;/a&gt;</span><br><span class=\"line\">&lt;span&gt;›&lt;/span&gt;</span><br><span class=\"line\">&lt;a class=&quot;board&quot; href=&quot;/bbs/movie/index.html&quot;&gt;&lt;span class=&quot;board-label&quot;&gt;看板 &lt;/span&gt;movie&lt;/a&gt;</span><br><span class=\"line\">&lt;a class=&quot;right small&quot; href=&quot;/about.html&quot;&gt;關於我們&lt;/a&gt;</span><br><span class=\"line\">&lt;a class=&quot;right small&quot; href=&quot;/contact.html&quot;&gt;聯絡資訊&lt;/a&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;div id=&quot;main-container&quot;&gt;</span><br><span class=\"line\">&lt;div id=&quot;action-bar-container&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;action-bar&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;btn-group btn-group-dir&quot;&gt;</span><br><span class=\"line\">&lt;a class=&quot;btn selected&quot; href=&quot;/bbs/movie/index.html&quot;&gt;看板&lt;/a&gt;</span><br><span class=\"line\">&lt;a class=&quot;btn&quot; href=&quot;/man/movie/index.html&quot;&gt;精華區&lt;/a&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;btn-group btn-group-paging&quot;&gt;</span><br><span class=\"line\">&lt;a class=&quot;btn wide&quot; href=&quot;/bbs/movie/index1.html&quot;&gt;最舊&lt;/a&gt;</span><br><span class=\"line\">&lt;a class=&quot;btn wide&quot; href=&quot;/bbs/movie/index9510.html&quot;&gt;‹ 上頁&lt;/a&gt;</span><br><span class=\"line\">&lt;a class=&quot;btn wide disabled&quot;&gt;下頁 ›&lt;/a&gt;</span><br><span class=\"line\">&lt;a class=&quot;btn wide&quot; href=&quot;/bbs/movie/index.html&quot;&gt;最新&lt;/a&gt;</span><br><span class=\"line\">&lt;div class=&quot;r-ent&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;nrec&quot;&gt;&lt;span class=&quot;hl f2&quot;&gt;6&lt;/span&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;title&quot;&gt;</span><br><span class=\"line\">&lt;a href=&quot;/bbs/movie/M.1652075354.A.59B.html&quot;&gt;[新聞] 民族誌紀錄片工作者，胡台麗逝世&lt;/a&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;meta&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;author&quot;&gt;mysmalllamb&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;article-menu&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;trigger&quot;&gt;⋯&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;dropdown&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;item&quot;&gt;&lt;a href=&quot;/bbs/movie/search?q=thread%3A%5B%E6%96%B0%E8%81%9E%5D+%E6%B0%91%E6%97%8F%E8%AA%8C%E7%B4%80%E9%8C%84%E7%89%87%E5%B7%A5%E4%BD%9C%E8%80%85%EF%BC%8C%E8%83%A1%E5%8F%B0%E9%BA%97%E9%80%9D%E4%B8%96&quot;&gt;搜尋同標題文章&lt;/a&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;item&quot;&gt;&lt;a href=&quot;/bbs/movie/search?q=author%3Amysmalllamb&quot;&gt;搜尋看板內 mysmalllamb 的文章&lt;/a&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;date&quot;&gt; 5/09&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;mark&quot;&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n\n<p>注意，我們現在目錄頁！但我們想要的資料在文章的網頁裡面。所以現在要找的是文章網頁的網址。仔細觀察一篇一篇的文章中，超連結通常會存在前面的標籤中，所以需要透過以下程式碼得到需要的那段 html：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">page.find(<span class=\"string\">&quot;div&quot;</span>, <span class=\"string\">&quot;r-ent&quot;</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;div class=&quot;r-ent&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;nrec&quot;&gt;&lt;span class=&quot;hl f2&quot;&gt;6&lt;/span&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;title&quot;&gt;</span><br><span class=\"line\">&lt;a href=&quot;/bbs/movie/M.1652075354.A.59B.html&quot;&gt;[新聞] 民族誌紀錄片工作者，胡台麗逝世&lt;/a&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;meta&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;author&quot;&gt;mysmalllamb&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;article-menu&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;trigger&quot;&gt;⋯&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;dropdown&quot;&gt;</span><br><span class=\"line\">&lt;div class=&quot;item&quot;&gt;&lt;a href=&quot;/bbs/movie/search?q=thread%3A%5B%E6%96%B0%E8%81%9E%5D+%E6%B0%91%E6%97%8F%E8%AA%8C%E7%B4%80%E9%8C%84%E7%89%87%E5%B7%A5%E4%BD%9C%E8%80%85%EF%BC%8C%E8%83%A1%E5%8F%B0%E9%BA%97%E9%80%9D%E4%B8%96&quot;&gt;搜尋同標題文章&lt;/a&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;item&quot;&gt;&lt;a href=&quot;/bbs/movie/search?q=author%3Amysmalllamb&quot;&gt;搜尋看板內 mysmalllamb 的文章&lt;/a&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;date&quot;&gt; 5/09&lt;/div&gt;</span><br><span class=\"line\">&lt;div class=&quot;mark&quot;&gt;&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br><span class=\"line\">&lt;/div&gt;</span><br></pre></td></tr></table></figure>\n\n<p><code>.find</code>雖然可以找到第一篇文章的部分 html 檔可是我們需要的是所有文章，這時可以用另外一個函式 <code>.find_all()</code> 回傳的就會是一個<code>list</code>，這裡就不貼輸出了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">page.find_all(<span class=\"string\">&quot;div&quot;</span>, <span class=\"string\">&quot;r-ent&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>但記得，我們需要的是網址，但並不是每個標籤都有我們想要的網址，這時 <code>try...except...</code>就派上用場了。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">divs = page.find_all(<span class=\"string\">&quot;div&quot;</span>, <span class=\"string\">&quot;r-ent&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> div <span class=\"keyword\">in</span> divs:</span><br><span class=\"line\">  <span class=\"keyword\">try</span>:</span><br><span class=\"line\">    href = div.find(<span class=\"string\">&#x27;a&#x27;</span>)[<span class=\"string\">&#x27;href&#x27;</span>]</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(href)</span><br><span class=\"line\">  <span class=\"keyword\">except</span>:</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<p>這時，我們就可以得到網址的「後半部分」</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">/bbs/movie/M.1652075354.A.59B.html</span><br><span class=\"line\">/bbs/movie/M.1652076208.A.B5C.html</span><br><span class=\"line\">/bbs/movie/M.1652077025.A.469.html</span><br><span class=\"line\">/bbs/movie/M.1652077434.A.D63.html</span><br><span class=\"line\">/bbs/movie/M.1652077492.A.F65.html</span><br><span class=\"line\">/bbs/movie/M.1652078673.A.2B7.html</span><br><span class=\"line\">/bbs/movie/M.1652079315.A.0AC.html</span><br><span class=\"line\">/bbs/movie/M.1652079447.A.571.html</span><br><span class=\"line\">/bbs/movie/M.1652081468.A.575.html</span><br><span class=\"line\">/bbs/movie/M.1652081735.A.D22.html</span><br><span class=\"line\">/bbs/movie/M.1652083107.A.CED.html</span><br><span class=\"line\">/bbs/movie/M.1652086174.A.524.html</span><br><span class=\"line\">/bbs/movie/M.1652087613.A.7D7.html</span><br><span class=\"line\">/bbs/movie/M.1652088346.A.D6C.html</span><br><span class=\"line\">/bbs/movie/M.1652089057.A.132.html</span><br><span class=\"line\">/bbs/movie/M.1652089740.A.BA4.html</span><br><span class=\"line\">/bbs/movie/M.1652089754.A.A1E.html</span><br><span class=\"line\">/bbs/movie/M.1652092096.A.6BC.html</span><br><span class=\"line\">/bbs/movie/M.1652097109.A.975.html</span><br><span class=\"line\">/bbs/movie/M.1630756788.A.1FE.html</span><br><span class=\"line\">/bbs/movie/M.1636002497.A.7EC.html</span><br></pre></td></tr></table></figure>\n\n<p>重新整理一遍，讓輸出的結果是完整網址。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">divs = page.find_all(<span class=\"string\">&quot;div&quot;</span>, <span class=\"string\">&quot;r-ent&quot;</span>)</span><br><span class=\"line\">baseURL = <span class=\"string\">&quot;https://www.ptt.cc&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> div <span class=\"keyword\">in</span> divs:</span><br><span class=\"line\">  <span class=\"keyword\">try</span>:</span><br><span class=\"line\">    href = baseURL+div.find(<span class=\"string\">&#x27;a&#x27;</span>)[<span class=\"string\">&#x27;href&#x27;</span>]</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(href)</span><br><span class=\"line\">  <span class=\"keyword\">except</span>:</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<p>現在試著把他包成函式：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">indexGrabber</span>(<span class=\"params\">inputURL</span>):</span><br><span class=\"line\">  r = requests.get(inputURL)</span><br><span class=\"line\">  page = bs(r.text, <span class=\"string\">&quot;html.parser&quot;</span>)</span><br><span class=\"line\">  divs = page.find_all(<span class=\"string\">&quot;div&quot;</span>, <span class=\"string\">&quot;r-ent&quot;</span>)</span><br><span class=\"line\">  UrlLIST = []</span><br><span class=\"line\">  baseURL = <span class=\"string\">&quot;https://www.ptt.cc&quot;</span></span><br><span class=\"line\">  <span class=\"keyword\">for</span> div <span class=\"keyword\">in</span> divs:</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">      href = baseURL + div.find(<span class=\"string\">&#x27;a&#x27;</span>)[<span class=\"string\">&#x27;href&#x27;</span>]</span><br><span class=\"line\">      UrlLIST.append(href)</span><br><span class=\"line\">    <span class=\"keyword\">except</span>:</span><br><span class=\"line\">      <span class=\"keyword\">pass</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> UrlLIST</span><br><span class=\"line\"></span><br><span class=\"line\">indexGrabber(<span class=\"string\">&quot;https://www.ptt.cc/bbs/movie/index9501.html&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>我們接下來抓取目錄頁的網址，這樣才能將前面的函式用上。</p>\n<p><img src=\"https://i.imgur.com/pwqViJm.png\"></p>\n<p>可以發現，一個一個的目錄頁，是由後面那段數字組成，所以到時候我們用<code>for ... in range():</code> 寫就可以了。（大部分目錄頁都會是用這種寫法，仔細觀察！）</p>\n<h3 id=\"要找前面那些標籤位置好像很容易？\"><a href=\"#要找前面那些標籤位置好像很容易？\" class=\"headerlink\" title=\"要找前面那些標籤位置好像很容易？\"></a>要找前面那些標籤位置好像很容易？</h3><p>前面要找那些資訊的時候，你可能發現在參數中，我加上了<code>(&quot;div&quot;, &quot;r-ent&quot;)</code> 這是因為，PTT的結構相對簡單，可是如果我們碰到結構複雜的網頁，該怎麼辦？我們可以用以下工具，你可以根據你的瀏覽器選擇對應的套件：</p>\n<ul>\n<li>Chrome: <a href=\"https://chrome.google.com/webstore/detail/chropath/ljngjbnaijcbncmcnjfhigebomdlkcjo?utm_source=chrome-ntp-icon\">ChroPath</a><ul>\n<li>(網頁任意處右鍵) -&gt; 檢查 -&gt; 元素 -&gt; (右側「樣式」列的最右側，若沒看到則按”&gt;&gt;”) -&gt; (檢查元素) -&gt; (複製CSS或XPath)</li>\n</ul>\n</li>\n<li>Firefox: <a href=\"https://addons.mozilla.org/zh-TW/firefox/addon/selectorshub/?utm_source=addons.mozilla.org&utm_medium=referral&utm_content=search\">SelectorsHub</a><ul>\n<li>(網頁任意處右鍵) -&gt; 檢測 -&gt; 檢測器 -&gt; (右側「樣式」列的最右側，若沒看到則按向下箭頭樣式) -&gt; (檢查元素) -&gt; (複製CSS或XPath)</li>\n<li>Alternatives: 移動游標至欲檢測之目標上方 -&gt; (右鍵) -&gt; SelectorsHub -&gt; “Copy Rel cssSelector” or “Copy Rel XPath”</li>\n</ul>\n</li>\n</ul>\n<p>現在來練習 imdb 的網站，我們要將標題跟分數抓下來，透過以上方式，我們可以找到標題跟分數的位置如下：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">titlePath = <span class=\"string\">&quot;.sc-b73cd867-0.eKrKux&quot;</span></span><br><span class=\"line\">scorePath = <span class=\"string\">&quot;div[class=&#x27;sc-db8c1937-0 eGmDjE sc-94726ce4-4 dyFVGl&#x27;] span[class=&#x27;sc-7ab21ed2-1 jGRxWM&#x27;]&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>那我們執行跟前面一樣的動作：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup <span class=\"keyword\">as</span> bs</span><br><span class=\"line\">r = requests.get(<span class=\"string\">&quot;https://www.imdb.com/title/tt3783958/?ref_=fn_al_tt_1&quot;</span>)</span><br><span class=\"line\">page = bs(r.text, <span class=\"string\">&quot;html.parser&quot;</span>)</span><br><span class=\"line\">rawTitle = page.select(titlePath)</span><br><span class=\"line\">rawScore = page.select(scorePath)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(rawTitle)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(rawScore)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(rawTitle))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(rawScore))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">[&lt;h1 class=&quot;sc-b73cd867-0 eKrKux&quot; data-testid=&quot;hero-title-block__title&quot; textlength=&quot;10&quot;&gt;La La Land&lt;/h1&gt;]</span><br><span class=\"line\">[&lt;span class=&quot;sc-7ab21ed2-1 jGRxWM&quot;&gt;8.0&lt;/span&gt;]</span><br><span class=\"line\">&lt;class &#x27;list&#x27;&gt;</span><br><span class=\"line\">&lt;class &#x27;list&#x27;&gt;</span><br></pre></td></tr></table></figure>\n\n<p>可以發現輸出的資料型態都是 <code>list</code>。所以要得到我們需要的資訊，除了指定索引值之外，需要再加上<code>.text</code>，告訴爬蟲，我們只需要中間的字串即可。我們可以這麼做：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">title = rawTitle[<span class=\"number\">0</span>].text</span><br><span class=\"line\">score = rawScore[<span class=\"number\">0</span>].text</span><br><span class=\"line\"><span class=\"built_in\">print</span>(title)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(score)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">La La Land</span><br><span class=\"line\">8.0</span><br></pre></td></tr></table></figure>\n\n<p>好，寫到這邊，大家應該都對爬蟲有概念了，若有任何問題可以一起來討論喔！我們明天見。</p>\n","categories":["網路爬蟲","自然語言處理"],"tags":["自然語言處理","程式工具"]},{"title":"【NLP】Day 12: 豬耳朵餅乾跟機器學習也有關係？機器學習：羅吉斯回歸 ","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-12-%E8%B1%AC%E8%80%B3%E6%9C%B5%E9%A4%85%E4%B9%BE%E8%B7%9F%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E4%B9%9F%E6%9C%89%E9%97%9C%E4%BF%82%EF%BC%9F%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%EF%BC%9A%E7%BE%85%E5%90%89%E6%96%AF%E5%9B%9E%E6%AD%B8/","content":"<blockquote>\n<p>麵粉、砂糖、水、植物油為主，其它成份則視做法及口感需求而有不同。餅乾之所以會出現螺旋線，這是由於兩種顏色的麵糰桿成皮狀，再將彼此疊合而捲成柱狀，以刀切割成每一片厚度相當的餅乾，就可以看出每一片餅乾都帶有螺旋線，經過烘烤後成形，即可食用。<br>維基百科《螺仔餅》</p>\n</blockquote>\n<p>我先承認我真的沒梗，引言就找這個了XD</p>\n<p>昨天我們講到了貝氏分類器，希望大家都有稍微理解機器學習模型大概會是以什麼方式呈現。我們在進入羅吉斯回歸的模型之前，先來回顧一下所謂的機率性機器學習分類器（probabilistic machine learning classifier）的大致組成吧，其實幾乎所有的機器學習模型都是依循這樣的模式在進行分類的，即便是神經網路也是幾乎雷同，而自然語言處理的模型也不排除在外。</p>\n<h2 id=\"模型架構\"><a href=\"#模型架構\" class=\"headerlink\" title=\"模型架構\"></a>模型架構</h2><ul>\n<li><strong>特徵輸入</strong><br>將資料整理成機器學習模型可以理解的形式，首先就是要將這些文字轉換成數字，相信大家都還記得。那其實這種所謂「不同種將文字轉成數字」的方式，也有個專有名詞，稱為<strong>特徵工程（feature engineering）</strong>，特徵工程所得之資料即可作為特徵輸入至模型。</li>\n<li><strong>換算機率</strong><br>完成前面的特徵工程之後，會得到一系列的特徵相關數字，由於這些數字並不是模型的理想輸出（因為記得前面所提模型需要輸出的是<em>分類的機率</em>）所以會經過一系列的數學運算過後，得到最後為0~1之間的機率呈現。</li>\n<li><strong>最大化與正確答案相符的可能性（Log-likelihood &amp; Loss function）</strong><br>接著得到的機率有可能也不會是最好的答案，因為我們需要最小化所謂的損失函數，那要最小化的話，可以透過梯度下降的方式來求得最佳解。<ul>\n<li>損失函數</li>\n<li>梯度下降</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"羅吉斯回歸\"><a href=\"#羅吉斯回歸\" class=\"headerlink\" title=\"羅吉斯回歸\"></a>羅吉斯回歸</h2><h3 id=\"特徵輸入\"><a href=\"#特徵輸入\" class=\"headerlink\" title=\"特徵輸入\"></a>特徵輸入</h3><p>在自然語言處理的領域中，除了可以透過<strong>直覺</strong>來判斷文本中可能的特徵之外，<strong>不同領域的文本</strong>也會有他們各自的特徵（比如說，前一陣子很有名的霸社，或者是將官首文化中的文本，可能就會有屬於其次文化的特殊語言特徵）；另外，近期流行的自然語言處理方式，也會透過先前所說的 <strong>Word embedding</strong> 的方式作為特徵加入BERT語言模型中。</p>\n<h3 id=\"利用特徵計算原始分數\"><a href=\"#利用特徵計算原始分數\" class=\"headerlink\" title=\"利用特徵計算原始分數\"></a>利用特徵計算原始分數</h3><p>經過特徵工程之後，我們會得到一串數字（或是向量vector）羅吉斯回歸的第一步，就是先將這些向量x與權重 w（weight）進行內積（dot product）<del>是不是又出現另外一個熟悉的名詞？</del> ，內積過後得到的數字再加上 b （bias term, a.k.a intercept），這是為了避免有些過度表現的特徵影響了模型的判斷。</p>\n<p><img src=\"https://i.imgur.com/q26xFAQ.png\"></p>\n<p>我會稱這段過程為<strong>計算原始分數</strong>。現在說考學測應該還不會暴露年齡吧？你可以這麼理解，我們計算的這個原始分數代表<strong>文章是正面或是負面的可能「程度」</strong>，就像是學測考完都會有一個「原始分數」。但這個所謂的程度或是原始分數，不是我們想要的結果，對吧？因為我們想要的是<strong>這個文本的可能是正面或是負面的「機率」是多少</strong>，就像是我們想要知道學測的「級分」，因為「級分」的高低才真正代表表現的好壞。為了要讓模型輸出機率，會再將這個「原始分數」加入激勵函數（activation function）將輸出數值轉換成機率。</p>\n<h3 id=\"利用激勵函數換算機率\"><a href=\"#利用激勵函數換算機率\" class=\"headerlink\" title=\"利用激勵函數換算機率\"></a>利用激勵函數換算機率</h3><p>經過前面的計算得到了原始分數之後，我們需要將這段原始分數加入激勵函數來輸出機率。激勵函數有很多種，其中包括今天會介紹的Sigmoid以及softmax，之後會再介紹另外兩種。同樣地，如同先前所說，這些函數各有優缺點，我們在下面來一一討論：</p>\n<h4 id=\"Sigmoid-function\"><a href=\"#Sigmoid-function\" class=\"headerlink\" title=\"Sigmoid function\"></a>Sigmoid function</h4><p><img src=\"https://i.imgur.com/28S02CU.png\"><br><img src=\"https://i.imgur.com/2wk2nWj.png\"><br>Sigmoid函數是最基礎，也是用來處理二元分類最常用的激勵函數。而Sigmoid activation function有幾個好處：</p>\n<ol>\n<li>Sigmoid可以將「原始分數」<strong>壓到[0, 1]之間</strong>，正好也是我們所需要的機率形式</li>\n<li>在接近0以及接近1的地方，可以發現函數圖形逐漸趨平。這代表<strong>若有任何離群值（outlier），Sigmoid函數也可以把他壓縮到接近0或1，但卻又不會超過</strong>。</li>\n<li><strong>可微分</strong>（differentiable），在後續做最佳解時會是一個有利的工具。</li>\n</ol>\n<h4 id=\"softmax-function\"><a href=\"#softmax-function\" class=\"headerlink\" title=\"softmax function\"></a>softmax function</h4><p>softmax跟Sigmoid不同的地方在於，這是做多元分類時常用的激勵函數，只是差別在於前者只會給一個機率，而softmax則是會給每個標籤相對應機率，然後加總起來都會等於1。<br><img src=\"https://i.imgur.com/K8zftXE.png\"><br>所以假如說加入「原始分數」，就會是這樣的結果。<br><img src=\"https://i.imgur.com/2ly0KbI.png\"></p>\n<h3 id=\"最大化與正確答案相符的可能性\"><a href=\"#最大化與正確答案相符的可能性\" class=\"headerlink\" title=\"最大化與正確答案相符的可能性\"></a>最大化與正確答案相符的可能性</h3><p>前面你可能會發現，咦？那權重是怎麼來的？這就要先從Loss function開始講起：</p>\n<h4 id=\"損失函數（loss-function）\"><a href=\"#損失函數（loss-function）\" class=\"headerlink\" title=\"損失函數（loss function）\"></a>損失函數（loss function）</h4><p>損失函數其實就是為了要評估<strong>實際答案跟預測答案之間的差距</strong>，而要找出這段差距，首先要先進行條件最大機率估算（conditional maximum likelihood estimation），聽起來很複雜，但其實就是要將<strong>預測正確的可能性最大化</strong>，也就是所謂的log-likelihood。而權重就會在每一次的計算過程中不斷調整，直到找到最佳解。在這邊不會有複雜的數學推演，我只會簡單介紹計算的邏輯。<br><img src=\"https://i.imgur.com/PJ2renE.png\"><br>這裡是一個最大化的最佳解問題。那為了將它變成損失函數，也就是最小化的最佳解問題，在這裡會乘上一個負號<br><img src=\"https://i.imgur.com/1PnzQsf.png\"></p>\n<h4 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h4><p>那要解這個最小化問題，在這邊會使用的方法就是梯度下降（gradient descent）。</p>\n<p>這名字聽起來好像很可怕，<del>用手算確實可怕，回想我大學在資管系修作業研究的辛酸血淚史（一抹淚）</del>，簡單來說，梯度下降法是一種找出最小化最佳解的一種演算法，意味著找出在函數空間中，斜率（slope）往上升程度最大的方向，並往這個方向的<strong>反方向</strong>前進。是不是不知道我在講什麼？我們馬上來看看一維簡化圖。</p>\n<p><img src=\"https://i.imgur.com/kW3hAki.png\"></p>\n<p>首先我們可以先對 w1 微分（這就是為什麼可微分很重要），找出在w1這個點的切線斜率是正亦或是負。若是正（切線方向為<code>/</code>）則代表應該要往左前進。而圖中 w1 的斜率是負的（切線方向為<code>\\</code>），所以方向則是往右，方能找到最佳最小值。</p>\n<p>但實務上不可能會有這種只有一維的資料吧！所以事實上函數圖形長得會像這樣：<br><img src=\"https://i.imgur.com/tb6k3ox.png\"></p>\n<p>我自己是覺得很像螺仔餅啦，不覺得嗎？<del>我也是去查知道這餅乾的正式名稱</del></p>\n<p><img src=\"https://cc.tvbs.com.tw/img/program/upload/2021/05/27/20210527131604-6951a668.jpg\"><br>圖片來源：<a href=\"https://woman.tvbs.com.tw/lifestyle/24209\">TVBS</a></p>\n<p>那因為 logistic regression 是一個凸函數（convex），而凸函數只會有一個最小值，所以 logistic regression 一定有解。但神經網路<em>不是凸函數（non-convex）</em>，所以在進行梯度下降時，就有可能困在同一個地方出不來，因此找不到最佳解。</p>\n<h2 id=\"問題與討論\"><a href=\"#問題與討論\" class=\"headerlink\" title=\"問題與討論\"></a>問題與討論</h2><p>最後來比較看看貝氏分類模型跟羅吉斯回歸模型。首先，<strong>貝氏分類模型過度強調事件的獨立性，而這代表每個輸入的特徵都是獨立的。</strong> 這會有什麼問題？假如我們輸入了兩次一模一樣的特徵，貝氏分類會將其視為兩個 <strong>完全不同</strong> 的特徵，並將這些特徵彼此相乘，最後結果導致模型過度側重這個特徵。但相對的，羅吉斯回歸就不會有這個問題，因為權重會依照特徵進行調整，將權重分配到高度相關的特徵。</p>\n<p>但這就代表貝氏分類一無是處嗎？倒也不然。貝氏分類在處理少量資料時，就可以有非常好的表現。且貝氏分類器簡單，訓練速度較快，因此貝氏分類器也是輸人不輸陣的喔！</p>\n<p>這裡就再次回應先前所說，沒有什麼最好的方法，只有最適合的方法。最重要的，是了解需求後，找出最適合任務的處理方式，才是最重要的！</p>\n<p>好，今天講到這裡，明天要進入深度學習的領域囉！</p>\n<p>資料與圖片來源：<br><a href=\"https://web.stanford.edu/~jurafsky/slp3/5.pdf\">Speech and Language Processing</a></p>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"【NLP】Day 13: 可不可以再深一點點就好？深度學習基礎知識：你需要知道的這些那些","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-13-%E5%8F%AF%E4%B8%8D%E5%8F%AF%E4%BB%A5%E5%86%8D%E6%B7%B1%E4%B8%80%E9%BB%9E%E9%BB%9E%E5%B0%B1%E5%A5%BD%EF%BC%9F%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E5%9F%BA%E7%A4%8E%E7%9F%A5%E8%AD%98%EF%BC%9A%E4%BD%A0%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E9%80%99%E4%BA%9B%E9%82%A3%E4%BA%9B/","content":"<blockquote>\n<p>人類與動物的學習大都是非監督學習。所以說如果「智慧」是一塊蛋糕，那麼非監督學習才是蛋糕本體，監督學習則是那層糖霜，強化學習不過是上頭的櫻桃。<br>Most of human and animal learning is unsupervised learning. If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake.<br><strong>楊立昆 Yann LeCun</strong></p>\n</blockquote>\n<p>不知道昨天大家有沒有看到一則<del>偏宅</del>的新聞，就是星際大戰中達斯維達的配音員詹姆士・厄爾・瓊斯（James Earl Jones），從今開始正式不再為達斯維達配音。我們再也沒有機會聽到本人講那經典的台詞：</p>\n<p><img src=\"https://media.giphy.com/media/xT9DPpf0zTqRASyzTi/giphy.gif\"></p>\n<p><del>喔，對了，防雷警報，達斯維達其實是路克・天行者的老爸。（誰在乎？）</del></p>\n<p>但是新聞還說，雖然本人不再配音，我們在電影中仍然還是可以聽得到他那經典的呼氣聲，並感受它強大的原力？為什麼？這一切都拜那個離我們又遙遠又接近的深度學習技術所賜。</p>\n<p>其實不僅僅是語音合成，文本處理上現在也很流行利用深度學習的技術來完成任務。今天，就讓我們先來簡單大概了解什麼是深度學習，<del>還有大家在吵什麼</del>。</p>\n<h2 id=\"釐清名詞觀念\"><a href=\"#釐清名詞觀念\" class=\"headerlink\" title=\"釐清名詞觀念\"></a>釐清名詞觀念</h2><p>在開始之前，我們得要先釐清名詞觀念。從以前到現在，我們已經學到了好多名詞：人工智慧、機器學習、深度學習、監督式學習、非監督式學習，那這些名詞之間到底有什麼不一樣呢？我們可以這樣理解：</p>\n<p><img src=\"https://blogs.nvidia.com/wp-content/uploads/2016/07/Deep_Learning_Icons_R5_PNG.jpg.png.webp\"><br>Source: <a href=\"https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/\">NVIDIA</a></p>\n<p>人工智慧是一個「雨傘術語」（umbrella term），而機器學習則是人工智慧底下最廣為人知的一個分支。主要目的在於設計和分析一些讓電腦可以自動「學習」的演算法，並利用規律對未知資料進行預測。機器學習又是另外一個「雨傘術語」，因為機器學習又根據演算法分為很多種不同的學習方法，我們這邊提到了監督式學習跟非監督式學習，如果沒看過我之前針對兩者差別進行的介紹，可以去看這篇 **<a href=\"https://ithelp.ithome.com.tw/articles/10298197\">Day 10: 進入偉大航道！機器學習基礎知識：你需要知道的這些那些</a>**。而深度學習是機器學習的其中一種方式。</p>\n<h2 id=\"什麼是深度學習？\"><a href=\"#什麼是深度學習？\" class=\"headerlink\" title=\"什麼是深度學習？\"></a>什麼是深度學習？</h2><p>深度學習作為機器學習的其中一個分支，若要「沒那麼精確」但要淺顯易懂的解釋，你可以將深度學習理解成「比較深的神經網路」目的在於將資料放進多個節點，經過線性以及非線性的轉換，從原始資料中自動抽取足以代表資料的特徵，自動抽取特徵這點又被稱為<strong>表徵學習</strong>（representation learning），最終都可以得到非常優秀的結果。</p>\n<p>類神經網路通常會有好幾層，每一層會有許多個節點（perceptron），就像人類的神經系統中，神經元與神經元之間的連結一樣，節點與節點之間也會彼此傳遞資訊，之後再經過激勵函數（activation function）轉換成輸出。</p>\n<h3 id=\"為什麼深度學習逐漸變成主流？\"><a href=\"#為什麼深度學習逐漸變成主流？\" class=\"headerlink\" title=\"為什麼深度學習逐漸變成主流？\"></a>為什麼深度學習逐漸變成主流？</h3><p>但其實深度學習的歷史已經非常悠久了。早在六七零年代，就有電腦科學家嘗試重現生物神經網路的架構，希望電腦可以達到跟人類一樣的優秀學習能力。但礙於當時電腦的運算能力還跟不上理論，再加上數位化的資料並不多，無法提供神經網路好的學習資源，所以當時神經網路仍無法展露頭角。</p>\n<p>但現在拜於網路與資訊技術的進步，有越來越多的數位資料可以提供這些神經網路小試身手，除此之外，逐漸成熟的GPU演算技術也幫助了神經網路的運算，速度也有大幅成長（但仍然很慢），天時地利人和之下，神經網路這個名詞才漸漸重新浮出檯面。</p>\n<h3 id=\"跟前面提到的機器學習有什麼不一樣？\"><a href=\"#跟前面提到的機器學習有什麼不一樣？\" class=\"headerlink\" title=\"跟前面提到的機器學習有什麼不一樣？\"></a>跟前面提到的機器學習有什麼不一樣？</h3><p><img src=\"https://i.imgur.com/jQ2IzW7.png\"><br>一般來說，像是我們先前所說的貝氏分類以及羅吉斯回歸這樣的監督式機器學習，都會需要我們手動加入特徵，而這些特徵都是透過我們人類觀察而得的；人工神經網路乃至於深度學習，其實並不需要我們人類進行特徵工程，深度學習模型會自己學習特徵並自動分類，最後輸出分類，但模型找的特徵可能跟我們一般所認知的特徵會有點差距就是了，也就是說，我們會看不懂神經網路找到的特徵。因此，人工神經網路也是黑盒子模型的一種，這部分後面我們會再提。</p>\n<p><img src=\"https://miro.medium.com/max/1400/0*LFr2lyb_FzhwT8he.png\"></p>\n<p>也就是說，資料越多，神經網路所能夠學的特徵就越多，深度學習模型的表現也就越好，好過過去監督式的機器學習模型。</p>\n<h2 id=\"可不可以再深一點點就好？越深越好？\"><a href=\"#可不可以再深一點點就好？越深越好？\" class=\"headerlink\" title=\"可不可以再深一點點就好？越深越好？\"></a>可不可以再深一點點就好？越深越好？</h2><p>深度學習的一個很常被討論的問題就是，既然叫深度學習，是不是越深就越好？現代知名電腦科學家楊立昆（Yann LeCun）在2008年的論文 <a href=\"http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf\">Scaling Learning Algorithms towards AI</a> 中就提到，在特定資料量底下，淺層架構的神經網路運算上或許沒辦法達到它最好的表現，深層才能達到最大效益。台大李宏毅老師在機器學習課程中也有提到，神經網路好像疊越多層、神經元越多，表現就越好。那我們就這樣一直無腦疊下去就好了嗎？模型就會越好嗎？<br><img src=\"https://i.imgur.com/hSJwdHz.png\"></p>\n<h3 id=\"深度的特性反而是一把雙面刃？\"><a href=\"#深度的特性反而是一把雙面刃？\" class=\"headerlink\" title=\"深度的特性反而是一把雙面刃？\"></a>深度的特性反而是一把雙面刃？</h3><p>後面我們會提到加深神經網路在優化上可能會發生的問題，比如像是神經網路的運算時間計算量大、耗時，<a href=\"https://proceedings.neurips.cc/paper/1988/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf\">即使是一個兩層三節點的神經網路，時間消耗仍然為NP-Complete</a>，NP-Complete就是運行複雜程度的等級，詳細可以來看這篇前輩的<a href=\"https://ithelp.ithome.com.tw/articles/10237364\">文章</a>。另外，也因為梯度消失以及梯度爆炸的現象（後面文章會介紹），也會導致神經網路找不到最佳解，你可以回想前面我們說羅吉斯迴歸為凸函數，找得到最小值；但神經網路為非凸函數，有可能只找到局部最小值，那該怎麼辦呢？</p>\n<p>答案是，不怎麼辦！因為我們不能保證模型找到最佳解，但是找到的最小值已經非常逼近最佳解，也就是說已經夠好到足以幫助人類完成許多任務了！</p>\n<p>所以其實還是老話一句，越深越好？還是要看任務是什麼，找到最適合的方法才是最重要的。</p>\n"},{"title":"【NLP】Day 14: 神經網路也會神機錯亂？不，只會精神錯亂...深度學習：前饋神經網路","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-14-%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E4%B9%9F%E6%9C%83%E7%A5%9E%E6%A9%9F%E9%8C%AF%E4%BA%82%EF%BC%9F%E4%B8%8D%EF%BC%8C%E5%8F%AA%E6%9C%83%E7%B2%BE%E7%A5%9E%E9%8C%AF%E4%BA%82-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%EF%BC%9A%E5%89%8D%E9%A5%8B%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/","content":"<blockquote>\n<p>在夜城，不要相信任何人，受到背叛也是自己的錯…<br><strong>琦薇《電馭叛客：邊緣行者》</strong></p>\n</blockquote>\n<p>我很喜歡賽博龐克風格的相關作品，像是銀翼殺手、攻殼機動隊，都是我很愛的作品。最近在看《電馭叛客：邊緣行者》（好看，推薦大家去看），在動畫中，或是說在賽博龐克風格作品中的人物，為了要達到更高更強大的身體能力，會用電子零件（像是電子手臂，或是增快移動速度的軍事武器植入物等）將身體的一部分改造。而在電馭叛客中有個名詞：<em>神機錯亂</em>，意指若身上已經乘載過多改裝與植入，這些電子組件會侵蝕人性，並擾亂心智，到最後迷失自我。</p>\n<p>最近在寫神經網路的我，突然覺得神經網路跟神機錯亂真像。在一開始，疊了一層兩層的神經網路，得到了還算可以（夠用）的結果，但為了要達到更高的準確度、更好的表現，疊了更多層的神經網路上去，卻在梯度下降找最佳解時，碰到梯度爆炸或是梯度消失的問題，最後就一直徘徊在局部最小值的谷底出不來，然後他就停止學習了，<strong>不就很像神機錯亂嗎？</strong></p>\n<p><img src=\"https://cdn.oneesports.gg/cdn-data/2022/07/CyberpunkEdgerunners_hack.jpg\"></p>\n<p>扯遠了<del>不可能每次都扯遠吧</del>，今天首先要介紹的是前饋神經網路，若說機器學習的第一個模型是貝氏分類模型，那麼深度學習就一定要從前饋神經網路開始講起。首先會先從神經網路的基本結構開始，一直到自然語言處理實務上的操作，話不多說，就讓我們開始吧！</p>\n<h2 id=\"神經網路的結構\"><a href=\"#神經網路的結構\" class=\"headerlink\" title=\"神經網路的結構\"></a>神經網路的結構</h2><p>神經網路的基本架構大致上可以分成三個部分：</p>\n<ul>\n<li><strong>輸入層（input layer）</strong>：初步資料會先經過輸入層進行處理。在自然語言處理中通常會是傳遞給embedding layer的one-hot vector。</li>\n<li><strong>隱藏層（hidden layer）</strong>：隱藏層可以有一層以上，在隱藏層中，會透過「權重」計算輸出。通常隱藏層會篩選出多餘的資訊，並將所習得之重要資訊傳遞給下一層隱藏層。</li>\n<li><strong>輸出層（output layer）</strong>：學習成果，通常會是以機率的形式呈現。</li>\n</ul>\n<h3 id=\"神經元\"><a href=\"#神經元\" class=\"headerlink\" title=\"神經元\"></a>神經元</h3><p>這些神經層中，是由一個一個的節點組成，這些節點有時候也會被稱為 <strong>「神經元」（neuron）</strong> ，這些節點其實就是將<strong>特徵乘上權重</strong>，再加上一個偏項b，那麼很多個節點所組成的神經網路，事實上也就是很多個函式所組成的函式組！</p>\n<!-- ![](https://i.imgur.com/u7qOiHj.png) -->\n<p><img src=\"https://i.imgur.com/zXooHW7.png\"></p>\n<p>有沒有覺得很熟悉，因為這個計算方式跟羅吉斯迴歸<strong>一模模一樣樣！！！</strong> 也就是當時在羅吉斯迴歸時所說的<strong>計算原始分數</strong>，再將我們計算的z值，也就是原始分數，放入<strong>激勵函數</strong>，將線性解轉換成非線性解，因為有些狀況之下，線性解並無法解決。</p>\n<p><img src=\"https://i.imgur.com/1BPTrAt.png\"></p>\n<p>比如說像以上這種狀況，要畫一條線把藍點跟白點分開，左邊數過來第一張圖跟第二張圖都可以只利用一條線將兩者區別，但難保現實中會有第三張圖，這時候就會需要非線性解來幫助我們解決這種類型的問題。</p>\n<h3 id=\"激勵函數\"><a href=\"#激勵函數\" class=\"headerlink\" title=\"激勵函數\"></a>激勵函數</h3><p>我們在前幾天有討論過Sigmoid以及Softmax，這裡就先不贅述，若遺漏掉那部份的可以回去看我之前的<a href=\"https://ithelp.ithome.com.tw/articles/10299617\">文章</a>。今天要來討論另外兩個激勵函數，也就是tanh 以及Rectified Linear Unit（ReLU）。<br><img src=\"https://i.imgur.com/UKx4SaC.png\"><br>A圖是tanh，B圖是ReLU，一樣也是各有優缺點。</p>\n<ul>\n<li><strong>tanh函式</strong>：在接近上下界時也趨近於平，所以也能像Sigmoid一樣擅長處理離群值，另外一樣也可以微分，記得先前所說找最佳解時，可不可微是很重要的特性，<del>通常我們希望可微啦，人稱<strong>欲可微</strong></del>。</li>\n<li><strong>ReLU</strong>：結果可以非常接近線性。</li>\n</ul>\n<blockquote>\n<p>但像是Sigmoid或是tanh這種上下界趨平的函式，一階導數很有可能接近於零，接近零的結果，就是在進行梯度下降時發現梯度不見，模型也停止學習了，這就是所謂的<strong>梯度消失（gradient vanishing）</strong></p>\n</blockquote>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>詳見我先前的文章：<a href=\"https://ithelp.ithome.com.tw/articles/10299617\">【NLP】Day 12: 豬耳朵餅乾跟機器學習也有關係？機器學習：羅吉斯回歸</a></p>\n<h3 id=\"Error-backpropagation\"><a href=\"#Error-backpropagation\" class=\"headerlink\" title=\"Error backpropagation\"></a>Error backpropagation</h3><p>神經網路有另外一個優點是，如果發現錯誤，模型會往回修正。但這部分有點複雜，我們在之後的文章再詳細介紹。</p>\n<h2 id=\"自然語言處理的運作\"><a href=\"#自然語言處理的運作\" class=\"headerlink\" title=\"自然語言處理的運作\"></a>自然語言處理的運作</h2><p>自然語言處理上的神經網路又有一點不一樣，在這裡為了處理訓練資料中所沒有的資料，會再多一個嵌入層（embedding layer）。<br><img src=\"https://i.imgur.com/YKY4TsI.png\"></p>\n<ol>\n<li><strong>輸入層（input layer）</strong>：one-hot vector<br>自然語言處理的第一層神經網路通常會是one-hot vector，通常是為了標注其在嵌入層中的位置，藉此幫助模型更好理解語意。假如說今天<code>海賊王</code>在embedding layer中的第三個位置，並假設embedding layer的矩陣維度為5，那麼one-hot vector就會是<code>[0, 0, 1, 0, 0]</code>。</li>\n<li><strong>嵌入層（embedding layer）</strong><br>記得在講Word2Vec時（忘記的話可以回去複習 <a href=\"https://ithelp.ithome.com.tw/articles/10297745\">Day 9: 又是國王皇后的例子啦！Word2Vec、N-Gram</a>，我這篇的點閱率低的可憐。）我們有提到可以訓練一個word embedding，讓電腦了解語意間的關係，但是要注意這裡的語意關係很吃訓練資料的領域跟品質。這麼一來，即使出現了模型先前沒「看」過的字，透過這個embedding也能較好地處理這類的問題。比如說： <figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">training data: 索隆/跟/龍馬/拿/劍/決鬥</span><br><span class=\"line\">test data: 鷹眼/跟/比斯塔/用/劍/對決</span><br></pre></td></tr></table></figure>\n 即使模型沒有在訓練資料看過鷹眼，但是因為Word embedding，我們可以推斷「索隆」跟「鷹眼」有很近的「語意關係」（可能因為都是劍客？），所以還是能推斷「鷹眼」後面可能會出現的字，跟「索隆」很接近。</li>\n<li><strong>隱藏層（hidden layer）</strong><br>再次乘上學習過後的權重。</li>\n<li><strong>輸出層（output layer）</strong>：activation function<br>同樣就是透過激勵函數輸出成機率，詳見前文。</li>\n</ol>\n<p>就是這樣，明天見！</p>\n<p>下一篇文章</p>\n<blockquote>\n<p>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10302256\">【NLP】Day 15: 跟你我一樣選擇性記憶的神經網路？深度學習：長短期記憶 LSTM</a></p>\n</blockquote>\n<p>若你有空，也歡迎來看看其他文章</p>\n<blockquote>\n<p>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10300307\">【NLP】Day 13: 可不可以再深一點點就好？深度學習基礎知識：你需要知道的這些那些</a><br>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10299617\">【NLP】Day 12: 豬耳朵餅乾跟機器學習也有關係？機器學習：羅吉斯回歸</a><br>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10298847\">【NLP】Day 11: 什麼？妳男友有乾妹妹？那你很大機率被綠了！機器學習：貝氏分類器</a></p>\n</blockquote>\n","tags":["自然語言處理","NLP"]},{"title":"【NLP】Day 15: 圓圓圈圈圓圓～深度學習：循環神經網路 RNN","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-15-%E5%9C%93%E5%9C%93%E5%9C%88%E5%9C%88%E5%9C%93%E5%9C%93%EF%BD%9E%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%EF%BC%9A%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-RNN/","content":"<blockquote>\n<p>對啊，這也是一種世界。也是我心中的可能性。現在的我並不只是我，還可以有很多種自我。<br><strong>《新世紀福音戰士》碇真嗣</strong></p>\n</blockquote>\n<p>循環神經網路（Recurrent Neural Network），作為神經網路的其中一種變體，是在所有神經網路的模型中，最常被拿來應用在自然語言處理任務的模型之一。原因有二，首先，相對於其他的神經網路模型，循環神經網路模型的輸入資料較為彈性，意即當其他的神經網路只能接受固定的字數時，循環神經網路亦能透過<strong>不同長度的文本</strong>輸入進行訓練；二，語言中我們常說要了解上下文、語境，才能對語言有更完善的判斷，而循環神經網路的<strong>記憶功能</strong>就可以做到考慮文本上下文的效果。因為以上兩種原因，循環神經網路才會是所有神經網路中最常應用在自然語言處理上的深度學習模型。讓我們一起來看看為什麼吧！</p>\n<h2 id=\"基本結構\"><a href=\"#基本結構\" class=\"headerlink\" title=\"基本結構\"></a>基本結構</h2><p>在正式進入架構之前，要先將循環神經網路跟其他神經網路進行區別。跟先前所介紹的神經網路模型相同，一個完整的循環神經網路模型同樣也有輸入層、隱藏層，以及輸出層，但既然基本架構都相同，那最關鍵的差異點在哪裡呢？我們先來觀察以下這兩張圖。上圖是人工神經網路，<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=x\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=x\">是輸入， <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=y\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=y\"> 是輸出；下圖則是循環神經網路， <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=x\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=x\"> 是輸入，<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=o\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=o\"> 則是輸出，其中<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=s\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=s\">是隱藏層。<br><img src=\"https://i.imgur.com/gueUHNo.png\"><br>Credit: Speech and Language Processing<br><img src=\"https://miro.medium.com/max/1400/1*901chLVbYLHQLc5EeZWIAw.jpeg\"><br>Credit: kdnuggets.com</p>\n<p>不知道大家有沒有發現到，這兩張圖之中最大的差異就在於，循環神經網路的隱藏層會自我循環，也就是說，若將隱藏層拆開來看的話，<strong>隱藏層中的節點會接收上一個隱藏層節點所傳過來的權重</strong>。也就是說，對資料的每一個節點來說，就像是碇真嗣所說：「現在的我並不只是我，同時也包含了過去到現在的很多種自我。」所以我們重新回來看神經網路的圖示架構可以發現，正是因為這樣的特性，循環神經網路才有記憶的功能。</p>\n<p>理解循環神經網路最大的特色之後，再讓我們回來看其中的數學運算原理。其實除了隱藏層之間的連接特性之外，循環神經網路的計算也跟一般的神經網路大同小異。首先是輸入資料<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=x_t\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=x_t\">與權重<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W\">相乘，接著再跟上一個隱藏層神經元所學習到的資訊<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h_%7Bt-1%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h_%7Bt-1%7D\">與權重<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=U\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=U\">相乘，再把兩者相加，把相加之後的值加入激勵函數<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=g\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=g\">，稱為當下隱藏層的輸出<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h_t\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h_t\">，接著當下的輸出層即是隱藏層輸出乘上另一個權重<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=V\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=V\">，再經由激勵函數產生輸出<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=y_t\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=y_t\">。</p>\n<!-- ![](https://i.imgur.com/gghC3Mp.png) -->\n<p><img src=\"https://i.imgur.com/pNfPyIB.png\"></p>\n<h3 id=\"應用\"><a href=\"#應用\" class=\"headerlink\" title=\"應用\"></a>應用</h3><p>除此之外，循環神經網路彈性的輸入與輸出特性，也讓我們在各種下游任務間來去自如，其輸入與輸出及對應的功能如下：</p>\n<ol>\n<li><strong>一對一（one to one）</strong>：輸入（input）及輸出（output）皆為固定，例如文本分類時輸出機率等。</li>\n<li><strong>一對多（one to many）</strong>：單一的輸入，但是可以有複數個輸出，例如文本生成，給予一個主題，使模型透過這個主題生成一串文本。</li>\n<li><strong>多對一（many to one）</strong>：多個輸入，但只有單一輸出，例如情感分析(Sentiment Analysis)，我們可以輸入任一一段文字，並使模型判斷這段文字正面以及負面情緒的程度。</li>\n<li><strong>多對多（many to many）</strong>：複數個輸入，複數個輸出，例如機器翻譯(Machine Translation)，輸入一段外國語言的句子，模型將這段話翻譯成中文。</li>\n</ol>\n<h2 id=\"問題與限制\"><a href=\"#問題與限制\" class=\"headerlink\" title=\"問題與限制\"></a>問題與限制</h2><ul>\n<li><strong>語意限制</strong><br>前面提過，循環神經網路有記憶的特性，因此可以將上下文也考慮進去。但其實嚴格來說，如你們在圖中所見，隱藏層中的節點只能接收<strong>上一個隱藏層的節點</strong>，沒有辦法同時也接收<strong>下一個隱藏層節點</strong>的資訊。聽起來很抽象，但這其實就意味著模型雖然可以記住上文，並消化下文，但卻也因為這個特性，使得模型無法理解下文與上文之間的關係。</li>\n<li><strong>梯度消失</strong><br>當神經網路在反向傳播中，也就是模型向回修正，並透過梯度下降法在找尋最佳解，找Loss的最小值時，可能會發生一個現象，就是梯度會越來越小、越來越小，最後變為零。這就導致模型沒有辦法再繼續找到最小的Loss。</li>\n<li><strong>梯度爆炸</strong><br>梯度爆炸正好與梯度消失相反，梯度會越來越大、越來越大，導致越接近淺層網路，權重變化就越大，也會造成模型訓練上的反效果。</li>\n</ul>\n<p>那該如何解決梯度消失以及梯度爆炸的問題呢？其實一般來說，在自然語言處理的任務中，不會僅僅只用循環神經網路，也會在裡面再加上另外一層深度學習模型：<strong>長短期記憶（LSTM）</strong>。同時應用循環神經網路以及長短期記憶模型，就可以有效避免梯度消失以及爆炸的問題了。</p>\n<p>那我們明天就相約在長短期記憶見囉！</p>\n<p>下一篇文章</p>\n<blockquote>\n<p>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10302256\">【NLP】Day 16: 跟你我一樣選擇性記憶的神經網路？深度學習：長短期記憶 LSTM</a></p>\n</blockquote>\n<p>若你有空，也歡迎來看看其他文章</p>\n<blockquote>\n<p>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10300871\">【NLP】Day 14: 神經網路也會神機錯亂？不，只會精神錯亂…深度學習：前饋神經網路</a><br>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10300307\">【NLP】Day 13: 可不可以再深一點點就好？深度學習基礎知識：你需要知道的這些那些</a><br>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10299617\">【NLP】Day 12: 豬耳朵餅乾跟機器學習也有關係？機器學習：羅吉斯回歸</a></p>\n</blockquote>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"【NLP】Day 16: 跟你我一樣選擇性記憶的神經網路？深度學習：長短期記憶 LSTM","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-16-%E8%B7%9F%E4%BD%A0%E6%88%91%E4%B8%80%E6%A8%A3%E9%81%B8%E6%93%87%E6%80%A7%E8%A8%98%E6%86%B6%E7%9A%84%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%EF%BC%9F%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%EF%BC%9A%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6-LSTM/","content":"<blockquote>\n<p>記憶是個很奇妙的東西。他並不如我想像中那樣運作的。我們太受限於時間了，尤其是時間的順序…<br><strong>《異星入境》Louise Banks</strong></p>\n</blockquote>\n<p>昨天我們剛介紹完循環神經網路（Recurrent Neural Network），提到說雖然循環神經網路很常被拿來利用在自然語言處理上，原因是因為循環神經網路有記憶的功能，可以考慮到輸入文本的上下文關係，並藉此來達到更好的效果。</p>\n<p>但再厲害的技術，也都會有它優缺點，就如同我第一週所說的那樣，科學的演進都源自於對完美的追求，這也正是 Steven Pinker 在《再啟蒙的年代》一書中所說，只要是可以衡量的事物，而這些事物若是有所變化的，那就是人類社會便是一直在進步的路上邁進。</p>\n<p>所以說同樣地，RNN 也有先天上的缺陷。像是因為過多層神經網路導致的梯度爆炸或是梯度消失，影響了模型學習成果以及表現，最後的準確度反倒下降了。昨天說到，LSTM 就是 RNN 的是日救星。</p>\n<h2 id=\"長短期記憶模型（Long-Short-term-memory）\"><a href=\"#長短期記憶模型（Long-Short-term-memory）\" class=\"headerlink\" title=\"長短期記憶模型（Long Short-term memory）\"></a>長短期記憶模型（Long Short-term memory）</h2><p>這邊用一個簡單的例子再來重新解釋梯度爆炸跟梯度消失：假如說我們今天利用以下的資料訓練一個可以完成QA任務的循環神經網路。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">魯夫說他想成為海賊王</span><br></pre></td></tr></table></figure>\n\n<p>在最後測試模型時，問了模型：「是誰想要成為海賊王？」。這時候可能會出現一個問題：</p>\n<p>記不記得我們先前所說 RNN 的記憶特性，而會有這個特性正是因為模型也會將先前所學習的詞彙資訊（權重）在隱藏層中傳遞，最後才會得到的結果。之後，模型會將這個結果進行反向傳播，也就是模型在「反省」先前的學習成果時，都會乘上一個自己的參數 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=w_i\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=w_i\">，若這個參數 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=w_i%3C1\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=w_i%3C1\"> 的話，在資訊通過不同節點的過程中，若太多層神經網路，則會越乘越小，最後無限逼近於0，模型也就「忘記」先前所學的資訊，是為<strong>梯度消失</strong>；反之，若<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=w_i%3E1\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=w_i%3E1\">，某個資訊的概率也就越乘越大，而神經網路就被撐死了，是為<strong>梯度爆炸</strong>。</p>\n<p><img src=\"https://programmathically.com/wp-content/uploads/2021/10/Screenshot-2021-10-29-at-15.21.19-1024x189.png\"></p>\n<p>由於「是誰想要成為海賊王？」的答案（魯夫）在訓練資料中幾乎是在最前面的位置。若模型產生梯度消失的現象，答案在訓練的過程中就會漸漸地被模型「忘記」，最後模型就無法正確回答問題的答案，準確率就下降了。而長短期記憶就可以有效解決這個問題。</p>\n<h3 id=\"到底什麼是LSTM？\"><a href=\"#到底什麼是LSTM？\" class=\"headerlink\" title=\"到底什麼是LSTM？\"></a>到底什麼是LSTM？</h3><p>LSTM（Long Short-term memory），作為循環神經網路的一個變體，可以先從這張圖來理解，在這邊允許我借用左岸朋友莫煩大大的劇情比喻。</p>\n<!-- ![](https://i.imgur.com/4G5veS4.png) -->\n<!-- ![](https://i.imgur.com/iGAHTFV.png) -->\n<p><img src=\"https://i.imgur.com/cnuDCQ3.png\"></p>\n<p>我們可以這麼理解，首先將藍色粗線視為電影的主線劇情、紅色線是為支線劇情，而輸入、忘記、以及輸出都是劇情的「把關者」。今天，「輸入把關者」在看電影時，會決定支線的劇情有多少對電影結局有影響，並依照劇情重要程度寫入主線劇情；但假如這些分線劇情會改變我們對於先前主線劇情的看法的話，「忘記把關者」就會將之前的主線劇情忘記，按照比例替換成現在的新劇情。也就是說，主線劇情的組成主要取決於「輸入把關者」跟「忘記把關者」。也就是說，相對於什麼都記，但時間一久就會忘記的單純循環神經網路，LSTM 則是會「選擇重點記，而且記得比較久」。</p>\n<h4 id=\"熟悉概念後，我們來看看數學原理以及模型吧！\"><a href=\"#熟悉概念後，我們來看看數學原理以及模型吧！\" class=\"headerlink\" title=\"熟悉概念後，我們來看看數學原理以及模型吧！\"></a>熟悉概念後，我們來看看數學原理以及模型吧！</h4><p><img src=\"https://pic4.zhimg.com/80/v2-e4f9851cad426dfe4ab1c76209546827_1440w.webp\"></p>\n<p>這是台大李弘毅教授機器學習課程中的投影片。在圖中，我們可以看到左邊的是一般的循環神經網路，右邊的則是  LSTM。可以發現，兩者最大的差別就是，LSTM 多了一個變數 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=c%5Et\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=c%5Et\">，稱為cell state，也就是前面所說的支線劇情。<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=c%5Et\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=c%5Et\"> 改變速度很慢且幅度小，而<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h\"> 的改變速度則較快，而且幅度比較大。</p>\n<p>在進入一個新節點時，總共得到四個狀態，分別是<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z\">, <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z%5Ef\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z%5Ef\">, <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z%5Ei\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z%5Ei\">, <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z%5Eo\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z%5Eo\">。<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z\"> 作為主要的資料輸入，而另外由激勵函數所包起來的 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z%5Ei\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z%5Ei\">, <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z%5Eo\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z%5Eo\">, <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z%5Ef\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z%5Ef\">，則是作為前面所說把關的三個「門閥」。</p>\n<!-- ![](https://pic4.zhimg.com/80/v2-15c5eb554f843ec492579c6d87e1497b_1440w.webp) -->\n<!-- ![](https://pic1.zhimg.com/80/v2-d044fd0087e1df5d2a1089b441db9970_1440w.webp) -->\n<p><img src=\"https://i.imgur.com/z9aXZdF.png\"></p>\n<p>而這四個狀態分別在長短期記憶的一個節點中，分別在運算的三個階段中扮演著不一樣的角色。先讓我們來看看以下這張圖，其中⊙代表矩陣相乘，⊕代表矩陣相加。（高中數學，還記得吧！）：</p>\n<p><img src=\"https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_1440w.webp\"></p>\n<p>在這張圖中的運算過程共分成三個階段：</p>\n<ol>\n<li><strong>忘記階段</strong><br>在這個階段會選擇性忘記上一個節點所傳來的資訊，簡單來說，就是決定「把重要的記起來，把不重要的忘記」。實際上的運算是，<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z%5E%7Bf%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z%5E%7Bf%7D\">作為遺忘閥（forget gate），像篩子一樣，來控制上一個階段的<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=c%5E%7Bt-1%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=c%5E%7Bt-1%7D\">哪些需要記得，哪些需要忘記。</li>\n<li><strong>選擇記憶階段</strong><br>而在這個階段，LSTM 則會決定哪些是重要的資訊，而哪些不是，並將重點記起來。主要是透過輸入閥（input gate）<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z%5Ei\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z%5Ei\"> 來決定當前的輸入內容<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=x%5Et\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=x%5Et\">哪些是重點，哪些不是。<blockquote>\n<p>以上兩者所得結果相加，就會是這個階段的<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=c%5Et\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=c%5Et\">。</p>\n</blockquote>\n</li>\n<li><strong>輸出階段</strong><br>這個階段則是以前述兩者之和<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=c%5Et\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=c%5Et\">，經由激勵函數<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=tanh\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=tanh\">的縮放後，以<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z%5E0\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z%5E0\">來決定經過<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=tanh\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=tanh\">的 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=c%5Et\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=c%5Et\">資訊中，有哪些適合做為當節點的輸出，並計算出下一個節點的 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h%5Et\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h%5Et\">。</li>\n</ol>\n<h2 id=\"問題與討論\"><a href=\"#問題與討論\" class=\"headerlink\" title=\"問題與討論\"></a>問題與討論</h2><p>在過去讀書階段中，各位一定都有碰過兩種同學，一種是死命地想辦法將所有資訊記下來的同學，另一種則是只記重點，一些太枝微末節的小細節就會選擇性地放棄。而因為前者想要將所有東西都記下來，看起來好像很厲害，但時間一久反而會忘記最重要的重點，考試的表現就容易比較差，這種的大家通常都會說「沒有讀書方法」；反而是後者因為只記重點，所以記的東西就比較少，反而不容易忘記，考試的表現就會比前者還要好。</p>\n<p>所以說，拉回來神經網路，比起 RNN 那樣死命地將所有的資訊記起來，不僅容易忘記先前所記下來的資訊，最後的表現也不會好；相對的 LSTM 會忘記不重要的資訊，只記得重要的，表現結果就會比較好。</p>\n<h3 id=\"需要改進的地方\"><a href=\"#需要改進的地方\" class=\"headerlink\" title=\"需要改進的地方\"></a>需要改進的地方</h3><ol>\n<li>語意限制：同樣是RNN的一種，所以也遺傳了RNN的其中一個缺點，詳見：<a href=\"https://ithelp.ithome.com.tw/articles/10301346\">Day 15: 圓圓圈圈圓圓～深度學習：循環神經網路 RNN</a></li>\n<li>訓練複雜度提升<br>相對於 RNN，我們光是看圖就可以發現，LSTM 其中新增了許多的參數，那一旦資料量提升，訓練複雜度也就會增加。而為了解決這個問題，資訊科學家又發明了與LSTM相同效果，訓練又較不複雜的 GRU（Gate Recurrent Unit），來搭建資料量大的模型，也就是我們明天的文章內容。</li>\n</ol>\n<p>下一篇文章</p>\n<blockquote>\n<p>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10302765\">【NLP】Day 17: 每天成為更好的自己！神經網路也是！深度學習模型 GRU</a></p>\n</blockquote>\n<p>若你有空，也歡迎來看看其他文章</p>\n<blockquote>\n<p>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10301346\">【NLP】Day 15: 圓圓圈圈圓圓～深度學習：循環神經網路 RNN</a><br>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10300871\">【NLP】Day 14: 神經網路也會神機錯亂？不，只會精神錯亂…深度學習：前饋神經網路</a><br>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10300307\">【NLP】Day 13: 可不可以再深一點點就好？深度學習基礎知識：你需要知道的這些那些</a></p>\n</blockquote>\n"},{"title":"【NLP】Day 17: 每天成為更好的自己！神經網路也是！深度學習模型 GRU","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-17-%E6%AF%8F%E5%A4%A9%E6%88%90%E7%82%BA%E6%9B%B4%E5%A5%BD%E7%9A%84%E8%87%AA%E5%B7%B1%EF%BC%81%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E4%B9%9F%E6%98%AF%EF%BC%81%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B-GRU/","content":"<blockquote>\n<p>今天沒有引言，但是有梗圖</p>\n</blockquote>\n<!-- ![](https://i.imgur.com/L5xk5fi.png) -->\n<p><img src=\"https://i.imgur.com/4Nwatt5.png\"></p>\n<p>前天的文章介紹了基本的循環神經網路RNN，但RNN的致命缺點是容易導致梯度下降或是梯度爆炸。為了要解決這個問題，必須在以下兩點有所突破：</p>\n<ol>\n<li>優化梯度演算法</li>\n<li>在神經網路的節點中，設計較好的激勵函數（activation function）</li>\n</ol>\n<p>昨天的文章所介紹的長短期記憶神經網路模型選擇往第二點的方向突破，在神經網路節點中加入了<strong>輸入閥（input gate）</strong>、<strong>遺忘閥（forget gate）</strong>、以及<strong>輸出閥（output gate）</strong>，這個做法可以有效地解決循環神經網路模型<strong>容易梯度下降或是梯度爆炸</strong>的缺點，但由於長短期記憶模型加入的參數過多，若透過大量的資料進行訓練，可能會導致<strong>模型運算量過大</strong>。那電腦科學家們為了降低運算成本以及減少佔用記憶體，同時又要維持相同的模型表現，於是就發明了<strong>GRU（Gate Recurrent Unit）</strong>。</p>\n<h2 id=\"GRU\"><a href=\"#GRU\" class=\"headerlink\" title=\"GRU\"></a>GRU</h2><p>GRU 也是循環神經網路 RNN 的另外一種變體，因此網路架構也跟 RNN 相差無幾，GRU 同樣有輸入層、隱藏層、以及輸出層，最大的差別只是在於其中的節點運算。</p>\n<p><img src=\"https://pic2.zhimg.com/80/v2-49244046a83e30ef2383b94644bf0f31_1440w.webp\"></p>\n<p>我們可以回想一下，在LSTM中，總共有三個門閥對資料進行「把關」，分別為<strong>輸入閥</strong>、<strong>遺忘閥</strong>，以及<strong>輸出閥</strong>；而在GRU中，同樣也有門閥，功能也是完全相同（皆為篩選記憶），只是三個變成了兩個，分別為<strong>重置閥</strong>（reset gate）以及<strong>更新閥</strong>，由於減少了門閥數量，因此運算量相對於LSTM來說，減輕了不少。</p>\n<h3 id=\"GRU的內部結構\"><a href=\"#GRU的內部結構\" class=\"headerlink\" title=\"GRU的內部結構\"></a>GRU的內部結構</h3><p>跟LSTM相同，我們先從GRU的兩個門閥開始，圖中的 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=r\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=r\"> 是重置閥，<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z\"> 則為更新閥，兩者在各自的權重 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W\"> 乘上輸入 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=x%5Et\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=x%5Et\"> 以及隱藏層 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h%5E%7Bt-1%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h%5E%7Bt-1%7D\"> 後，皆會經過激勵函數將數值變換成0-1之間的數字，篩選模型記憶的資訊，也就是說，這些數值代表節點對資訊的記憶程度。我們在這裡將 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=r\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=r\"> 跟 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z\"> 計算出來之後，先放著備用。</p>\n<p><img src=\"https://pic3.zhimg.com/80/v2-7fff5d817530dada1b279c7279d73b8a_1440w.webp\"></p>\n<p>接下來讓我們進入節點的設計，前面有說到 GRU 從原本 LSTM 的三個門閥變成兩個門閥，所以以下會分成兩個階段來講解。</p>\n<h4 id=\"重置閥\"><a href=\"#重置閥\" class=\"headerlink\" title=\"重置閥\"></a>重置閥</h4><p>話不多說，先上圖！⊙ 代表矩陣相乘，⊕ 代表矩陣相加</p>\n<!-- ![](https://i.imgur.com/KhfB9zz.jpg) -->\n<p><img src=\"https://i.imgur.com/XFTckFV.jpg\"></p>\n<blockquote>\n<p>請允許我把這部分用不到的路徑先遮蓋起來，這樣比較不容易迷路。</p>\n</blockquote>\n<p>在這裡，我們會用到之前所計算的重置閥 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=r\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=r\">（reset gate）。首先，當我們得到了重置閥r之後，要透過重置閥來「重置」上一個隱藏層傳來的資訊： </p>\n<p><img src=\"https://i.imgur.com/iIUe83D.png\"></p>\n<p>接著 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h%5E%7B%7Bt-1%7D%5E%5Cprime%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h%5E%7B%7Bt-1%7D%5E%5Cprime%7D\"> 與輸入 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=x%5Et\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=x%5Et\">以及權重相乘後，再加入 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=tanh\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=tanh\"> 的激勵函數，將資料壓到 [-1, 1] 之間，就會得到 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h%5E%5Cprime\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h%5E%5Cprime\">。<br><img src=\"https://pic4.zhimg.com/80/v2-390781506bbebbef799f1a12acd7865b_1440w.webp\"></p>\n<ul>\n<li><strong>代表意義</strong>：<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h%5E%5Cprime\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h%5E%5Cprime\"> 主要是透過 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=r\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=r\"> 來篩選上一層隱藏層 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h%5E%7Bt-1%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h%5E%7Bt-1%7D\"> 所傳來的資訊，接著結合本節點的輸入 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=x%5Et\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=x%5Et\"> 。以上動作都是<strong>為了將 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h%5E%5Cprime\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h%5E%5Cprime\"> 加入到這個節點的隱藏狀態</strong>。功能類似於LSTM的選擇記憶階段。<!-- ![](https://i.imgur.com/2lJVxDZ.png) --></li>\n</ul>\n<h4 id=\"更新閥\"><a href=\"#更新閥\" class=\"headerlink\" title=\"更新閥\"></a>更新閥</h4><p><img src=\"https://i.imgur.com/FGNpyoA.jpg\"></p>\n<p>透過這個更新閥，就可以同時做到記憶以及遺忘的功能。在這裡，我們會用到之前所計算的更新閥 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=z\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=z\">（update gate），在這裡的計算是：</p>\n<p><img src=\"https://i.imgur.com/JnpJN2K.png\"></p>\n<p>其中更新閥的數值，因為sigmoid激勵函數的緣故，數值會介在[0, 1]之間，這代表GRU在這一層需要記憶的程度。<strong>越接近1代表記憶的資訊越多；越接近0代表遺忘的資訊越多。</strong> 不知你是否發現到，只要單單透過一個門閥，就可以<strong>同時控制遺忘以及選擇記憶</strong>，相對於LSTM要用多個門閥才能達到相同的效果，這就是為什麼GRU的計算成本可以低於LSTM的原因。</p>\n<ul>\n<li><strong>代表意義</strong>：<ul>\n<li>「忘記」上一層隱藏狀態所帶來的資訊比例，主要是要放掉上一層隱藏層帶來的不重要資訊，功能接近於LSTM的遺忘閥（forget gate）。<br>  <img src=\"https://i.imgur.com/ymYxYbk.png\"></li>\n<li>表示從 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h%5E%5Cprime\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h%5E%5Cprime\"> 中所要選擇記憶的比例。<br>  <img src=\"https://i.imgur.com/wYk3fCN.png\"></li>\n<li>兩者結合起來就是忘記上一層隱藏層 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=h%5E%7Bt-1%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=h%5E%7Bt-1%7D\"> 所傳下來的資訊，並從當節點所接收的輸入資料中進行選擇性記憶。<br>  <img src=\"https://i.imgur.com/RRH8Cgo.png\"></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"問題與討論\"><a href=\"#問題與討論\" class=\"headerlink\" title=\"問題與討論\"></a>問題與討論</h2><p>因為GRU用了比較少門閥的緣故，所以相對於長短期記憶LSTM來說，訓練的時間較短，佔用的記憶體也較少。只是模型表現上，兩者其實是差不多的。所以如果考量到訓練時間還有運算資源的限制的話，鐵定是毫不猶豫地選擇GRU的啦！</p>\n<p><img src=\"https://i.imgur.com/FiOAAHy.jpg\"></p>\n<p>不過就像在<a href=\"https://ithelp.ithome.com.tw/articles/10301346\">Day 15: 圓圓圈圈圓圓～深度學習：循環神經網路 RNN</a>中所說，這仍然無法解決模型無法理解下文與上文之間關係的問題。那人工智慧學家又是怎麼解決這個問題的呢？就讓我們等到明天介紹BiLSTM的文章再來聊吧！</p>\n<p>若你有空，也歡迎來看看其他文章：</p>\n<blockquote>\n<p>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10302256\">【NLP】Day 16: 跟你我一樣選擇性記憶的神經網路？深度學習：長短期記憶 LSTM</a><br>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10301346\">【NLP】Day 15: 圓圓圈圈圓圓～深度學習：循環神經網路 RNN</a><br>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10300871\">【NLP】Day 14: 神經網路也會神機錯亂？不，只會精神錯亂…深度學習：前饋神經網路</a></p>\n</blockquote>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"【NLP】Day 18: 沒有什麼是一條神經網路解決不了的，如果有，就兩條！深度學習：BiLSTM ","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-18-%E6%B2%92%E6%9C%89%E4%BB%80%E9%BA%BC%E6%98%AF%E4%B8%80%E6%A2%9D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A7%A3%E6%B1%BA%E4%B8%8D%E4%BA%86%E7%9A%84%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%9C%89%EF%BC%8C%E5%B0%B1%E5%85%A9%E6%A2%9D%EF%BC%81%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%EF%BC%9ABiLSTM/","content":"<blockquote>\n<p>我們透過最具革新性且客製化的裝修結構，將兩片晶片合在一起。這種雙晶片的結構領先於現今業界的任何一種晶片。<br><strong>Apple Event, March 8, 2022, Introduction to Apple M1 Ultra</strong></p>\n</blockquote>\n<p>不知道你是否記得，我們在<a href=\"https://ithelp.ithome.com.tw/articles/10301346\">圓圓圈圈圓圓～深度學習：循環神經網路 RNN</a>、<a href=\"https://ithelp.ithome.com.tw/articles/10302256\">跟你我一樣選擇性記憶的神經網路？深度學習：長短期記憶 LSTM</a>，以及<a href=\"https://ithelp.ithome.com.tw/articles/10302765\">每天成為更好的自己！神經網路也是！深度學習模型 GRU</a> 等三篇文章中有提到，因為這兩種神經網路模型都是由前到後地對資料進行「學習」，且相對於簡單的機器學習模型，神經網路有記憶特性，因此可以考慮到上下文的關係（儘管不同種神經網路有其各自的缺陷）。然而，LSTM 同樣也繼承著循環神經網路的其中一個缺點，而 GRU 雖然運算速度比較快，但作為 RNN 的一員，也是在劫難逃，而這個缺點就是<strong>無法考慮「下上文」的關係</strong>。</p>\n<p>什麼意思？</p>\n<h2 id=\"下上文的關係\"><a href=\"#下上文的關係\" class=\"headerlink\" title=\"下上文的關係\"></a>下上文的關係</h2><p>想像一下，我們要理解一段文字中的某一個字時，不只需要了解這個字的上一個字，有時候也會需要知道這個字的下一個字是什麼，才能確定這個字所真正代表的意義。舉例來說，讓我們看看以下這張圖：</p>\n<p><img src=\"https://miro.medium.com/max/1218/1*wODEqmbZyAPH4lihCgpjdQ.gif\"><br>source: <a href=\"https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0\">Bi-LSTM</a></p>\n<p>在圖中，單從前面的said並沒有辦法確定Teddy是什麼意思，因為Teddy有可能是一個人名（美國前總統 Teddy Roosevelt），也有可能是一種玩具（泰迪熊），而要確認Teddy指的是什麼，就會需要Teddy後面的字。比如說在這裡的狀況，若後面是bear，那就是一種玩具；若後面是大寫名詞，那代表這裡的Teddy很有可能是人名。</p>\n<p>不然就是，左到右 LSTM 今天所學到的句子是「魯夫出海時搭著」，而右到左 LSTM 所學的是「打倒了近海王者」，那麼對於一個模型來說，有了上下文的資訊，就能比較準確地預測在中間的字是什麼吧！而對於RNN體系的單向神經網路來說，以上這些任務的表現就比較差了。</p>\n<p>也就是說，<strong>我們前三天所介紹的三種模型：RNN、LSTM、GRU，都沒有辦法有效地解決這個問題</strong>。這時機器學習學家就想</p>\n<blockquote>\n<p>「那既然原來的LSTM是從左邊到右邊學習，那就再放另一條LSTM從右邊到左邊，就好啦！」</p>\n</blockquote>\n<p>當然我把它變得超級白話啦！但真的原理就是這麼簡單。所謂的BiLSTM，就是把從左到右的LSTM，以及從右到左的LSTM合在一起，就這樣！<del>不可能是什麼雙性戀的LSTM的吧，誰知道LSTM的自我認同是什麼呢？(X)</del> </p>\n<p><img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2022/01/bilstm-1.png\"></p>\n<p>從圖中我們可以看到，每一個節點的輸出，都是左到右 LSTM 跟右到左 LSTM 所綜合的結果，這就是我們前面所說，<strong>單純將兩個方向的 LSTM 結合，其實就可以很有效地解決RNN體系無法理解「下上文」的缺陷。</strong></p>\n<h2 id=\"問題與討論\"><a href=\"#問題與討論\" class=\"headerlink\" title=\"問題與討論\"></a>問題與討論</h2><p>所以 BiLSTM 通常也可以有效地解決各類的自然語言處理任務，像是文本分類、機器翻譯、實體辨識等等，除此之外，在聲音辨識中，BiLSTM也是可以佔有一席之地的喔！</p>\n<p>不過熟悉我前面文章風格的都知道，接下來就要講不好的地方啦！俗話說：「有一好，沒兩好。」記得前面在講LSTM的時候，有提到因為在節點中加入了三個門閥（輸入閥、遺忘閥、輸出閥），所以<strong>不管是訓練時間還是運算資源，消耗量都相當可觀</strong>。那聰明的你可以想想看，兩條 LSTM 會怎麼樣呢？</p>\n<p>沒錯，<strong>BiLSTM是個運算量更大、訓練時間更久的深度學習模型</strong>！你可能會想說，有LSTM就有BiLSTM，那有GRU，就會有BiGRU吧？是有BiGRU的，但通常大家都會直接轉向我們自然語言處理界的超大型巨人 BERT 了。也就是說，從明天開始，會花三篇的篇幅來，從注意機制（attention）開始，一路到transformer，接著到BERT的介紹，之後才會進入實作，還請大家一定要撐下去啊（其實是在對自己說）！</p>\n<p>若大家沒事的話，也可以點進昨天的文章，不知道為什麼點閱率少得可憐啊！</p>\n<blockquote>\n<p>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10302765\">【NLP】Day 17: 每天成為更好的自己！神經網路也是！深度學習模型 GRU</a><br>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10302256\">【NLP】Day 16: 跟你我一樣選擇性記憶的神經網路？深度學習：長短期記憶 LSTM</a><br>➡️ <a href=\"https://ithelp.ithome.com.tw/articles/10301346\">【NLP】Day 15: 圓圓圈圈圓圓～深度學習：循環神經網路 RNN</a></p>\n</blockquote>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"【NLP】Day 19: 注意！謝謝你的注意！Transformer （上）","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-19-%E6%B3%A8%E6%84%8F%EF%BC%81%E8%AC%9D%E8%AC%9D%E4%BD%A0%E7%9A%84%E6%B3%A8%E6%84%8F%EF%BC%81Transformer-%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"<blockquote>\n<p>如果我能看得更遠，那是因為站在巨人的肩膀上。<br><strong>牛頓</strong></p>\n</blockquote>\n<p>經過了前幾天的旅程，相信大家對於運用在自然語言處理的神經網路，應該已經有了一定程度的認識。神經網路是深度學習的起點，我們一路從機器學習一直到現在的深度學習，其實都依循著一定的脈絡在往前走。不知道大家是否還記得我們在旅程一開始的時候曾經說過，為了要讓機器學習模型理解文字，首要工作是要將文字轉換成數字，因此發展出了許多如詞頻、TF-IDF等<strong>以詞為模型理解語言的最小單位</strong>的方式，接著以詞頻為基礎，將語言理解成<strong>具有統計概念的分布模型</strong>，因此這樣的模型被稱為<strong>語言模型</strong>，接著將這些轉成數值的文字資料放進傳統機器學習模型。後來，人工智慧學家發現，比起讓模型從詞彙的角度出發，讓模型可以理解文字、句子、甚至是文章上下文之間的連續關係，其實應用上可以更為廣泛。</p>\n<p>就跟我們人有著不同專長一樣，不同的模型也擅長處理不同的資料。當模型處理具有<strong>序列關係</strong>的資料時，這種<strong>sequence model</strong>就是最好的選擇，也就是說，我們前幾天所說到的模型，RNN、LSTM、GRU等，都是sequence model的一種。</p>\n<p>今天要介紹的，也是一種sequence model，稱為Transformer。今天這篇文將會有很大一部分是來自於 <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a>這篇文章，若各位想拜讀原文，可以毫不猶豫地點進去這篇文章喔！</p>\n<h2 id=\"自然語言處理的變形金剛：Transformer\"><a href=\"#自然語言處理的變形金剛：Transformer\" class=\"headerlink\" title=\"自然語言處理的變形金剛：Transformer\"></a>自然語言處理的變形金剛：Transformer</h2><p>如果我們將模型視作一個黑盒子，那首先要先釐清，輸入模型的資料型態會是一個句子，輸出也會是一個句子（畢竟是sequence model，輸入及輸出也會是 “sequence”），而在這個模型內部，是由一長串的編碼器（encoder）跟解碼器（decoder）所組成。我們可以參考以下示意圖：</p>\n<p><img src=\"https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\"></p>\n<p>在圖中我們可以看到，輸入及輸出之間是由一串的編碼器和解碼器所組成。雖然在原文中的圖也是用各六個編碼及解碼器，但事實上跟6這個數字一點關係都沒有。 <del>畢竟不是每個人都是諫山創一樣對數字有執著</del></p>\n<p>首先我們先來將注意力放在編碼器（Pun intended！），編碼器的構造又是由 <strong>自注意力機制（self-attention mechanism）以及一組前饋神經網路（feed-forward neural network）</strong> 所組成。另外在解碼器中，除了自注意力機制以及前饋神經網路之外，也多了<strong>編解碼注意力機制（encoder-decoder attention）</strong>，前饋神經網路在我的 <a href=\"https://ithelp.ithome.com.tw/articles/10300871\">神經網路也會神機錯亂？不，只會精神錯亂…深度學習：前饋神經網路</a> 這篇文章中已經提過，想必大家應該都很清楚了（吧？）（嗎？）所以今天會將主力放在解釋何謂<strong>自注意力機制</strong>。</p>\n<p><img src=\"https://jalammar.github.io/images/t/Transformer_encoder.png\"></p>\n<h3 id=\"自注意力機制（self-attention-mechanism）\"><a href=\"#自注意力機制（self-attention-mechanism）\" class=\"headerlink\" title=\"自注意力機制（self-attention mechanism）\"></a>自注意力機制（self-attention mechanism）</h3><p>自注意力機制的要點在於，深度學習模型可以判斷句子中某個字與句中其他字的關聯性大小，比如說，句中某個字的指涉對象是同一句中的哪個字。這樣說或許有點抽象，什麼指涉，什麼關聯性的吊書袋幹嘛？其實若用原文的舉例來說的話：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;The animal didn&#x27;t cross the street because it was too tired.&quot;</span></span><br></pre></td></tr></table></figure>\n<p>在這句話中，<code>it</code>的指涉對象就是<code>animal</code>，也就是說，整句話之中，<code>it</code>跟<code>animal</code>的關係最接近，所以模型在進行自注意力機制的時候，就會給<code>animal</code>比較高的權重。這個運作方式就像是先前在<a href=\"https://ithelp.ithome.com.tw/articles/10301346\">圓圓圈圈圓圓～深度學習：循環神經網路 RNN</a>之中所說，RNN透過隱藏層之間的資料傳遞（也就是RNN的記憶能力）來了解當前處理的字，與先前處理過的字，兩者之間的關係。</p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_self-attention_visualization.png\"></p>\n<h4 id=\"如何計算的呢？\"><a href=\"#如何計算的呢？\" class=\"headerlink\" title=\"如何計算的呢？\"></a>如何計算的呢？</h4><ol>\n<li><strong>步驟一</strong>：從編碼器輸入層中（在這裡則是詞嵌入層）中計算出三個向量，分別為key vector、query vector以及value vector。這三個向量分別乘上權重<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W%5EQ\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W%5EQ\">、<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W%5EK\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W%5EK\"> 以及 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W%5EV\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W%5EV\"> 之後，就可以得到這個字在句子中的投射（projection），也就是用比較短的向量來代表一個字在一個句子之中的「定位」（我的理解，若有錯請指出）。</li>\n</ol>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_self_attention_vectors.png\"></p>\n<ol start=\"2\">\n<li><strong>步驟二</strong>：在這裡要做的是，要計算對<strong>當位置</strong>輸入字詞來說，其他字的「重要程度」。作法則是將計算所得的key vector，與自己還有其他字詞的query vector相乘（也就是內積），即可獲得前述所謂「重要程度」的分數。</li>\n</ol>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_self_attention_score.png\"></p>\n<ol start=\"3\">\n<li><strong>步驟三</strong>：再來就是除以8，接著把他丟入softmax激勵函數。之所以除以8的原因只是因為這是64的平方根，也就是key vector維度的平方根，這可以幫助在梯度下降時穩定找到最佳解。我們在這裡最終會獲得<code>[0, 1]</code>之間的數字。</li>\n</ol>\n<p><img src=\"https://jalammar.github.io/images/t/self-attention_softmax.png\"></p>\n<ol start=\"4\">\n<li><strong>步驟四</strong>：最後就是相乘再相加。<ul>\n<li><strong>相乘</strong>：將Softmax所得的數值，乘以value vector。這裡的作用是，將重要的字保留下來，並將不重要的字過濾掉。（因為不重要的字會乘上一個超小的數值，就可以把他的重要性降低了）。</li>\n<li><strong>相加</strong>：將前面計算的數值相加，就可以得到在這個「位置」的自注意層輸出了。</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"https://jalammar.github.io/images/t/self-attention-output.png\"></p>\n<p>是不是已經腦洞大開了呢！但這還只是single-head attention，在原論文 <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>中其實又再針對這個概念進行了改良 <del>套句高中數學老師說的，是想逼死誰？</del> ，加入了multi-head attention，而這可以更準確地判斷句中不同位置的字的相對重要程度，不過我們明天再來詳細介紹是怎麼回事，還有後續的decoder的運作方式；另外，朋友們也可以想想看，為什麼在文中，我只要寫到「位置」，我都會特別加上引號。不過我們在這邊先休息一下，明天進入Transformer 下篇！</p>\n"},{"title":"【NLP】Day 2: 工欲善其事，必先利其器！Python基礎介紹！","url":"/2022/09/03/%E3%80%90NLP%E3%80%91Day-2-%E5%B7%A5%E6%AC%B2%E5%96%84%E5%85%B6%E4%BA%8B%EF%BC%8C%E5%BF%85%E5%85%88%E5%88%A9%E5%85%B6%E5%99%A8%EF%BC%81Python%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9%EF%BC%81/","content":"<blockquote>\n<p><em>萬事開頭難，難就難在初始階段。在此期間，必須窮盡心力，使諸方面保持均衡，以利於今後的發展。</em><br><strong>法蘭克・赫伯特《沙丘》</strong></p>\n</blockquote>\n<p>在一起踏上偉大的航道前，由於在這三十天會運用Python作為主要的程式語言，所謂工欲善其事，必先利其器，畢竟就連魯夫一開始出海前，也是需要一艘船嘛！所以免不了總是要稍微解釋一下在之後的文章會用到的簡單語法以及概念。</p>\n<p>但因為在我之前已經有許多大神以及前輩寫了許多Python的教學文章，Python開發者社群也有<a href=\"https://docs.python.org/zh-tw/3/tutorial/index.html\">翻譯官方文件</a>，而且翻譯得非常清楚，所以今天這篇文章只會解釋Python中最常用的語法，也就是基本的資料型態、for迴圈以及if條件敘述式，還有function，也會解釋一份Python的程式，大致上會有什麼架構，畢竟大學的計算機概論教授有云：「你只要會寫for迴圈以及會寫if，你會寫程式啦！」有點誇張了，但相信你可以的。</p>\n<p>不過如果之後的文章用到其他在這裡沒有解釋到的語法，也會在那篇文章特別介紹。</p>\n<h2 id=\"資料型態以及資料結構\"><a href=\"#資料型態以及資料結構\" class=\"headerlink\" title=\"資料型態以及資料結構\"></a>資料型態以及資料結構</h2><p>Python共有種的資料型態以及種的資料結構，不同的型態與結構，在不同的使用情境下有不同的用途，以下幫助你簡單理解：</p>\n<h3 id=\"字串-string\"><a href=\"#字串-string\" class=\"headerlink\" title=\"字串 string\"></a>字串 <code>string</code></h3><p>字串顧名思義就是單純字串，一般來說會以雙引號來表示，例如<code>&quot;偉大航道&quot;</code>就是一種字串，但單引號也可以，像是<code>&#39;偉大航道&#39;</code>，只是大部分都會用雙引號，而且字串也不能進行數學上的四則運算，在程式編寫上要特別留意。性質如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;1&quot;</span>+<span class=\"string\">&quot;2&quot;</span>+<span class=\"string\">&quot;3&quot;</span>) </span><br><span class=\"line\"><span class=\"comment\"># output: 123</span></span><br></pre></td></tr></table></figure>\n\n<p>你也可以將其他種資料型態透過<code>str()</code>轉換成字串：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">num = <span class=\"number\">123</span></span><br><span class=\"line\">num_string = <span class=\"built_in\">str</span>(num)</span><br><span class=\"line\"><span class=\"built_in\">type</span>(num_string)</span><br><span class=\"line\"><span class=\"comment\"># output: str</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"整數-int-amp-浮點數-float\"><a href=\"#整數-int-amp-浮點數-float\" class=\"headerlink\" title=\"整數 int &amp; 浮點數 float\"></a>整數 <code>int</code> &amp; 浮點數 <code>float</code></h3><p>整數就是整數，這不需要特別說明。浮點數就是有小數點的數字，也就是小數！而這兩種資料型態分別可以透過<code>int()</code>以及<code>float()</code>來轉換。這兩種資料型態可以進行四則運算，讓我們來看看以下例子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"number\">1</span>+<span class=\"number\">2</span>+<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"comment\"># output: 6</span></span><br></pre></td></tr></table></figure>\n\n<p>前面已經了解到，資料型態也可以用對應的函式互相轉換，但要特別注意的是，不同資料型態不能混在一起運算。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"number\">123</span>+<span class=\"string\">&quot;123&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># output: </span></span><br><span class=\"line\">---------------------------------------------------------------------------</span><br><span class=\"line\">TypeError                                 Traceback (most recent call last)</span><br><span class=\"line\">&lt;ipython-<span class=\"built_in\">input</span>-<span class=\"number\">3</span>-a752e60be039&gt; <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">----&gt; <span class=\"number\">1</span> <span class=\"number\">123</span>+<span class=\"string\">&quot;123&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">TypeError: unsupported operand <span class=\"built_in\">type</span>(s) <span class=\"keyword\">for</span> +: <span class=\"string\">&#x27;int&#x27;</span> <span class=\"keyword\">and</span> <span class=\"string\">&#x27;str&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>但浮點數跟整數間可以進行四則運算嗎？我們來試試看：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"number\">1.23</span>+<span class=\"number\">123</span>)</span><br><span class=\"line\"><span class=\"comment\"># output: 124.23</span></span><br></pre></td></tr></table></figure>\n<p>這兩者之間由上可知是可以進行四則運算的喔！</p>\n<h3 id=\"串列-list\"><a href=\"#串列-list\" class=\"headerlink\" title=\"串列 list\"></a>串列 <code>list</code></h3><p><code>list</code>用中括號表示，可以將前述講到的資料，像火車一樣一個一個分別存在不同車廂裡面，並以逗號分隔。即使不同資料型態也可以一起放進去。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 建立新的空串列，並命名為example</span></span><br><span class=\"line\">example = []</span><br><span class=\"line\"><span class=\"comment\"># 預設資料</span></span><br><span class=\"line\">example = [<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>]</span><br><span class=\"line\"><span class=\"comment\"># 將資料加入串列的最尾端</span></span><br><span class=\"line\">example.append(<span class=\"number\">123</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(example)</span><br><span class=\"line\"><span class=\"comment\"># output: [1, 2, 3, 123]</span></span><br><span class=\"line\"><span class=\"comment\"># 不同的資料型態也可以存在同一個串列中</span></span><br><span class=\"line\">example.append(<span class=\"string\">&quot;海賊王&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(example)</span><br><span class=\"line\"><span class=\"comment\"># output: [1, 2, 3, 123, &quot;海賊王&quot;]</span></span><br><span class=\"line\"><span class=\"comment\"># 轉換方式則可以透過list()，如：</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"字典-dictionary\"><a href=\"#字典-dictionary\" class=\"headerlink\" title=\"字典 dictionary\"></a>字典 <code>dictionary</code></h3><p><code>dictionary</code>則是用大括號表示，是python特有的資料型態，像是字典一樣，每個字都有其對應的意義。那麼當值與值之間互相對應的時候，就可以透過<code>dict()</code>來表示：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 建立新的空字典，並命名為example</span></span><br><span class=\"line\">example = &#123;&#125;</span><br><span class=\"line\"><span class=\"comment\"># 字典形式分成key跟value，每個key有對應的value，並用冒號表示：</span></span><br><span class=\"line\">example = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;魯夫&quot;</span>: <span class=\"string\">&quot;船長&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;索隆&quot;</span>: <span class=\"string\">&quot;戰鬥員&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;娜美&quot;</span>: <span class=\"string\">&quot;航海士&quot;</span>,</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\"># 並透過以下方式取出值（value）：</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(example[<span class=\"string\">&#x27;魯夫&#x27;</span>])</span><br><span class=\"line\"><span class=\"comment\"># output: 船長</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Python語法\"><a href=\"#Python語法\" class=\"headerlink\" title=\"Python語法\"></a>Python語法</h2><p>那接下來就讓我們來看看可以透過哪些方式操作以上這些資料吧！</p>\n<h3 id=\"for迴圈\"><a href=\"#for迴圈\" class=\"headerlink\" title=\"for迴圈\"></a><code>for</code>迴圈</h3><p><code>for</code>迴圈，顧名思義，如果你需要重複做某件相同的事時，就可以透過<code>for</code>迴圈讓程式來幫你做，你可以指定迴圈運行的次數，或是讓迴圈「遍歷」串列或是字典的key或是value，但方式有點不一樣。常用的寫法如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">example = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">123</span>, <span class=\"string\">&quot;海賊王&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> example:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># output: </span></span><br><span class=\"line\"><span class=\"comment\"># 1</span></span><br><span class=\"line\"><span class=\"comment\"># 2</span></span><br><span class=\"line\"><span class=\"comment\"># 3</span></span><br><span class=\"line\"><span class=\"comment\"># 123</span></span><br><span class=\"line\"><span class=\"comment\"># 海賊王</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 字典本身不能遍歷，但可以遍歷key或是value</span></span><br><span class=\"line\">example = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;魯夫&quot;</span>: <span class=\"string\">&quot;船長&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;索隆&quot;</span>: <span class=\"string\">&quot;戰鬥員&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;娜美&quot;</span>: <span class=\"string\">&quot;航海士&quot;</span>,</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> example.keys():</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\"><span class=\"comment\"># output: </span></span><br><span class=\"line\"><span class=\"comment\"># 魯夫</span></span><br><span class=\"line\"><span class=\"comment\"># 索隆</span></span><br><span class=\"line\"><span class=\"comment\"># 娜美</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> example.values(): <span class=\"comment\"># 要加s</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\"><span class=\"comment\"># output: </span></span><br><span class=\"line\"><span class=\"comment\"># 船長</span></span><br><span class=\"line\"><span class=\"comment\"># 戰鬥員</span></span><br><span class=\"line\"><span class=\"comment\"># 航海士</span></span><br></pre></td></tr></table></figure>\n\n<p><code>for</code>迴圈也有另外一種常見的寫法，我們來看看：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">example = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">123</span>, <span class=\"string\">&quot;海賊王&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(example)):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\"><span class=\"comment\"># output: </span></span><br><span class=\"line\"><span class=\"comment\"># 1</span></span><br><span class=\"line\"><span class=\"comment\"># 2</span></span><br><span class=\"line\"><span class=\"comment\"># 3</span></span><br><span class=\"line\"><span class=\"comment\"># 123</span></span><br><span class=\"line\"><span class=\"comment\"># 海賊王</span></span><br></pre></td></tr></table></figure>\n<p>其中用到了兩個之前沒用過的函式，分別是<code>range()</code>以及<code>len()</code>，<code>range()</code>中的參數代表的就是讓<code>i</code>運行這個次數，<code>len()</code>則是串列的長度，那這行就是讓<code>for</code>迴圈運行串列長度的次數。</p>\n<h3 id=\"if條件敘述式\"><a href=\"#if條件敘述式\" class=\"headerlink\" title=\"if條件敘述式\"></a><code>if</code>條件敘述式</h3><p>當程式達到某特定條件時，可以透過<code>if</code>，指定程式執行其他動作，一般會在迴圈中使用。用法如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">example = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">123</span>, <span class=\"string\">&quot;海賊王&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(example)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"string\">&quot;海賊王&quot;</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\"><span class=\"comment\"># output: 海賊王</span></span><br></pre></td></tr></table></figure>\n<p>若有不同條件，則可以透過<code>elif</code>來指定條件。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">example = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">123</span>, <span class=\"string\">&quot;海賊王&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(example)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"string\">&quot;海賊王&quot;</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> i == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\"><span class=\"comment\"># output: </span></span><br><span class=\"line\"><span class=\"comment\"># 海賊王</span></span><br><span class=\"line\"><span class=\"comment\"># 1</span></span><br></pre></td></tr></table></figure>\n\n<p>若執行了<code>if</code>就不會執行<code>elif</code>，若包含<code>elif</code>條件就只能擇一。但如果全部都寫<code>if</code>就所有條件都會檢驗一遍。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">example = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">123</span>, <span class=\"string\">&quot;海賊王&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(example)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"string\">&quot;海賊王&quot;</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> i == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\">example = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">123</span>, <span class=\"string\">&quot;海賊王&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(example)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"string\">&quot;海賊王&quot;</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br></pre></td></tr></table></figure>\n\n<p>最後前面的條件都不符合，但也要對其進行處理，則可以用<code>else</code>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">example = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">123</span>, <span class=\"string\">&quot;海賊王&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(example)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"string\">&quot;海賊王&quot;</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> i == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;我是else&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># or</span></span><br><span class=\"line\">example = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">123</span>, <span class=\"string\">&quot;海賊王&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(example)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"string\">&quot;海賊王&quot;</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(i)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;我是else&quot;</span>)</span><br></pre></td></tr></table></figure>\n<p>跳出迴圈可以用<code>break</code>，繼續執行迴圈可以用<code>continue</code>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">example = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">123</span>, <span class=\"string\">&quot;海賊王&quot;</span>]</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(example)):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"string\">&quot;海賊王&quot;</span>:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> i == <span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">continue</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;我是else&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"def-函式\"><a href=\"#def-函式\" class=\"headerlink\" title=\"def 函式\"></a><code>def</code> 函式</h3><p>不知道大家還記不記得國高中的函數，給定一個x值，函數會計算出y值；同理，一樣是function，程式中也會有所謂的函式，括號中給定參數，函式則會回傳值，或是不回傳值只執行特定動作也可以。當你發現一直在重複相同動作又要寫很多次的時候，就是函式派上用場的時候了。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">example_func</span>(<span class=\"params\">parameter_1</span>):</span><br><span class=\"line\">\tresult = <span class=\"number\">1</span> + parater_1</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 在這裡你可以回傳值，也可以將回傳值指配給變數，如：</span></span><br><span class=\"line\"></span><br><span class=\"line\">result_var = example_func(<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result_var)</span><br><span class=\"line\"><span class=\"comment\"># output: 3</span></span><br></pre></td></tr></table></figure>\n\n<p>好的！大致上會用到的基本語法就是這樣，若想知道更多可以去看<a href=\"https://docs.python.org/zh-tw/3/tutorial/index.html\">官方說明文件</a>，網路上也有很多的教學，明天就要進入偉大航道了，我們明天見。</p>\n","categories":["自然語言處理"]},{"title":"【NLP】Day 20: 放點注意力在多頭上（NLP也有多頭啊！）：Transformer（下）","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-20-%E6%94%BE%E9%BB%9E%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%9C%A8%E5%A4%9A%E9%A0%AD%E4%B8%8A%EF%BC%88NLP%E4%B9%9F%E6%9C%89%E5%A4%9A%E9%A0%AD%E5%95%8A%EF%BC%81%EF%BC%89%EF%BC%9ATransformer%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"<blockquote>\n<p>空頭不死，多頭不止；多頭不死，空頭不止<br><strong>不詳</strong></p>\n</blockquote>\n<p>在股票市場中，人人著稱的一句話：「空頭不死，多頭不止；多頭不死，空頭不止。」意思是，如果股價在下降的趨勢時中，如果想一直有抄底，期望做反彈的人不斷進場的話，那麼市場就會不斷下跌。我想我在這裡可以再多加一句：</p>\n<blockquote>\n<p>單頭一有，強到流油；注意多頭，更上層樓！<br><strong>By me</strong></p>\n</blockquote>\n<p>只是差別是，一個在講的是股市走向及股民心理，另一個則是在講 <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a> 論文的 Transformer 模型，以及在其中所運用的自注意力機制（Self-attention Mechanism）。昨天的文章中，我們已經簡單地了解何謂自注意力機制。在同一篇文章中，作者又針對這個自注意力機制再次進行了改良 <del>完全不給其他人優化的機會喔</del>，也就是將原本提出的單一自注意力機制，在模型中改良成<strong>多頭自注意力機制（Multi-head Self-attention Mechanism）</strong>。今天，就讓我們一起來看看 Multi-head Self-Attention Mechanism 跟原本的有什麼不一樣呢？接著，再把剩下的 Encoder 以及 Decoder 走完吧！</p>\n<h3 id=\"Multi-head-Self-attention-Mechanism\"><a href=\"#Multi-head-Self-attention-Mechanism\" class=\"headerlink\" title=\"Multi-head Self-attention Mechanism\"></a>Multi-head Self-attention Mechanism</h3><p>若你還記得的話，在昨天的文章，<a href=\"https://ithelp.ithome.com.tw/articles/10303885\">【NLP】Day 19: 注意！謝謝你的注意！Transformer （上）</a>之中，曾經提到 Transformer 會將每一個字的向量（也就是Word-embedding）乘上一個權重 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W\"> 之後，會得到三個不同的向量，分別是 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=Q\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=Q\">、<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=K\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=K\"> 以及 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=V\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=V\"> ，也就是 queue vector、key vector，以及value vector。那 Multi-head Self-attention Mechanism 其實就是<strong>不同的字要乘上不同的權重</strong> <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W\"> ，也就是說，不同的字會有不同的<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=Q\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=Q\">、<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=K\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=K\"> 以及 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=V\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=V\"> 。我們可以先看看下圖：</p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png\"></p>\n<p>在這邊再強調一次，這邊的 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=Q\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=Q\">、<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=K\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=K\"> 以及 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=V\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=V\"> 三個向量，是 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=X\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=X\"> 分別與 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W_%7Bi%5EQ%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W_%7Bi%5EQ%7D\"> 、 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W_%7Bi%5Ek%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W_%7Bi%5Ek%7D\"><br>以及 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W_%7Bi%5Ev%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W_%7Bi%5Ev%7D\"> 內積所得的結果。有幾個維度，就有幾個 Head；同理，若訓練資料的 Word Embedding 總共有 8 個維度，那麼就會有 8 個不同的 Head。最後我們就會得到 8 個 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=Z\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=Z\"> 的<strong>矩陣</strong>。 但是問題在於，接下來要將資料餵入的模型事前饋神經網路（Feed-forward Neural Network），吃的是每個 <strong>字</strong> 一個 <strong>向量</strong>。這時候就是要再乘上 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W_o\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W_o\"> 將這一大串的矩陣再轉回成一個字一個向量（因為在圖中是兩個字，所以會變成是兩個矩陣）。</p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png\"></p>\n<p>那我們綜觀下來，self-attention 的機制就如下圖運作。</p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\"></p>\n<h4 id=\"Multi-head-Self-attention-Mechanism-哪裡比較好？\"><a href=\"#Multi-head-Self-attention-Mechanism-哪裡比較好？\" class=\"headerlink\" title=\"Multi-head Self-attention Mechanism 哪裡比較好？\"></a>Multi-head Self-attention Mechanism 哪裡比較好？</h4><ol>\n<li><p>Multi-head 的機制更可以讓模型專注在一句話中的不同字上。雖然說在 Single-head 的機制底下，對其中一個字來說，一句話的其他字在這個字的編碼中會含有一點點的權重，但在某些情況下，還是有可能會特別側重在某個字上。但 Multi-head 就可以淡化這個問題。</p>\n</li>\n<li><p>若你還記得的話，Multi-head 的機制針對一句話中的不同字，各自會有不同的 <a href=\"https://chart.googleapis.com/chart?cht=tx&chl=W_%7Bi%5EQ%7D\">https://chart.googleapis.com/chart?cht=tx&chl=W_%7Bi%5EQ%7D</a> 、 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W_%7Bi%5Ek%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W_%7Bi%5Ek%7D\"><br>以及 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=W_%7Bi%5Ev%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=W_%7Bi%5Ev%7D\"> 權重，這麼一來也可以提供模型的語意空間更高的彈性。（這是我的理解，若有錯誤還請前輩不吝指正。）</p>\n</li>\n</ol>\n<h3 id=\"位置編碼（Positional-Encoding）\"><a href=\"#位置編碼（Positional-Encoding）\" class=\"headerlink\" title=\"位置編碼（Positional Encoding）\"></a>位置編碼（Positional Encoding）</h3><p>要知道字在文中的不同位置對於語意理解上也扮演著很重要的角色，所以我們必須在模型中加入位置的資訊，也就是所謂的位置編碼（Positional Encoding），這可以幫助模型理解<strong>一個字在資料中的位置</strong>，以及<strong>與其他字的距離</strong>。</p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png\"></p>\n<p>實際上會長這樣：</p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_example.png\"></p>\n<h3 id=\"殘差標準化（Residual-amp-Normalization）\"><a href=\"#殘差標準化（Residual-amp-Normalization）\" class=\"headerlink\" title=\"殘差標準化（Residual &amp; Normalization）\"></a>殘差標準化（Residual &amp; Normalization）</h3><p>因為在運算資料的過程中會產生殘差，所以也要經過標準化。這層標準化層，在編碼器以及解碼器中都會加入。</p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\"></p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\"></p>\n<h2 id=\"解碼器（decoder）\"><a href=\"#解碼器（decoder）\" class=\"headerlink\" title=\"解碼器（decoder）\"></a>解碼器（decoder）</h2><p>在編碼器，我們將文本序列作為輸入資料，在最後一個編碼器會得到兩個 attention vector，分別是 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=K\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=K\"> 以及 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=V\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=V\"> 。 這兩個向量可以幫助在每個解碼器中的 encoder-decoder attention 了解輸入序列中這些字的資訊。</p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_decoding_1.gif\"></p>\n<p>在接下來的解碼中，除了前面兩個 attention vector 的幫助，同樣也會在解碼器端加入前文的 word embedding 以及 positional encoding，同樣也是為了要幫助模型了解語意距離、位置資訊等等，以利模型進行解碼。</p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_decoding_2.gif\"></p>\n<h2 id=\"最終輸出層（Final-Linear-and-Softmax-Layer）\"><a href=\"#最終輸出層（Final-Linear-and-Softmax-Layer）\" class=\"headerlink\" title=\"最終輸出層（Final Linear and Softmax Layer）\"></a>最終輸出層（Final Linear and Softmax Layer）</h2><p>最後一哩路啦！在解碼器的尾端，我們最後得到的輸出是由浮點數所組成的向量，但我們需要文字輸出的話，該怎麼辦？所以最後就會需要<strong>線性層（Linear Layer）</strong>以及<strong>激勵函數層（Softmax Layer）</strong>來幫助模型將這些數值資料轉回文字。</p>\n<p>線性層是一個完全連接的神經網路。簡單的說，線性層可以將解碼器的輸出向量<strong>轉化成另外一種可以「儲存字彙」的向量</strong>，文中稱為 Logits vector。假如說今天模型在訓練資料中學習了 10000 個字，那麼在這個叫 Logits vector 的向量中就會有 10000 個元素，這些元素分別又代表了不同分數（score），這些分數都可以對應回去每一個字。</p>\n<p>Softmax 則會在這10000個元素中的每一個元素各分配一個概率，讓模型用這個概率決定該以哪個字作為最後的輸出。</p>\n<p><img src=\"https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png\"></p>\n<h2 id=\"問題與討論\"><a href=\"#問題與討論\" class=\"headerlink\" title=\"問題與討論\"></a>問題與討論</h2><ol>\n<li><strong>解釋力偏低</strong>：由於 Transformer 算是深度學習模型的其中一種，所以通常解釋力都比較低。我們沒有辦法跟人解釋模型是如何運算得出某個特定的輸出結果。其實無論是 Transformer 還是前面介紹的 RNN，還有接下來要介紹的 BERT，都有一樣的問題。</li>\n<li><strong>難以控制自注意力機制的運作</strong>：因為自注意力的運算機制，導致模型有漏字的可能。舉例來說，我們透過Google翻譯 <code>one one one one two two one one twenty two</code> 可以得到 <code>一一一二二一一二二</code> ，但我們都知道少了一個<code>一</code>。 <a href=\"https://www.quora.com/Which-are-the-weaknesses-of-transformers-in-deep-learning\">Source</a><br><img src=\"https://i.imgur.com/uxuaDTX.png\"></li>\n<li><strong>訓練時間長</strong>：我們用了好多attention head、好多的參數、好多的運算，那訓練一個模型的時間就會拉的特別長。一天兩天已算少，一週兩週剛剛好。</li>\n</ol>\n<h2 id=\"結語\"><a href=\"#結語\" class=\"headerlink\" title=\"結語\"></a>結語</h2><p>好！既然我們已經了解了什麼是 Transformer，就代表我們已經做好萬全準備進入現在最強最常用的語言模型 BERT 啦！我們明天見！</p>\n<p>參考資料：</p>\n<ol>\n<li><a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></li>\n<li><a href=\"https://medium.com/ching-i/transformer-attention-is-all-you-need-c7967f38af14\">謦伊的閱讀筆記</a></li>\n</ol>\n"},{"title":"【NLP】Day 22: 幫你解決各類NLP任務的BERT，以及其他在芝麻街的好捧油們（上）","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-22-%E5%B9%AB%E4%BD%A0%E8%A7%A3%E6%B1%BA%E5%90%84%E9%A1%9ENLP%E4%BB%BB%E5%8B%99%E7%9A%84BERT%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E5%9C%A8%E8%8A%9D%E9%BA%BB%E8%A1%97%E7%9A%84%E5%A5%BD%E6%8D%A7%E6%B2%B9%E5%80%91%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"<p><img src=\"https://i.imgur.com/tBCYtrl.jpg\"></p>\n<blockquote>\n<p>在這裡，你可以盡情揮灑你的想像力，不覺得這很令人難以想像嗎？<br><strong>《芝麻街》Elmo</strong></p>\n</blockquote>\n<p>在過去幾天的旅程中，我們了解如何利用各種神經網路模型來幫助我們處理不同自然語言處理任務，計算語言學家接著也基於前面講過的那些神經網路模型，陸續發展了許多其他新型態的語言模型，例如：基於BiLSTM的ELMo。我想應該是研究太苦悶了，或者說是一種惡趣味吧？自然語言處理專家特別喜歡用芝麻街的角色來為這些語言模型命名，就算是硬湊名稱，也要把它湊成芝麻街角色的名字（e.g. 今天要介紹的 BERT 就是一個案例）。假如說，現在小小孩想上網抓 Elmo 的照片，結果卻出現一堆莫名其妙的節點跟線的連接圖，會被嚇哭吧？ <del>計算語言學家都是邪惡的。</del> </p>\n<p>不過如果你還有印象，就會記得 BiLSTM 儘管記憶力已經比 RNN 還要好了，但人工智慧學家對仍然不滿足，他們認為如果要了解一個字在文章中的「定位」，就必須要讓模型理解全文才能真正決定。所以接下來學者又開發出了 Transformer ，而其中的 Self-Attention 機制，使每一個字對每一個字來說「沒有位置的概念」。好，我知道有點玄，就套句台大李宏毅老師所說：</p>\n<blockquote>\n<p>海內存知己，天涯若比鄰。</p>\n</blockquote>\n<p>這時候，兩個有關係的字，即使離很遠，模型也能理解之間的關係。就是這樣，Transformer 的 Self-Attention 機制，就可以完全解決 RNN 系列的天生缺陷。而今天要介紹的 BERT 就是基於 Transformer 所搭建的模型。</p>\n<p>在 2018 年時（其實沒有很久以前喔），Google 集先前所有語言模型大成，打造了 BERT，並釋出了模型及原始碼，幫助處理各式各樣的下游任務，而這個 BERT 的表現，以及準確度，都到了迄今仍無人能超越的地步。</p>\n<h2 id=\"如果有一天，有個模型幫助我們完成任何事，那該有多好？\"><a href=\"#如果有一天，有個模型幫助我們完成任何事，那該有多好？\" class=\"headerlink\" title=\"如果有一天，有個模型幫助我們完成任何事，那該有多好？\"></a>如果有一天，有個模型幫助我們完成任何事，那該有多好？</h2><p>話說得有點誇張，不過當時 BERT 模型的釋出，確確實實地撼動了計算語言學界與自然語言處理業界。在那篇被引用了四萬多次的論文（<a href=\"https://arxiv.org/abs/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>）之中，介紹了如何用 BERT 進行兩階段的訓練方法，幫助後續自然語言的處理任務。</p>\n<h2 id=\"預訓練模型的訓練方法：遷移學習（Transfer-Learning）\"><a href=\"#預訓練模型的訓練方法：遷移學習（Transfer-Learning）\" class=\"headerlink\" title=\"預訓練模型的訓練方法：遷移學習（Transfer Learning）\"></a>預訓練模型的訓練方法：遷移學習（Transfer Learning）</h2><p>如果說能夠先訓練出一個對自然語言有一個大致了解的模型，並且基於這模型再針對各種不同下游任務進行調整，這種方法稱為：<strong>遷移學習（Transfer Learning）</strong>。這篇論文的作者就是為了實現這一個目標，將 Transformer 中的 encoder、大量的文本，以及針對目標進行的預訓練，來搭建整個 BERT 模型。而作者為了要達成這個目標，將訓練過程分成兩個階段：</p>\n<p><img src=\"https://leemeng.tw/images/bert/bert-2phase.jpg\"></p>\n<ol>\n<li><strong>預訓練（Pretraining）</strong><br>在論文中有提到，為了要訓練這樣的模型，首先會進行兩種預訓練任務，分別為<strong>克漏字填空</strong>、以及<strong>下文預測</strong>。對於我們人類來說，要理解自然語言是輕而易舉的事，但對於電腦來說卻是一件難如登天的任務。我們一一來說明模型的訓練過程。<ul>\n<li>克漏字填空（Mask Language Model）<br> 比較專業一點的名詞稱為 <strong>Masked Language Model（MLM）</strong>。之所以會說這是克漏字填空，其實因為這完完全全就是我們高中在學英文時最討厭的那種題型，只是我們把它拿給機器來做。大家在圖中可以看到，這些句子在丟進去模型進行訓練的時候，會把一些字詞遮蓋起來，讓模型去預測被遮蓋起來的字詞是什麼。若是以圖中李宏毅老師的簡報為例的話，我們人類很快就可以知道 Mask</li>\n<li>下文預測（Next Sentence Prediction）<br> 英文又叫做 Next Sentence Prediciton (NSP)。其實就是預測下一個可能的句子是什麼。要以這種方式對模型進行預訓練，其實就是為了要解決例如文本生成，或是 QA 的任務。</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"https://leemeng.tw/images/bert/bert-pretrain-tasks.jpg\"></p>\n<ol start=\"2\">\n<li><strong>模型微調（Fine-Tuning）</strong><br>前面的模型訓練結束後，再針對不同的下游任務進行微調。在論文中，作者將模型進行微調後，都達到了空前的正確率。這部分的詳細做法，我們留到明天再來詳細講解。今天只會簡單地介紹 BERT，讓各位先有個概念即可。這些任務包括了以下四種，分別為：<strong>單一句子分類</strong>、<strong>單一句子標註</strong>、<strong>成對句子預測</strong>，以及<strong>問答任務</strong>。明天我們再詳細介紹。</li>\n</ol>\n<p><img src=\"https://leemeng.tw/images/bert/bert_fine_tuning_tasks.jpg\"></p>\n<h2 id=\"跟使用-LSTM-訓練的模型有什麼不一樣？\"><a href=\"#跟使用-LSTM-訓練的模型有什麼不一樣？\" class=\"headerlink\" title=\"跟使用 LSTM 訓練的模型有什麼不一樣？\"></a>跟使用 LSTM 訓練的模型有什麼不一樣？</h2><p>看到這邊，大家可能想說，咦？那跟先前介紹的使用 LSTM 訓練的模型，有什麼差別？好像就是個很厲害的語言模型不是嗎？為什麼好像 BERT 獲得較高的關注？不公平啊！其實使用 LSTM 訓練的模型跟 BERT 有一些結構上還有運用上的差異及差異，這就讓我們一一來看看是怎麼回事吧！</p>\n<ul>\n<li><strong>雙向架構：</strong> 因為前面所說的克漏字填空任務，相較於過去介紹過的模型來說，可以幫助模型確實地「雙向」理解文本。也就是說，雖然説是雙向，但事實上透過 Transformer 中的自注意力機制，對所有字來說，彼此之間的關係「既很近又很遠」，你可以好像就在我身邊，但你其實卻又遠在天邊 <del>就像你的女朋友一樣</del> 。那因為這樣的特性，讓 BERT 模型確實地展露出 「天涯若比鄰」的概念。所以說 BERT 實際上<strong>並不是雙向，但是卻有著雙向的特性，達成了雙向欲達成的目標，因此有了比雙向還要更優秀的理解上下文的能力</strong> <del>這就是為什麼我說其實 BERT 並不是 Bidirectional 的緣故，但人工智慧學家為了湊出 BERT 硬給了他一個 B 開頭的字</del></li>\n<li><strong>適合大量資料訓練：</strong> 相較於其它用 LSTM 訓練而成的深度學習模型，用 Transformer 訓練的模型可以透過平行運算的方式，而這很適合幫助進行資料大的模型訓練。所謂平行運算，簡單來說，就是模型可以將一份大資料分成很多個小部分資料進行計算，比起以往全部的資料一次計算的方式來說，平行運算的速度快非常多，而 BERT 適合這種方式。</li>\n</ul>\n<p>好，今天就講到這，明天就上我們拿起放大鏡，來一一了解 BERT 的訓練過程以及下游任務的應用囉！明天見。</p>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"【NLP】Day 23: 幫你解決各類NLP任務的BERT，以及其他在芝麻街的好捧油們（下）","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-23-%E5%B9%AB%E4%BD%A0%E8%A7%A3%E6%B1%BA%E5%90%84%E9%A1%9ENLP%E4%BB%BB%E5%8B%99%E7%9A%84BERT%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E5%9C%A8%E8%8A%9D%E9%BA%BB%E8%A1%97%E7%9A%84%E5%A5%BD%E6%8D%A7%E6%B2%B9%E5%80%91%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"<blockquote>\n<p>真正掌握權力的人，通常都躲在表面上有權力的人後面，操控著一切。<br><strong>法蘭西斯・安德伍德《紙牌屋》</strong></p>\n</blockquote>\n<p>這幾天在研究 BERT 的時候想著，如果要拿流行文化來比喻的話，可以用什麼。嗯…利用別人達到更好的成就、亦正亦邪，表現上看起來風風光光，但其實細思極恐，突然想到，還能有比紙牌屋的主角法蘭克還要更好的比喻了嗎？法蘭克也是為了要達成自己的目標，不斷踩在別人（屍）身前進，表面上看起來很厲害，但站上高位之後，卻也容易被別人陷害，受到影響。</p>\n<p><img src=\"https://cdn.cnn.com/cnnnext/dam/assets/150226184423-orig-house-of-cards-recap-season-2-00013812.jpg\"><br>Source: <a href=\"https://edition.cnn.com/videos/tv/2015/02/26/orig-house-of-cards-recap-season-2.cnn\">CNN</a>（小知識：現在 NETFLIX 開頭的「咚咚」就是出自於這一幕喔！）（沒人在乎）</p>\n<p>我想我大概是全世界第一個把 BERT 跟紙牌屋拿來放在一起比較的人了吧！</p>\n<p>幹話說完，該進入正題（終於嗎？）昨天我們提到兩階段的遷移式學習（Transfer Learning） 以及 BERT 的訓練方法，還有 BERT 可以完成的那些下游任務。今天來仔細一一了解 BERT 的內部構造吧！</p>\n<h2 id=\"預訓練模型\"><a href=\"#預訓練模型\" class=\"headerlink\" title=\"預訓練模型\"></a>預訓練模型</h2><p>在昨天的文章中有提到，原文獻將遷移學習（Transfer Learning）分成了兩階段，第一階段為Pretraining、第二階段則為模型微調。首先，讓我們來看看預訓練模型在一開始是如何訓練出來的。預訓練時，原文獻的作者將模型訓練分成了兩個任務，一個是克漏字填空（Masked Language Model），另一個則是下文預測（Next Sentence prediction）。</p>\n<h3 id=\"克漏字填空\"><a href=\"#克漏字填空\" class=\"headerlink\" title=\"克漏字填空\"></a>克漏字填空</h3><p><img src=\"https://i.imgur.com/vHHndIP.png\"><br>首先，模型會先把文本中的字分成 character-level。如果你還記得的話，在中文中，character-level就是一個字一個字。接著再利用特殊字元 <code>[MASK]</code> 把其中一個字遮住。</p>\n<h4 id=\"在-BERT-中，共有五種特殊字元。\"><a href=\"#在-BERT-中，共有五種特殊字元。\" class=\"headerlink\" title=\"在 BERT 中，共有五種特殊字元。\"></a>在 BERT 中，共有五種特殊字元。</h4><ul>\n<li><code>[CLS]</code>：分類相關的資訊會放在這個 token 中，讓模型知道我們要進行分類，通常在下游任務才會有用。關於這個標籤是否應該存在，目前也有些爭議，大家如果有興趣看更深一點的文章，可以往<a href=\"https://www.yanxishe.com/columnDetail/26335\">這裡</a>去。</li>\n<li><code>[SEP]</code>：用於下文預測的預訓練任務中。用於分隔兩個句子的 token。</li>\n<li><code>[UNK]</code>：在 BERT 的字典中沒有的字元，會用此 token 取代。</li>\n<li><code>[PAD]</code>：記得我們在循環神經網路模型中，為了將句子調整至相同長度，會將長度不夠的句子補足到一樣的長度，那用的就是 <code>[PAD]</code> token。</li>\n<li><code>[MASK]</code>：就是克漏字專用的 token。</li>\n</ul>\n<p>所以說，克漏字填空的訓練資料就會如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">[&#x27;[CLS]&#x27;, &#x27;等&#x27;, &#x27;到&#x27;, &#x27;潮&#x27;, &#x27;水&#x27;, &#x27;[MASK]&#x27;, &#x27;了&#x27;, &#x27;，&#x27;, &#x27;就&#x27;, &#x27;知&#x27;]...</span><br></pre></td></tr></table></figure>\n\n<p>在進行訓練的時候，會將訓練資料中 15% 的詞彙遮蓋住。接著，這些輸入資料通過 BERT 之後，會得到一個 output embedding（還記得 embedding 嗎？）再透過 Linear Multi-class Classifier ，讓模型猜測（決定）被遮蓋住的字是哪一個字。模型之所以有辦法預測答案，是因為被遮蓋掉的字，其上下文的 embedding，一定會與那個被遮蓋掉的字接近，從這些 embedding 接近的字詞中去挑選最接近的，作為模型預測的正確答案。</p>\n<p>我很喜歡李宏毅老師在課堂中做的比喻：</p>\n<blockquote>\n<p>如果兩個詞彙填在同一個地方沒有違和感，那它們就有類似的 embedding。</p>\n</blockquote>\n<h3 id=\"下文預測\"><a href=\"#下文預測\" class=\"headerlink\" title=\"下文預測\"></a>下文預測</h3><p><img src=\"https://i.imgur.com/Na0QM9K.png\"><br>至於下文預測也跟克漏字填空接近。在這裡，作者給了 BERT 兩個句子，讓 BERT 判斷兩個句子之間是否有相互關聯。<code>[CLS]</code> 標籤會放在句首，這是因為在 BERT 之中使用的是 Transformer 的 encoder，其中的 Self-attention 機制，讓標籤位置並不受位置的影響。所以放在句首，仍然可以獲得所有句子的資訊。</p>\n<p>所以說，上下文預測放入模型的資料如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">[&#x27;[CLS]&#x27;, &#x27;等&#x27;, &#x27;到&#x27;, &#x27;潮&#x27;, &#x27;水&#x27;, &#x27;[MASK]&#x27;, &#x27;了&#x27;, &#x27;，&#x27;, &#x27;就&#x27;, &#x27;知&#x27;] </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"下游任務\"><a href=\"#下游任務\" class=\"headerlink\" title=\"下游任務\"></a>下游任務</h2><p><img src=\"https://leemeng.tw/images/bert/bert_fine_tuning_tasks.jpg\"></p>\n<p>當預訓練模型準備完成之後，就可以用這個模型針對我們想要完成的下游任務進行調整。前面的兩個預訓練任務就是為了要幫助以上這四個下游任務所進行的，如同昨天所是，共有四個任務：單一句子分類任務、單一句子標註任務、成對句子分類任務，以及問答任務。</p>\n<p>其中克漏字填空為的就是單一句子分類以及標註。</p>\n<ul>\n<li><strong>單一句子分類任務</strong>：多對一，我們將文本資料放進去經過微調的 BERT 模型之後，模型訓練得到 embedding，再透過訓練所得的線性分類器（Linear Classifier），將句子分類到特定類別去。例如：判斷句子情緒是正向還是負向。</li>\n<li><strong>單一句子標註任務</strong>：多對多，將文本資料放入 BERT，在每一個節點各有一個線性分類器，藉由分類器取得對應類別，例如：POS tagging。</li>\n</ul>\n<p>另外，下文預測的訓練目標就是成對句子分類任務以及問答任務。</p>\n<ul>\n<li><strong>成對句子分類任務</strong>：輸入兩句不同的句子，BERT 會藉由放在<code>[CLS]</code>中的分類資訊，對這些句子進行分類。例如，給出一段前情提要，接著放入一段句子，使模型判斷兩者是否相關，或是可否從前情提要中推斷出這段句子。</li>\n<li><strong>問答任務</strong>：將問題以及包含答案的文檔放入 BERT 之後，BERT 會將訓練所的資訊再透過先前訓練所得的 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=q\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=q\">、<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=k\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=k\">，以及 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=v\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=v\"> ，並經過激勵函數 softmax，從文檔中找出合適的答案。</li>\n</ul>\n<h2 id=\"問題與討論\"><a href=\"#問題與討論\" class=\"headerlink\" title=\"問題與討論\"></a>問題與討論</h2><p>每一個章節最後一定都要來討論一下優缺點，BERT即使作為當代最厲害的語言模型，仍然也是有他的壞處，而我們則必須衡量這些優缺點，來決定最適合的解決方案。先讓我們來看看 BERT 有哪些缺點吧，當然這裡就不再提深度學習模型的學習時間較為耗時（是要講幾遍？）。</p>\n<p>在 <a href=\"https://linguistics.uchicago.edu/people/allyson-ettinger\">Allyson Ettinger</a> 發表的<a href=\"https://aclanthology.org/2020.tacl-1.3.pdf\">What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models</a> 論文中透過實驗以及以心理語言學的角度，探討了 BERT 的可能缺陷。這裡來一一整理：</p>\n<ol>\n<li><strong>難以基於常識處理複雜的語意推斷</strong></li>\n</ol>\n<p><img src=\"https://i.imgur.com/34zLBOC.png\"></p>\n<p>從圖中，BERT 可以基於第二句的內容推斷空格中該填入的字，但是卻無法基於第一句的內容來對第二句的空格進行推斷。例如：</p>\n<blockquote>\n<p>Pablo 想要砍一些柴來做木櫃。於是他詢問了鄰居是否可以借他______ 。 </p>\n</blockquote>\n<p>人類可以很快地推斷出空格應該填入如斧頭、電鋸等可以砍柴的工具，這是基於我們看過第一句話之後所做的推斷，認為 Pablo 應該要借的是可以砍柴的工具。但模型只能基於第二句的上下文進行推斷，無法像我們一樣基於生活常識進行推斷，於是預測了像是車、房子等詞彙，這就顯示了 BERT 的限制。</p>\n<ol start=\"2\">\n<li><strong>難以預測基於語意角色的事件</strong></li>\n</ol>\n<p><img src=\"https://i.imgur.com/sEXgEcB.png\"></p>\n<p>BERT 同樣也難以判斷基於語意角色的事件。所謂的語意角色是一個語言學專有名詞，簡單來說就是在句中的參與某個事件或是帶有某個狀態的「參與者」。在圖中的範例，作者刻意將主詞對調，發現 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=BERT_%7BBASE%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=BERT_{BASE}\"> 還是會預測相同的詞彙（推測大概是因為 Self-attention 的機制），而且雖然 <img src=\"https://chart.googleapis.com/chart?cht=tx&chl=BERT_%7BLARGE%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=BERT_LARGE\"> 沒有預測跟前句相同的詞彙了，但預測結果仍不理想。兩者仍然都無法預測出適當地符合句意的詞彙。例如：</p>\n<blockquote>\n<p>紮營者回報有小女孩被熊______了！</p>\n</blockquote>\n<p>模型可以正確預測出「攻擊」、「啃咬」，但一旦兩者對調，變成：</p>\n<blockquote>\n<p>紮營者回報有熊被小女孩______了！</p>\n</blockquote>\n<p>模型預測的詞彙仍然是「攻擊」、「啃咬」。可以發現 BERT 同樣難以推斷詞彙之間的關係，並預測出符合常識的語句。</p>\n<ol start=\"3\">\n<li><strong>難以推斷否定句</strong></li>\n</ol>\n<p><img src=\"https://i.imgur.com/BKOIs9S.png\"></p>\n<p>其中提到 BERT 在推斷具有否定含義的語句時，準確度較肯定直述句還要低。從圖中可以看到說，肯定直述句的案例中，BERT 推斷出的詞放在空格中都是合理的，只是一但加上否定詞，BERT卻仍然會判斷相同的詞彙。例如：</p>\n<blockquote>\n<p>Robin 是 ＿＿＿＿＿。</p>\n</blockquote>\n<p>模型可以很好地推斷出 Robin 是隻鳥，或是一個人。一旦變成否定句：</p>\n<blockquote>\n<p>Robin 不是 _____。</p>\n</blockquote>\n<p>BERT 仍然會在空格中填入 Robin。同樣也足見 BERT，或者是說 Attention 機制的缺點。</p>\n<p>好，今天說到這！</p>\n<p>Source:</p>\n<ol>\n<li><a href=\"https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html\">進擊的 BERT：NLP 界的巨人之力與遷移學習</a></li>\n<li><a href=\"https://hackmd.io/@shaoeChen/Bky0Cnx7L#Contextualized-Word-Embedding\">李宏毅_ELMO, BERT, GPT</a></li>\n<li><a href=\"https://aclanthology.org/2020.tacl-1.3.pdf\">What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models</a></li>\n</ol>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"【NLP】Day 24: 欸！BERT！你在幹嘛呀？BERT 模型實作＆程式碼解析","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-24-%E6%AC%B8%EF%BC%81BERT%EF%BC%81%E4%BD%A0%E5%9C%A8%E5%B9%B9%E5%98%9B%E5%91%80%EF%BC%9FBERT-%E6%A8%A1%E5%9E%8B%E5%AF%A6%E4%BD%9C%EF%BC%86%E7%A8%8B%E5%BC%8F%E7%A2%BC%E8%A7%A3%E6%9E%90/","content":"<blockquote>\n<p>別想太多，做就對了！<br><strong>《捍衛戰士：獨行俠》</strong></p>\n</blockquote>\n<p>前兩天我們已經了解 BERT 的內部運作，還有 BERT 在進行語言處理上的一些缺陷。今天不聊理論，我們來簡單一一解析 Tensorflow 的官方教學 <del>沒錯，又是 Tensorflow 了喔</del>，並結合前兩天所學到的知識，來對教學中程式碼進行剖析。由於 BERT 的應用實在太多了，所以今天只會提到最簡單的情緒文本分析，也就是先前所說的多對一文本分類，若各位有興趣，或許我們明年還可以相見？在下面留言：）</p>\n<p>今天的程式碼將會全部取自於 <a href=\"https://www.tensorflow.org/text/tutorials/classify_text_with_bert\">Tensorflow 官方教學文件</a>。</p>\n<h2 id=\"透過-BERT-進行文本分類\"><a href=\"#透過-BERT-進行文本分類\" class=\"headerlink\" title=\"透過 BERT 進行文本分類\"></a>透過 BERT 進行文本分類</h2><p>在開始進入正題之前，我們得先在 <a href=\"https://colab.research.google.com/\">Colab</a> 中建置環境。Colab 是線上的編譯工具，也是機器學習常用的程式平台，更是初學者剛學習 Python 的最佳途徑。之所以 ML 常用，是因為在上面可以調用 Google 分配的運算資源，讓運算速度可以比較快一點。在開始之前，讓我們先來建置一下環境。</p>\n<h3 id=\"環境建置\"><a href=\"#環境建置\" class=\"headerlink\" title=\"環境建置\"></a>環境建置</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\"># A dependency of the preprocessing for BERT inputs</span><br><span class=\"line\">pip install -q -U &quot;tensorflow-text==2.8.*&quot;</span><br><span class=\"line\">pip install -q tf-models-official==2.7.0</span><br></pre></td></tr></table></figure>\n\n<p>搭建 BERT 通常會需要兩個步驟：資料前處理、模型建置。在這裡的資料前處理跟先前我們所學利用正規表達式來清理資料的那種前處理有一點差別。還記得我們先前學習正規表達式的時候，是為了要將文本中不需要的詞以及符號等等的刪除，只留下我們所需要的資料即可；但在搭建 BERT 時，所謂的前處理就不再是指清理資料了，而是<strong>將資料轉換成 BERT 可以理解的格式</strong>。</p>\n<p>接著我們在這邊將需要的套件引入程式中。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> shutil </span><br><span class=\"line\"><span class=\"comment\"># 這兩個套件是為了要將訓練資料的路徑讀取工具也引進來</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf <span class=\"comment\"># 就是 Tensorflow 不用多說</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> tensorflow_hub <span class=\"keyword\">as</span> hub <span class=\"comment\"># Tensorflow bert 社群</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> tensorflow_text <span class=\"keyword\">as</span> text <span class=\"comment\"># 前處理</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> official.nlp <span class=\"keyword\">import</span> optimization  <span class=\"comment\"># 優化工具</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt <span class=\"comment\"># 畫圖用的套件</span></span><br><span class=\"line\"></span><br><span class=\"line\">tf.get_logger().setLevel(<span class=\"string\">&#x27;ERROR&#x27;</span>) <span class=\"comment\"># Debug的工具，也可以不寫</span></span><br></pre></td></tr></table></figure>\n\n<p>接下來就是要引入訓練的資料。我們在這邊引用網路上常用的 imdb 情感分析資料。在這邊可以看到套件 <code>os</code> 是為了要將路徑整合，使模型可以更方便讀取我們下載的訓練資料。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 這個是資料集的下載連結，如果你複製這段 url 到瀏覽器上，也可以下載的到</span></span><br><span class=\"line\">url = <span class=\"string\">&#x27;https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">dataset = tf.keras.utils.get_file(<span class=\"string\">&#x27;aclImdb_v1.tar.gz&#x27;</span>, url,</span><br><span class=\"line\">                                  untar=<span class=\"literal\">True</span>, cache_dir=<span class=\"string\">&#x27;.&#x27;</span>,</span><br><span class=\"line\">                                  cache_subdir=<span class=\"string\">&#x27;&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 所謂的 os.path.join 就是把兩個路徑結合起來的意思，如果路徑是 ./data/，join之後就會變成 ./data/aclImdb，而以下所做的，就是要整合資料路徑，方便之後模型讀取</span></span><br><span class=\"line\">dataset_dir = os.path.join(os.path.dirname(dataset), <span class=\"string\">&#x27;aclImdb&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">train_dir = os.path.join(dataset_dir, <span class=\"string\">&#x27;train&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># remove unused folders to make it easier to load the data</span></span><br><span class=\"line\">remove_dir = os.path.join(train_dir, <span class=\"string\">&#x27;unsup&#x27;</span>)</span><br><span class=\"line\">shutil.rmtree(remove_dir)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</span><br><span class=\"line\">84131840/84125825 [==============================] - 7s 0us/step</span><br><span class=\"line\">84140032/84125825 [==============================] - 7s 0us/step</span><br></pre></td></tr></table></figure>\n\n<p>接下來就是把資料轉換成訓練資料集以及測試資料集，在這裡我們將資料以 8:2 的形式，將資料進行分割。官方文件是透過 keras 的 <code>text_dataset_from_directory</code> 函式中的一個參數來分割資料。至於所謂的 <code>batch_size</code> 就是每一批放進模型訓練的批量。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">AUTOTUNE = tf.data.AUTOTUNE</span><br><span class=\"line\">batch_size = <span class=\"number\">32</span></span><br><span class=\"line\">seed = <span class=\"number\">42</span></span><br><span class=\"line\"></span><br><span class=\"line\">raw_train_ds = tf.keras.utils.text_dataset_from_directory(</span><br><span class=\"line\">    <span class=\"string\">&#x27;aclImdb/train&#x27;</span>,</span><br><span class=\"line\">    batch_size=batch_size,</span><br><span class=\"line\">    validation_split=<span class=\"number\">0.2</span>,</span><br><span class=\"line\">    subset=<span class=\"string\">&#x27;training&#x27;</span>,</span><br><span class=\"line\">    seed=seed)</span><br><span class=\"line\"></span><br><span class=\"line\">class_names = raw_train_ds.class_names</span><br><span class=\"line\">train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><br><span class=\"line\"></span><br><span class=\"line\">val_ds = tf.keras.utils.text_dataset_from_directory(</span><br><span class=\"line\">    <span class=\"string\">&#x27;aclImdb/train&#x27;</span>,</span><br><span class=\"line\">    batch_size=batch_size,</span><br><span class=\"line\">    validation_split=<span class=\"number\">0.2</span>,</span><br><span class=\"line\">    subset=<span class=\"string\">&#x27;validation&#x27;</span>,</span><br><span class=\"line\">    seed=seed)</span><br><span class=\"line\"></span><br><span class=\"line\">val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><br><span class=\"line\"></span><br><span class=\"line\">test_ds = tf.keras.utils.text_dataset_from_directory(</span><br><span class=\"line\">    <span class=\"string\">&#x27;aclImdb/test&#x27;</span>,</span><br><span class=\"line\">    batch_size=batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\">test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Found 25000 files belonging to 2 classes.</span><br><span class=\"line\">Using 20000 files for training.</span><br><span class=\"line\">Found 25000 files belonging to 2 classes.</span><br><span class=\"line\">Using 5000 files for validation.</span><br><span class=\"line\">Found 25000 files belonging to 2 classes.</span><br></pre></td></tr></table></figure>\n\n<p>最後我們可以得到訓練資料集 <code>train_ds</code> 以及驗證資料集 <code>val_ds</code>，以及最後的測試資料集 <code>test_ds</code>。如果我們看一下裡面的資料長什麼樣子，就會發現<code>0</code>代表負面影評，<code>1</code>代表正面影評。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> text_batch, label_batch <span class=\"keyword\">in</span> train_ds.take(<span class=\"number\">1</span>):</span><br><span class=\"line\">   <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Review: <span class=\"subst\">&#123;text_batch.numpy()[<span class=\"number\">0</span>]&#125;</span>&#x27;</span>)</span><br><span class=\"line\">   label = label_batch.numpy()[i]</span><br><span class=\"line\">   <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Label : <span class=\"subst\">&#123;label&#125;</span> (<span class=\"subst\">&#123;class_names[label]&#125;</span>)&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>這邊就不印出三份資料了，印出第一份讓大家來有點概念即可。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Review: b&#x27;&quot;Pandemonium&quot; is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. &quot;Airplane&quot;, &quot;The Naked Gun&quot; trilogy, &quot;Blazing Saddles&quot;, &quot;High Anxiety&quot;, and &quot;Spaceballs&quot; are some of my favorite comedies that spoof a particular genre. &quot;Pandemonium&quot; is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\&#x27;t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\&#x27;s all this film has going for it. Geez, &quot;Scream&quot; had more laughs than this film and that was more of a horror film. How bizarre is that?&lt;br /&gt;&lt;br /&gt;*1/2 (out of four)&#x27;</span><br><span class=\"line\">Label : 0 (neg)</span><br></pre></td></tr></table></figure>\n\n<p>記得我們先前所說的遷移學習（Transfer Learning）嗎？我們可以取用別人已經訓練好的預訓練模型，再針對預訓練模型加上我們自己的資料，讓模型找出特徵之後，來解決自然語言處理的下游任務。那該要到哪裡去下載別人已經預訓練好的模型呢？網路上有兩個常用的平台，一個是我們現在用的 Tensorflow 的 hub ，另一個則是大家更常用的抱臉怪（huggingface），在抱臉怪上有各式各樣別人從各領域的文本所訓練的預訓練模型，而大家可以在上面任意取用符合自己需求的預訓練模型。</p>\n<ul>\n<li><a href=\"https://huggingface.co/\">Huggingface</a></li>\n<li><a href=\"https://www.tensorflow.org/hub?hl=zh-tw\">Tensorflow Hub</a></li>\n</ul>\n<p>而以下所做的，就是取用別人已經訓練好的模型，並把它用在我們的任務當中。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">tfhub_handle_encoder = <span class=\"string\">&quot;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1&quot;</span> <span class=\"comment\"># 這是預訓練模型</span></span><br><span class=\"line\">tfhub_handle_preprocess = <span class=\"string\">&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;</span> <span class=\"comment\"># 這是 BERT 前處理需要用的模型</span></span><br></pre></td></tr></table></figure>\n\n<p>以下就可以來研究看看輸入進去的資料會有哪些。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) <span class=\"comment\"># 呼叫前處理模型</span></span><br><span class=\"line\">text_test = [<span class=\"string\">&#x27;this is such an amazing movie!&#x27;</span>] <span class=\"comment\"># 先輸入簡單的句子看看會變成什麼樣子</span></span><br><span class=\"line\">text_preprocessed = bert_preprocess_model(text_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Keys       : <span class=\"subst\">&#123;<span class=\"built_in\">list</span>(text_preprocessed.keys())&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Shape      : <span class=\"subst\">&#123;text_preprocessed[<span class=\"string\">&quot;input_word_ids&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Word Ids   : <span class=\"subst\">&#123;text_preprocessed[<span class=\"string\">&quot;input_word_ids&quot;</span>][<span class=\"number\">0</span>, :<span class=\"number\">12</span>]&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Input Mask : <span class=\"subst\">&#123;text_preprocessed[<span class=\"string\">&quot;input_mask&quot;</span>][<span class=\"number\">0</span>, :<span class=\"number\">12</span>]&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Type Ids   : <span class=\"subst\">&#123;text_preprocessed[<span class=\"string\">&quot;input_type_ids&quot;</span>][<span class=\"number\">0</span>, :<span class=\"number\">12</span>]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Keys       : [&#x27;input_type_ids&#x27;, &#x27;input_word_ids&#x27;, &#x27;input_mask&#x27;]</span><br><span class=\"line\">Shape      : (1, 128)</span><br><span class=\"line\">Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]</span><br><span class=\"line\">Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]</span><br><span class=\"line\">Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]</span><br></pre></td></tr></table></figure>\n\n<p>我們可以看到輸出總共分成 <code>input_word_ids</code>、<code>input_mask</code>、以及 <code>input_type_ids</code>。其中<code>input_word_ids</code>就是每個文字分配的識別碼，可以讓模型分別對應回去的數字。 <code>input_mask</code> 則是在進行 Masked 的程序（我的理解，若有錯還請不吝賜教），而<code>input_type_ids</code>則是會依照不同句子給予不同的id，也就是識別句子的功能。這邊是因為測試的句子只有一句，所以只會有 0 這個數字。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">bert_model = hub.KerasLayer(tfhub_handle_encoder)</span><br><span class=\"line\">bert_results = bert_model(text_preprocessed)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Loaded BERT: <span class=\"subst\">&#123;tfhub_handle_encoder&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Pooled Outputs Shape:<span class=\"subst\">&#123;bert_results[<span class=\"string\">&quot;pooled_output&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Pooled Outputs Values:<span class=\"subst\">&#123;bert_results[<span class=\"string\">&quot;pooled_output&quot;</span>][<span class=\"number\">0</span>, :<span class=\"number\">12</span>]&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Sequence Outputs Shape:<span class=\"subst\">&#123;bert_results[<span class=\"string\">&quot;sequence_output&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Sequence Outputs Values:<span class=\"subst\">&#123;bert_results[<span class=\"string\">&quot;sequence_output&quot;</span>][<span class=\"number\">0</span>, :<span class=\"number\">12</span>]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1</span><br><span class=\"line\">Pooled Outputs Shape:(1, 512)</span><br><span class=\"line\">Pooled Outputs Values:[ 0.76262873  0.99280983 -0.1861186   0.36673835  0.15233682  0.65504444</span><br><span class=\"line\">  0.9681154  -0.9486272   0.00216158 -0.9877732   0.0684272  -0.9763061 ]</span><br><span class=\"line\">Sequence Outputs Shape:(1, 128, 512)</span><br><span class=\"line\">Sequence Outputs Values:[[-0.28946388  0.3432126   0.33231565 ...  0.21300787  0.7102078</span><br><span class=\"line\">  -0.05771166]</span><br><span class=\"line\"> [-0.28742015  0.31981024 -0.2301858  ...  0.58455074 -0.21329722</span><br><span class=\"line\">   0.7269209 ]</span><br><span class=\"line\"> [-0.66157013  0.6887685  -0.87432927 ...  0.10877253 -0.26173282</span><br><span class=\"line\">   0.47855264]</span><br><span class=\"line\"> ...</span><br><span class=\"line\"> [-0.2256118  -0.28925604 -0.07064401 ...  0.4756601   0.8327715</span><br><span class=\"line\">   0.40025353]</span><br><span class=\"line\"> [-0.29824278 -0.27473143 -0.05450511 ...  0.48849759  1.0955356</span><br><span class=\"line\">   0.18163344]</span><br><span class=\"line\"> [-0.44378197  0.00930723  0.07223766 ...  0.1729009   1.1833246</span><br><span class=\"line\">   0.07897988]]</span><br></pre></td></tr></table></figure>\n\n<p>在輸出中，總共有三個不同需要注意的欄位：<code>pooled_output</code>、<code>sequence_output</code>、<code>encoder_outputs</code>。</p>\n<ul>\n<li><code>pooled_output</code>：代表所有資料經由 BERT 之後所取出的 embedding。在這裡代表的就是電影評論資料整體的 embedding。</li>\n<li><code>sequence_output</code>：代表的是每一個 token 在上下文中的 embedding。</li>\n</ul>\n<p>透過 BERT 取得了需要的 embedding 之後，接下來就是要進行下游任務，在這裡就是進行文本分類。</p>\n<p>Tutorial 在這裡用一個函式來定義模型搭建：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">build_classifier_model</span>():</span><br><span class=\"line\">  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=<span class=\"string\">&#x27;text&#x27;</span>)</span><br><span class=\"line\">  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=<span class=\"string\">&#x27;preprocessing&#x27;</span>)</span><br><span class=\"line\">  encoder_inputs = preprocessing_layer(text_input)</span><br><span class=\"line\">  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=<span class=\"literal\">True</span>, name=<span class=\"string\">&#x27;BERT_encoder&#x27;</span>)</span><br><span class=\"line\">  outputs = encoder(encoder_inputs)</span><br><span class=\"line\">  net = outputs[<span class=\"string\">&#x27;pooled_output&#x27;</span>]</span><br><span class=\"line\">  net = tf.keras.layers.Dropout(<span class=\"number\">0.1</span>)(net)</span><br><span class=\"line\">  net = tf.keras.layers.Dense(<span class=\"number\">1</span>, activation=<span class=\"literal\">None</span>, name=<span class=\"string\">&#x27;classifier&#x27;</span>)(net)</span><br><span class=\"line\">  <span class=\"keyword\">return</span> tf.keras.Model(text_input, net)</span><br></pre></td></tr></table></figure>\n\n<p>模型架構如下：<br><img src=\"https://www.tensorflow.org/static/text/tutorials/classify_text_with_bert_files/output_0EmzyHZXKIpm_0.png\"><br>Source: <a href=\"https://www.tensorflow.org/text/tutorials/classify_text_with_bert\">Tensorflow</a></p>\n<p>接下來就是要用 Cross entropy 來計算損失函數，前面沒有介紹到損失函數。簡單的說，損失函數就是在進行梯度下降時，模型在評估與正確答案的相近程度時所計算的最小化最佳解問題，即為負對數似然（negative log-likelihood）。</p>\n<p>接著設定後面的模型參數。Learning rate 則採取原論文建議的參數：3e-5。原論文建議三種參數，分別為5e-5、3e-5、2e-5。至於 epoch、batch、以及 learning rate，大家可以來看這位前輩寫的<a href=\"https://medium.com/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7-%E5%80%92%E5%BA%95%E6%9C%89%E5%A4%9A%E6%99%BA%E6%85%A7/epoch-batch-size-iteration-learning-rate-b62bf6334c49\">文章</a>。最後在這裡將優化參數加進 <code>create_optimizer</code> 函式中，以利後續在編譯時對模型進行優化。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">loss = tf.keras.losses.BinaryCrossentropy(from_logits=<span class=\"literal\">True</span>)</span><br><span class=\"line\">metrics = tf.metrics.BinaryAccuracy()</span><br><span class=\"line\"></span><br><span class=\"line\">epochs = <span class=\"number\">5</span></span><br><span class=\"line\">steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()</span><br><span class=\"line\">num_train_steps = steps_per_epoch * epochs</span><br><span class=\"line\">num_warmup_steps = <span class=\"built_in\">int</span>(<span class=\"number\">0.1</span>*num_train_steps)</span><br><span class=\"line\"></span><br><span class=\"line\">init_lr = <span class=\"number\">3e-5</span></span><br><span class=\"line\">optimizer = optimization.create_optimizer(init_lr=init_lr,</span><br><span class=\"line\">                                          num_train_steps=num_train_steps,</span><br><span class=\"line\">                                          num_warmup_steps=num_warmup_steps,</span><br><span class=\"line\">                                          optimizer_type=<span class=\"string\">&#x27;adamw&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>最後就是開始訓練：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">classifier_model.<span class=\"built_in\">compile</span>(optimizer=optimizer,</span><br><span class=\"line\">                         loss=loss,</span><br><span class=\"line\">                         metrics=metrics)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Training model with <span class=\"subst\">&#123;tfhub_handle_encoder&#125;</span>&#x27;</span>)</span><br><span class=\"line\">history = classifier_model.fit(x=train_ds,</span><br><span class=\"line\">                               validation_data=val_ds,</span><br><span class=\"line\">                               epochs=epochs)                         </span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1</span><br><span class=\"line\">Epoch 1/5</span><br><span class=\"line\">625/625 [==============================] - 90s 136ms/step - loss: 0.4784 - binary_accuracy: 0.7528 - val_loss: 0.3713 - val_binary_accuracy: 0.8350</span><br><span class=\"line\">Epoch 2/5</span><br><span class=\"line\">625/625 [==============================] - 83s 133ms/step - loss: 0.3295 - binary_accuracy: 0.8525 - val_loss: 0.3675 - val_binary_accuracy: 0.8472</span><br><span class=\"line\">Epoch 3/5</span><br><span class=\"line\">625/625 [==============================] - 83s 133ms/step - loss: 0.2503 - binary_accuracy: 0.8963 - val_loss: 0.3905 - val_binary_accuracy: 0.8470</span><br><span class=\"line\">Epoch 4/5</span><br><span class=\"line\">625/625 [==============================] - 83s 133ms/step - loss: 0.1930 - binary_accuracy: 0.9240 - val_loss: 0.4566 - val_binary_accuracy: 0.8506</span><br><span class=\"line\">Epoch 5/5</span><br><span class=\"line\">625/625 [==============================] - 83s 133ms/step - loss: 0.1526 - binary_accuracy: 0.9429 - val_loss: 0.4813 - val_binary_accuracy: 0.8468</span><br></pre></td></tr></table></figure>\n\n<p>接著就可以來看模型表現如何了！</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\">loss, accuracy = classifier_model.evaluate(test_ds)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Loss: <span class=\"subst\">&#123;loss&#125;</span>&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Accuracy: <span class=\"subst\">&#123;accuracy&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">782/782 [==============================] - 59s 75ms/step - loss: 0.4568 - binary_accuracy: 0.8557</span><br><span class=\"line\">Loss: 0.45678260922431946</span><br><span class=\"line\">Accuracy: 0.855679988861084</span><br></pre></td></tr></table></figure>\n\n<p>我們在這裡可以得知模型的準確率約為 0.86。</p>\n<h2 id=\"結語\"><a href=\"#結語\" class=\"headerlink\" title=\"結語\"></a>結語</h2><p>其實若你實作過一遍，就會知道 BERT 的訓練時間，相較於前面所介紹過的任何一種模型，像是 LSTM、RNN等這些深度學習模型來說，體感上有非常明顯的差距。BERT 光是這麼一點點資料，就可能會需要訓練到快一個小時，資料一多，甚至可能要跑到一週兩週都是家常便飯，所以這也是 BERT 一個最大的缺點之一。</p>\n<p>另外，我國的中研院所開發的 CKIP 繁體中文預訓練模型，也發佈在 Huggingface 上了，可以<a href=\"https://huggingface.co/ckiplab\">點我</a> 進去看看，CKIP透過 BERT 完成了命名實體辨識、詞性標註等任務，我們也可以基於模型，再訓練符合下游任務的模型。除此之外，Huggingface 上面有很多好玩的<a href=\"https://huggingface.co/spaces\">模型實作</a>，大家有興趣也可以去玩玩看，比如說像是文本生成、QA等等的 demo，或許大家可以對自然語言處理有更多的想像空間，這樣的社群需要你我一起來建構與發揮。</p>\n"},{"title":"【NLP】Day 25: 自然語言處理的另外一種想像！Articut、Loki 以及他的好助手們！（上）","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-25-%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E7%9A%84%E5%8F%A6%E5%A4%96%E4%B8%80%E7%A8%AE%E6%83%B3%E5%83%8F%EF%BC%81Articut%E3%80%81Loki-%E4%BB%A5%E5%8F%8A%E4%BB%96%E7%9A%84%E5%A5%BD%E5%8A%A9%E6%89%8B%E5%80%91%EF%BC%81%EF%BC%88%E4%B8%8A%EF%BC%89/","content":"<blockquote>\n<p>我們尋找的並不應該是英雄，而是一個好的想法<br><strong>Noam Chomsky</strong></p>\n</blockquote>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>還記得碩一下的時候，Lab 的老師找了以前一個現在在擔任軟體工程師的同學來課堂上演講，還有職涯分享。這位工程師說：「自然語言處理在台灣還不算特別盛行，但我覺得有慢慢被重視的感覺。就像每年 台灣PyCon 都會有一位名字叫 PeterWolf 的人，都會在 PyCon 上分享自然語言處理的技術，我覺得他超級猛。」</p>\n<p>心想，真巧，我在大學時就擔任了他第一屆的實習生呢。</p>\n<p>其實上了語言所之後，對自然語言處理以及計算語言學才開始漸漸理出個頭緒，如今再回頭來看當時所學的這些，才發現，若要單純依靠先前所介紹的那些模型，又要求要在短時間內訓練並達到相同目的，實在是不簡單。我想，過去的鐵人賽中，也尚未有其他參賽者介紹過 PeterWolf 以及他的團隊所開發的這些產品以及工具，那這次就先由我來拋磚引玉一下吧。</p>\n<p>不知道你記不記得，我在第八天的文章：<a href=\"https://ithelp.ithome.com.tw/articles/10296818\">【NLP】Day 8: 你拿定主意的話…葛萊芬多！BOW＆TF-IDF</a> 之中曾經提到，在過去的研究脈絡中，所謂的<strong>語言模型，指的是運用統計方法以及機率來估計語言使用的可能性嗎？</strong> 在那之後，計算語言學以及自然語言處理的應用，大多都是延續這個邏輯脈絡，發展出各式各樣不同的模型，如 RNN、N-Gram 等。</p>\n<p>今天，我需要你把先前所學的那些模型，<strong>通通忘掉</strong>。</p>\n<h2 id=\"語言學是什麼？能吃嗎？\"><a href=\"#語言學是什麼？能吃嗎？\" class=\"headerlink\" title=\"語言學是什麼？能吃嗎？\"></a>語言學是什麼？能吃嗎？</h2><p>台灣的各位想必對語言學應該都很陌生，每次跟其他人說自己是讀語言學的時候，別人總會說：「哇！那你是讀哪種語言？」心裡總是百般的無奈，我想這句話對語言學人來說，一定都心有戚戚焉。其實語言學並不是在學習「語言」，而是學習「語言結構」的一門「科學方法」。語言學分成了許多不同的支派，研究聲音的語音學、音韻學，還有語用學、語意學，一直到我們今天會提到「一點點」的句法學，都是語言學的大家庭。</p>\n<p>太複雜了嗎？簡單來說，語言學就是一門</p>\n<blockquote>\n<p>觀察語言，並歸納出一系列規則的學問。</p>\n</blockquote>\n<p>研究語句架構的句法學更是一門已經研究多年的學問，已經有相對較為龐大的研究基礎。我還記得，當時 Peter 對我們所有實習生說：</p>\n<blockquote>\n<p>「明明語言學家已經分析並歸納出了語言系統的運作規則，那就來好好運用那些規則，不應該繼續以為只能用統計機率的角度來進行斷詞的計算。」</p>\n</blockquote>\n<p>PeterWolf 會這麼說，原因是：</p>\n<p>在過去，語言學界的巨擘 Noam Chomsky 提出了一個假設。在這個假設裡，所有的語言，在底層都擁有相同的文法結構。 Chomsky 的理論革新點在於，只要是符合語法規則的語句，都可以透過程式語言邏輯產生。而每一句話皆可以透過語言規則將其進行歸納，其表現方法就是透過句法樹的形式呈現，而 Chomsky 後來也發展出了一套句法範疇理論，也就是 X-bar theory，X-bar theory 是每一個語言學專業的學生都一定學過的句法學理論，一定也基於 X-bar theory 畫了不少句法樹。</p>\n<p>也就是說：</p>\n<blockquote>\n<p>以電腦科學的角度來看，語法規則看似是無窮無盡，但在語言學的觀點裡，語言學規則是列得完的！</p>\n</blockquote>\n<blockquote>\n<h4 id=\"而-Articut，就是基於-X-bar-theory-所打造的斷詞系統。\"><a href=\"#而-Articut，就是基於-X-bar-theory-所打造的斷詞系統。\" class=\"headerlink\" title=\"而 Articut，就是基於 X-bar theory 所打造的斷詞系統。\"></a>而 Articut，就是基於 X-bar theory 所打造的斷詞系統。</h4></blockquote>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d7/The_X-bar_structure_of_%22John_studies_linguistics_at_the_university%22.png/600px-The_X-bar_structure_of_%22John_studies_linguistics_at_the_university%22.png\"><br><a href=\"https://en.wikipedia.org/wiki/X-bar_theory\">Source</a>: 基於X-bar theory 規則所畫出來的句法樹</p>\n<h3 id=\"Chomsky-是怎麼看待-NLP-的？來源\"><a href=\"#Chomsky-是怎麼看待-NLP-的？來源\" class=\"headerlink\" title=\"Chomsky 是怎麼看待 NLP 的？來源\"></a>Chomsky 是怎麼看待 NLP 的？<a href=\"https://kopu.chat/%E5%96%AC%E5%A7%86%E6%96%AF%E5%9F%BA%E7%9A%84%E6%99%AE%E9%81%8D%E8%AA%9E%E6%B3%95%E7%90%86%E8%AB%96/#lwptoc8\">來源</a></h3><p>在 2011 年，麻省理工學院所舉辦的研討會，就有人問了 Chomsky 這樣的一個問題：</p>\n<blockquote>\n<p>如何看待機器學習、機率模型近年來被應用在自然語言處理 (NLP) 與認知科學領域的趨勢。畢竟機率的方法在喬姆斯基的年代並不是主流的研究方法。</p>\n</blockquote>\n<p>Chomsky 回應道，確實現在有許多的自然語言處理相關研究，是透過<strong>統計模型</strong>來解決各式各樣的語言學問題，有些成功，有些失敗。成功的案例，大多是因為統計方法跟語言的基本理論的結合。但若不考慮語言的實際結構就應用統計方法，那所謂的成功就不是正常意義下的成功。Chomsky 用蜜蜂的行為研究做了個比喻，他說，這就像科學家只是對蜜蜂錄影，並記錄過去蜜蜂的行為資料，並透過這些資料來預測蜜蜂的行為。機器學習的方法可能可以預測的很好，但並不算真正科學意義上的成功，因為沒有真正了解語言底層的實際架構。 <del>老人家一番話讓一堆人中槍XD</del></p>\n<p>但… Articut 辦到了？</p>\n<h2 id=\"Articut\"><a href=\"#Articut\" class=\"headerlink\" title=\"Articut\"></a>Articut</h2><p>Articut 是基於語法規則所搭建的斷詞系統，跟先前我們介紹過的斷詞系統不一樣，Articut 可以離線運行，且不需要大數據訓練，因此修改模型速度也較迅速，即使是新詞，也可以得到正確的結果。但我想要特別提出以下幾點來講：</p>\n<h3 id=\"極高的模型解釋力\"><a href=\"#極高的模型解釋力\" class=\"headerlink\" title=\"極高的模型解釋力\"></a>極高的模型解釋力</h3><p>在所有斷詞系統中，Articut 最吸引我的就是有著<strong>非常非常高的解釋力</strong>（在最後一天的文章中，我會好好說明為什麼我認為解釋力非常重要）舉例來說：</p>\n<blockquote>\n<p>張三愛女生</p>\n</blockquote>\n<p>CKIP &#x3D;&gt; <code>張三愛(Nb)　女生(Na)</code>  且其語義辨識結果是 <code>張三愛 (person) 女生 (woman)</code></p>\n<p>Articut &#x3D;&gt; <code>張三(ENTITY_person)  愛(ACTION_verb)  女生(ENTITY_noun)</code></p>\n<p>CKIP （及其它基於資料模型的方案）  的運算是由左到右，遇到「張」這個中文姓氏時，就開始計算。因為中文人名「絕大部份是三個字符」，因此接下來就會在這「三個字符」的長度裡計算它是人名的可能性，的確「張三愛」有可能是個人名；接著後面的「女」+「生」+ ending 的分佈裡，也很容易在資料裡出現「女生」這樣的組合，因此結果就是「張三愛&#x2F;女生」。</p>\n<p>但 Articut 是基於「句法 （syntax）」的，而一個完整的句子一定要有動詞！動詞出現以前，這個句子的意義是模糊的。就像日文的動詞在最後面，是一種 SOV 語言，V 表示動詞。在句子的動詞出現以前，是無法確定對方究竟說的是什麼的。</p>\n<p>所以，Articut 的運算步驟會是…「這五個字符，哪一個有動詞的時態標記啊？哇～都沒有，那麼因為中文是動詞置中的 SVO 語言，所以先假設最靠近中間的那幾個是動詞。於是「愛」就先被擺在上述那個英文的句法樹的 V 的位置 （剛好英文也是 SVO 語言，所以它的 V 也在句子的中間）。」</p>\n<p>然後開始第二步的推測「如果 V&#x3D;愛，那麼前面有一個 NP  嗎？有，剛好有一個中文姓氏+數字。這在結構和音韻上都是可行的中文單名人名。那後面有其它的標記表示它不是一個名詞嗎？（e.g., 「很」是副詞&#x2F;形容詞標記，但這句並不是「張三愛很滿」），沒有！那就把後面的字符當名詞，組成一個句法樹看看它能爬到多高的節點位置。」於是就能產生出「張三_person&#x2F;愛_verb&#x2F;女生_noun」這樣同時處理完詞性與斷詞的結果了。 </p>\n<h3 id=\"功能多合一\"><a href=\"#功能多合一\" class=\"headerlink\" title=\"功能多合一\"></a>功能多合一</h3><p>除了解釋力超級高之外，我覺得功能多合一的 Articut 也非常厲害。做一次斷詞，就可以得到不只斷詞結果，也可以得到 Part-of-speech tag，更可以進行命名實體辨識。由於是台灣本土的公司，所以也在原來的基礎上加入了許多台灣的資料，例如觀光景點。除此之外，也加入了化學命名的實體辨識。<del>越寫越覺得好像在業配呀</del> 如果想知道更多，可以按這個<a href=\"https://github.com/Droidtown/ArticutAPI/wiki/09_%E5%91%BD%E5%90%8D%E5%AF%A6%E9%AB%94%E6%A8%99%E8%A8%98-(NER)\">連結</a>！</p>\n<p>我覺得這是自然語言處理的另外一種可能性。用機率統計的方式計算語言固然是一種觀點，也是解決問題的方法，但或許也可以考慮看看用這個觀點去看待自然語言處理，或許可以有更多不一樣的想法。</p>\n<p>好，今天先講到這，明天再來介紹基於 Articut 的自然語言理解引擎 Loki。</p>\n"},{"title":"【NLP】Day 26: 自然語言處理的另外一種想像！Articut、Loki 以及他的好助手們！（下）","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-26-%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E7%9A%84%E5%8F%A6%E5%A4%96%E4%B8%80%E7%A8%AE%E6%83%B3%E5%83%8F%EF%BC%81Articut%E3%80%81Loki-%E4%BB%A5%E5%8F%8A%E4%BB%96%E7%9A%84%E5%A5%BD%E5%8A%A9%E6%89%8B%E5%80%91%EF%BC%81%EF%BC%88%E4%B8%8B%EF%BC%89/","content":"<blockquote>\n<p>你所做的這些浮誇的浪漫舉動—其實你做的、你所說的根本一點都不重要，真正重要的是你的意圖。真正重要的是你願意花時間在那個你在乎的人身上，告訴他：「我願意就這樣看著你，也願意聆聽你的聲音。」我很清楚，你現在需要的是什麼，然後我現在在告訴你，你知道這件事情，對我來說有多麼重要。<br>Jack Pearson《這就是我們》</p>\n</blockquote>\n<p>昨天分享了 Articut ，今天來介紹 Loki。</p>\n<p>有了 Articut 的基礎，Loki 就誕生了。不是雷神的弟弟，而是 Linguistic Oriented Keyword Interface (語言導向的關鍵詞介面)，是一種自然語言理解的引擎。但在開始之前，我們得先知道，什麼是自然語言理解？</p>\n<h2 id=\"自然語言理解（Natural-Language-Understanding-NLU）\"><a href=\"#自然語言理解（Natural-Language-Understanding-NLU）\" class=\"headerlink\" title=\"自然語言理解（Natural Language Understanding, NLU）\"></a>自然語言理解（Natural Language Understanding, NLU）</h2><p>自然語言理解，是自然語言處理的其中一種技術應用，旨在讓電腦可以理解更複雜的語言輸入。我們現在有很多日常生活中的科技都會需要用到自然語言理解的技術，比如說我們平常手機常用的語音助理，就是一種自然語言理解。其最終目的在於，我們有辦法用日常生活中所運用的自然語言，直接與機器進行溝通。這是為了讓人機互動可以更為順暢，且幫助未來的生活可以更加便利，不需要讓人類順應機器的理解模式，來與機器互動。如果我們想知道高鐵時刻表，我們會在 Google 打入：</p>\n<blockquote>\n<p>高鐵 時刻表</p>\n</blockquote>\n<p>但若電腦可以直接理解你的自然語言，我們就不需要使用這種順應機器的說話方式，我們可以直接說：</p>\n<blockquote>\n<p>我想知道有沒有兩點左右台北往台南的高鐵</p>\n</blockquote>\n<p>從這句話中，電腦可以立刻就知道你的<strong>意圖</strong>是想要知道「兩點左右」<strong>（時間）</strong>是否有「台北」<strong>（起點站）</strong>到「台南」<strong>（終點站）</strong>的「高鐵」<strong>交通工具</strong>，這麼一來，電腦就可以直接回答：</p>\n<blockquote>\n<p>兩點十六分有一班台北往台南的高鐵，請問要幫您訂票嗎？</p>\n</blockquote>\n<p>這是自然語言理解的技術上所欲達成的目標。想像一下，我們可以透過日常生活的對話，直接告訴電腦我想做的事情，不正是大家想像中未來世界應該要有的模樣嗎？</p>\n<h3 id=\"眼尖的朋友可能發現新名詞了：意圖\"><a href=\"#眼尖的朋友可能發現新名詞了：意圖\" class=\"headerlink\" title=\"眼尖的朋友可能發現新名詞了：意圖\"></a>眼尖的朋友可能發現新名詞了：意圖</h3><p>我們人類說的每一句話，裡面都一定隱藏著所謂的「意圖」，而我們就是透過這個「意圖」來互相理解彼此，對於電腦來說也是，而我前面所舉的例子，就是。我們重新回到訂票的案例。</p>\n<p>例如：</p>\n<blockquote>\n<p>我要一張兩點以前左營往南港的票</p>\n</blockquote>\n<p>在這句話中，就包含了三個意圖：「車票張數」、「乘車時間」、「起點站」、「終點站」。</p>\n<p>實習時，Peter 跟我們說，他的假設是，在同一個情境下，為了達成某種目的或意圖，使用的句型是有限的。想想看，假如要買車票，你可以想出幾種不一樣的說法？直到想不到為止，你就會發現能夠買車票的句型，其實也就那幾句而已。那在這樣的基礎下，也就是「意圖」＋「有限句型」兩個要素之下，Loki 誕生了。</p>\n<h3 id=\"Loki\"><a href=\"#Loki\" class=\"headerlink\" title=\"Loki\"></a>Loki</h3><p>在這邊就以我當時實作的專案為例，來進行簡單的 Tutorial。</p>\n<ol>\n<li>首先先來到卓騰的 <a href=\"https://api.droidtown.co/member/\">API 官網</a></li>\n<li>在登入畫面，選取 Loki 之後，可以看到這樣的畫面</li>\n</ol>\n<p><img src=\"https://i.imgur.com/sB7GFMf.png\"></p>\n<ol start=\"3\">\n<li>選取專案後，就可以看到在專案底下的不同意圖，在建立意圖前，你必須要先思考的是情境中會應用到的句型，其中可能會有哪些意圖（intent），例如我這裡就是大人小孩的車票張數、起點站、終點站、啟程時間、抵達時間、以及詢問時間的句型。</li>\n</ol>\n<p><img src=\"https://i.imgur.com/4PDNoin.png\"></p>\n<ol start=\"4\">\n<li>我們在這裡就先以 <code>departure</code> 意圖為例，在建立好意圖後，我們可以看到第三部分。在這裡要思考的是<strong>跟售票員說從哪裡到哪裡的時候，有幾種說法？</strong>，當初在建置模型時，大家集思廣益了這些句型。</li>\n</ol>\n<p><img src=\"https://i.imgur.com/hebPIqz.png\"></p>\n<ol start=\"5\">\n<li>按下全句分析後，會出現以下畫面：</li>\n</ol>\n<p><img src=\"https://i.imgur.com/3vvoC5l.png\"></p>\n<p>垃圾桶就是刪除語句，這應該大家都懂；另外你可能可以發現，在每一句型右邊，都會有數字。以第4句為例，第4句右邊有一個數字6，這代表第4句跟第6句對 Loki 來說是相同的。至於你問我為什麼我還有這些數字，這是因為我實習的時候還沒有這功能啊！<del>歲月催人老</del></p>\n<p>來一一解說各個按鈕的功能各是什麼：若你按下數字，會出現 Articut 的斷詞結果，例如：</p>\n<p><img src=\"https://i.imgur.com/jkUqMwi.png\"></p>\n<p>在這裡，你就可以確認 Articut 是否有正確理解你的句型。若進階一點的人，則可以按下 regex 確認句型是否有被正確理解。若你按下紅色框框，在這裡我按下「出發」，則會出現以下視窗：</p>\n<p><img src=\"https://i.imgur.com/3puZiqw.png\"></p>\n<p>在這裡，系統也會根據機器學習的運算結果，來提示你同義詞可能有哪些，若你勾選這些同義詞，則可以讓 Loki 同時也理解這些詞彙。</p>\n<ol start=\"6\">\n<li>假如我今天想要測試句子，則可以在區塊 6 測試。例如：我想測試「自新竹」是否有對到句型，則可以這麼做：</li>\n</ol>\n<p><img src=\"https://i.imgur.com/M6JItSk.png\"></p>\n<p>從圖中可以發現，「自新竹」對到的是「從台北」的語句，因為「自」跟「從」都是 FUNC_inner，「台北」跟「新竹」則都是 LOCATION。所以這兩種是同一種句型。等一切都確定之後，你再按下部署模型之後，就可以回到上上一頁，再按下<strong>「下載範本」</strong>，並選擇 Python，就會出現 Python 的範本。我們就用編譯器打開樣本。</p>\n<ol start=\"7\">\n<li>來一一介紹範本功能，若按到 <code>intent/Loki_departure.py</code>，你就可以看到以下畫面。</li>\n</ol>\n<p><img src=\"https://i.imgur.com/MqXplqq.png\"></p>\n<p>在這裡，你可以設定抓到意圖之後要做些什麼，我在這裡做的是將抓取到的資訊存到<code>dict</code>字典裡，接著就可以透過這些搜集到的資料來做後續的任務。</p>\n<p><img src=\"https://i.imgur.com/G4ammO9.png\"></p>\n<ol start=\"8\">\n<li>運行程式檔 <code>python3 TransportationBot.py</code> 就可以一次測試句子在所有意圖的運行結果為何。</li>\n</ol>\n<p><img src=\"https://i.imgur.com/hCaPzHc.png\"></p>\n<h2 id=\"結語\"><a href=\"#結語\" class=\"headerlink\" title=\"結語\"></a>結語</h2><p>在過去抓取意圖的處理方式，通常也運用統計語言模型，搭配上 embedding 去讓模型猜測你可能的意圖是什麼，但大家都不知道為什麼模型做出的樣的決定，就變得比較像是模型「剛好」猜中你想表達的意思，因此 NLU 對統計觀點的自然語言處理來說，是相對困難的技術。因為解釋力低、而且難以訓練。但是運用 Loki，我們就可以合理推斷並猜測，「從台北」的句型與「自新竹」一致，所以對模型來說，只要能抓到「從台北」的台北，那麼即使你用相同句型的「自新竹」，同樣也能讓模型透過「從台北」的句型抓到「新竹」。</p>\n<h4 id=\"延伸閱讀：深度研討｜NLP≠NLU，機器學習無法理解人類語言\"><a href=\"#延伸閱讀：深度研討｜NLP≠NLU，機器學習無法理解人類語言\" class=\"headerlink\" title=\"延伸閱讀：深度研討｜NLP≠NLU，機器學習無法理解人類語言\"></a>延伸閱讀：<a href=\"https://communeit.medium.com/%E6%B7%B1%E5%BA%A6%E7%A0%94%E8%A8%8E-nlp-nlu-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%84%A1%E6%B3%95%E7%90%86%E8%A7%A3%E4%BA%BA%E9%A1%9E%E8%AA%9E%E8%A8%80-4ea83b7bd356\">深度研討｜NLP≠NLU，機器學習無法理解人類語言</a></h4>"},{"title":"【NLP】Day 4: 什麼！文本也可以偷天換日？正規表達式 (2) 替換","url":"/2022/09/18/%E3%80%90NLP%E3%80%91Day-4-%E4%BB%80%E9%BA%BC%EF%BC%81%E6%96%87%E6%9C%AC%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%81%B7%E5%A4%A9%E6%8F%9B%E6%97%A5%EF%BC%9F%E6%AD%A3%E8%A6%8F%E8%A1%A8%E9%81%94%E5%BC%8F-2-%E6%9B%BF%E6%8F%9B/","content":"<blockquote>\n<p>你低著頭，他們就會知道你在說謊；即便你抬起頭，他們也會知道其實你根本不知道真相。如果只用四個字就能說明清楚，就別用七個字。身體不要搖來搖去，要用堅定的眼神看著對方的眼睛，但別一直盯著看。說話清楚，但別太讓人印象深刻，可以偶爾幽默一下，但是也不要過頭到讓他捧腹大笑。這麼一來，他就只會在當下對你有好感，而你一離開，他也會立刻忘了你這個人。而且看在老天的份上，無論你想做什麼，都別做，因為在任何情況下…<br><strong><a href=\"https://www.youtube.com/watch?v=VTKgyZZP5KQ\">Rusty Ryan《瞞天過海》</a></strong></p>\n</blockquote>\n<p>昨天我們已經簡單地認識了什麼是正規表達式（Regular Expression），也瞭解了正規表達式的性質，更認識了正規表達式在Python的函式庫中最常見的功能之一，也就是搜尋功能。讓我們在這裡簡單複習一下，正規表達式可以運用單一個字串，藉以匹配所有符合某個句法規則的字串；而這些字串藉由各種不同函式，可以幫助我們針對文本進行驗證、萃取，以及今天所要介紹的替換功能。正規表達式的運用，加上Python各種不同函式的靈活搭配，是踏入自然語言處理的第一張門票，是一切資料處理的根基，因此掌握正規表達式的運用其實比你我想的還要再更重要一點點。</p>\n<h2 id=\"啊所以到底什麼才是替換功能？\"><a href=\"#啊所以到底什麼才是替換功能？\" class=\"headerlink\" title=\"啊所以到底什麼才是替換功能？\"></a>啊所以到底什麼才是替換功能？</h2><p>讓我們把話題拉回正規表達式的替換功能。大家看到這裡可能還不清楚，所謂正規表達式的「替換」功能，實際上到底指的是什麼事情。就讓我們直接來看一個例子：</p>\n<p>最近有所上同學問我説，她在搭建聊天機器人的時候，發現若是在iPhone按下系統推薦的字詞，會自動產生出一個空白字元<code>&quot; &quot;</code>，而這會導致機器人在判讀字串時產生錯誤。因為同學做的是查詢英漢字典機器人，機器人需要完整正確的字串，才有辦法依照字串找出正確的中文翻譯，但這時卻多了一個空白字元，該怎麼處理才好呢？比如說今天輸入的是<code>ocean</code>，但程式端所接收到的字串卻是<code>ocean </code>。同學很苦惱。</p>\n<p>其實對Python稍微有一點點研究的同學，第一個想到的函式大概是 <code>.replace()</code> ，程式如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">message = <span class=\"string\">&quot;ocean &quot;</span></span><br><span class=\"line\">inputSTR = message.replace(<span class=\"string\">&quot; &quot;</span>, <span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(inputSTR)</span><br><span class=\"line\"><span class=\"comment\"># output: ocean</span></span><br></pre></td></tr></table></figure>\n\n<p>但以上這段程式碼好像也不能有效解決這位同學的問題，因為仔細想想，假如說我們今天想要知道 <em>One Piece</em> 是什麼意思？若套用以上的程式碼，反而會變成 <em>OnePiece</em>，像這種中間有空格的名詞，在計算語言學中，我們稱為<strong>proper noun（專有名詞）</strong>，另外，儘管其實是有些差別，但有些人也會一併稱其為 <strong>entity（實體）</strong>。在這時，我們發現<code>.replace()</code>已經不管用了，這時該怎麼辦？這時我們就必須要想辦法「將字串尾的空白字元取代掉」。</p>\n<p>在軟體工程中有個很重要的觀念，叫做Divide and Conquer，也就是將一個看似難以解決的問題分成很多個小問題，並且逐一擊破，最後即便是大問題也可以解決，大至軟體工程，小至正規表達式，都是一樣的道理。讓我們重新回來看這個問題，「將字串尾的空白字元取代掉」，其中第一個問題則是要找出「字串尾」。</p>\n<p>在正規表達式中，若要表達指定字串的第一個字元，會用<code>^</code>來表示，也就是說，你要告訴電腦你想要的字串格式，其中的第一個字是什麼的時候，就可以透過<code>^</code>來表達；另外，同樣的，若要表達指定字串的最後一個字元，在正規表達式中則會以<code>$</code> 來表示。再來會碰到的第二個問題則是「空白字元」。昨天，我們學到了數字的表達方式，也就是透過<code>\\d</code>來代表數字。而空白字元在正規表示法中，則是以<code>\\s</code>。這麼一來，我們就可以匹配所有字串尾有空白字元的字了。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">\\s$</span><br></pre></td></tr></table></figure>\n\n<p>欸？沒錯，就是這麼簡單。那麼接著就可以將這串正規表達式加入Python當中進行替換了。在正規表達式的函式庫中，將字串進行替換的函式為<code>re.sub()</code>，其中第一個參數放正規表達式，第二個參數則是放你想替換成的字符，在我們的例子裡就是「無」，第三個參數則是放要進行替換的字串。這麼一來，程式就可以寫成：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> re <span class=\"comment\">#記得要加入正規表達式函式庫</span></span><br><span class=\"line\">example_1 = <span class=\"string\">&quot;Ocean&quot;</span></span><br><span class=\"line\">example_1_with_space = <span class=\"string\">&quot;Ocean &quot;</span></span><br><span class=\"line\">example_2 = <span class=\"string\">&quot;One Piece&quot;</span></span><br><span class=\"line\">example_2_with_space = <span class=\"string\">&quot;One Piece &quot;</span></span><br><span class=\"line\"><span class=\"comment\"># 這邊我們都印出字串的最後一個字看看空白字元是否都已成功替換</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(re.sub(<span class=\"string\">r&quot;\\s$&quot;</span>, <span class=\"string\">&quot;&quot;</span>, example_1)[-<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(re.sub(<span class=\"string\">r&quot;\\s$&quot;</span>, <span class=\"string\">&quot;&quot;</span>, example_1_with_space)[-<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(re.sub(<span class=\"string\">r&quot;\\s$&quot;</span>, <span class=\"string\">&quot;&quot;</span>, example_2)[-<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(re.sub(<span class=\"string\">r&quot;\\s$&quot;</span>, <span class=\"string\">&quot;&quot;</span>, example_2_with_space)[-<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(re.sub(<span class=\"string\">r&quot;\\s$&quot;</span>, <span class=\"string\">&quot;&quot;</span>, example_2_with_space))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">n</span><br><span class=\"line\">n</span><br><span class=\"line\">e</span><br><span class=\"line\">e</span><br><span class=\"line\">One Piece</span><br></pre></td></tr></table></figure>\n<p>從結果我們可以發現幾件事：<br>一、最後一個字都不是空白字元，這代表空白字元已經成功地被我們的正規表達式進行替換了！<br>二、就算最後一個字不是空白字元，正規表達式沒有匹配，函式仍會回傳原本的字串！<br>三、<em>One Piece</em>中間的空白字元仍留著，這代表所有proper noun最後一個字都不會是空白字元！</p>\n<h2 id=\"進階的替換功能\"><a href=\"#進階的替換功能\" class=\"headerlink\" title=\"進階的替換功能\"></a>進階的替換功能</h2><p>前面我們提到了如何運用正規表達式將字串整理成我們理想中的形式，現在要來介紹一些正規表達式中比較進階的替換用法。先讓我們進入實例：</p>\n<p>美國佬很喜歡跟全世界不一樣，無論是重量單位、長度單位，以及日期都跟其他國家的寫法不一樣，那今天我們的任務是要將美國日期寫法改成大家都習慣的形式。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 美國日期寫法為 mm/dd/yyyy</span></span><br><span class=\"line\"><span class=\"comment\"># 其他國家的日期格式則為 dd/mm/yyyy</span></span><br></pre></td></tr></table></figure>\n<p>一樣，我們先來想想該如何以正規表達式將美國日期格式進行匹配。這邊可以回去看第三篇文章，其中提到的逃脫字元<code>\\</code>，由於<code>/</code>也是屬於正規表達式的一員，所以這時就要透過逃脫字元<code>\\</code>告訴電腦在這裡的<code>/</code>不是正規表達式，而是真正的 &#x2F;。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">\\d&#123;2&#125;\\/\\d&#123;2&#125;\\/\\d&#123;4&#125;</span><br></pre></td></tr></table></figure>\n<p>這時問題就來了，該如何透過替換功能，將一串日期字串轉換成其他國家較常用的日期格式呢？不知你是否還記得，昨天我們提到可以將括號<code>()</code>作為群組（group），並透過<code>re.search()</code>將想要的群組輸出，這次我們就要透過先前學過的群組概念來進行替換。首先，先將想要替換的字串加入群組：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">(\\d&#123;2&#125;)\\/(\\d&#123;2&#125;)\\/(\\d&#123;4&#125;)</span><br></pre></td></tr></table></figure>\n<p>圈好群組後，就可以透過<code>re.match()</code>來進行替換：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">usa_date = <span class=\"string\">&quot;09/19/2022&quot;</span></span><br><span class=\"line\">other_date = re.sub(<span class=\"string\">&quot;(\\d&#123;2&#125;)\\/(\\d&#123;2&#125;)\\/(\\d&#123;4&#125;)&quot;</span>, <span class=\"string\">&quot;\\\\2/\\\\1/\\\\3&quot;</span>, usa_date)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(other_date)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">19/09/22</span><br></pre></td></tr></table></figure>\n<p>其中<code>\\\\</code>後面的數字就是群組的數字，就是這麼簡單！</p>\n<p>好啦！那我們今天的旅程就到這邊先結束了，明天就要邁入正規表達式的最終章，也就是萃取！</p>\n","categories":["自然語言處理"]},{"title":"【NLP】Day 3: 自然語言處理的內力，一切的基礎！正規表達式 (1) 搜尋","url":"/2022/09/14/%E3%80%90NLP%E3%80%91Day-3-%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E7%9A%84%E5%85%A7%E5%8A%9B%EF%BC%8C%E4%B8%80%E5%88%87%E7%9A%84%E5%9F%BA%E7%A4%8E%EF%BC%81%E6%AD%A3%E8%A6%8F%E8%A1%A8%E9%81%94%E5%BC%8F-1-%E6%90%9C%E5%B0%8B/","content":"<blockquote>\n<p><em>自來修習內功，不論是為了強身治病，還是為了作為上乘武功的根基，必當水火互濟，陰陽相配，練了「足少陰腎經」之後，便當練「足少陽膽經」，少陰少陽融會調合，體力便逐步增強。</em><br><strong>金庸《俠客行》</strong></p>\n</blockquote>\n<p>在進行自然語言處理的同時，常常會需要將資料變得乾淨，或是也有些特定的資料格式要從大量語料中抽取出來，比如說像是地址、電話等等，才有辦法取出我們想要的理想資料，而在這過程中常常會用到的就是正規表達式（regular expression），正規表達式也會被簡稱為re。re可以針對符合的字串大致上分成三種功能：搜尋、替換，以及抽取。</p>\n<h2 id=\"正規表達式\"><a href=\"#正規表達式\" class=\"headerlink\" title=\"正規表達式\"></a>正規表達式</h2><p>讓我們先以手機號碼舉例，假如我們要從大量語料中抽取手機號碼，例如：0912-345-678，該怎麼做呢？首先，編寫regular expression一個很重要的觀念是要一個字一個字看，並輸入對應的正規表達式。回到手機號碼，我們觀察手機號碼0912-345-678，可以發現全部都是用數字組成，數字的正規表達式為<code>\\d</code>，我們首先可以將手機號碼以這種方法表示：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">\\d\\d\\d\\d-\\d\\d\\d-\\d\\d\\d</span><br></pre></td></tr></table></figure>\n<p>但是這樣並不是一個足夠有效率的寫法，若在如<code>\\d</code>後面加上大括號，則可以指定其出現次數。我們可以發現以橫槓分隔，第一組是由四個數字，後面則是由兩組三個數字所組成。逗號的分隔方式代表其次數，<code>&#123;1,&#125;</code>為一次以上，<code>&#123;,2&#125;</code>為兩次以下，<code>&#123;1,3&#125;</code>則為一到三次。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">\\d&#123;4&#125;-\\d&#123;3&#125;-\\d&#123;3&#125;</span><br></pre></td></tr></table></figure>\n<p>但若要取出符合這個樣式的第一組數字的話，則可以透過小括號，將這些數字分組，並在撰寫程式時指定回傳特定組的字串，整串符合的字串為group 0，第一組，四個數字的，也就是group 1，以此類推。寫法如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">(\\d&#123;4&#125;)-(\\d&#123;3&#125;)-(\\d&#123;3&#125;)</span><br></pre></td></tr></table></figure>\n<p>那假如今天要找的是電話號碼，像是(01)234-5678時，裡面有小括號，該怎麼辦才能讓程式知道這裡得括號並不是前述分組的意思？這時我們可以利用逃脫符號<code>\\</code>告訴編譯器，這裡的括號是指真的括號，任何會跟正規表達式衝突的符號，都可以透過這種方法進行編寫，寫法如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">\\(\\d&#123;2&#125;\\)\\d&#123;3&#125;-\\d&#123;4&#125;</span><br></pre></td></tr></table></figure>\n<p>這邊只有介紹到數字的寫法，欲得知其他寫法，如英文字母等等的，可以參考以下這個 <a href=\"https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf\">cheatsheet</a>，另外若要確認自己寫的正規表達式是否符合的話，也可以運用這個網站 <a href=\"https://regex101.com/\">regex101</a> 來進行比對。</p>\n<h2 id=\"搜尋\"><a href=\"#搜尋\" class=\"headerlink\" title=\"搜尋\"></a>搜尋</h2><p>前面有提到說，regular expression有進行搜尋的功能，我們可以將正規表達式的字串放入函式中，確認資料中是否有我們想要的字段。這時，可以運用<code>re</code>函式庫中的<code>search()</code>來達到我們的目的。第一個參數放正規表達式，第二個參數放欲檢驗的字串。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> re <span class=\"comment\"># 記得加入re函式庫</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(re.search(<span class=\"string\">&quot;(\\d&#123;4&#125;)-(\\d&#123;3&#125;)-(\\d&#123;3&#125;)&quot;</span>, <span class=\"string\">&quot;0987-654-321&quot;</span>))</span><br><span class=\"line\"><span class=\"comment\"># output: &lt;re.Match object; span=(0, 12), match=&#x27;0987-654-321&#x27;&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 輸出的意思是說，若有符合的字串，就會回傳正規表達式的物件</span></span><br><span class=\"line\"><span class=\"comment\"># span則為符合物件的字串在語料中的位置為何，像這邊是整串符合</span></span><br><span class=\"line\"><span class=\"comment\"># 那就是從index = 0 的地方開始到12都是符合的字串。 </span></span><br><span class=\"line\"><span class=\"comment\"># match則是符合的字串。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(re.search(<span class=\"string\">&quot;(\\d&#123;4&#125;)-(\\d&#123;3&#125;)-(\\d&#123;3&#125;)&quot;</span>, <span class=\"string\">&quot;0987654321&quot;</span>))</span><br><span class=\"line\"><span class=\"comment\"># output: None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 若沒有符合字串，就會回傳None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 小應用是，也可以把這個當作條件判斷式的條件之一，例如：</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> re.search(<span class=\"string\">&quot;(\\d&#123;4&#125;)-(\\d&#123;3&#125;)-(\\d&#123;3&#125;)&quot;</span>, <span class=\"string\">&quot;0987-654-321&quot;</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"number\">123</span>)</span><br><span class=\"line\"><span class=\"comment\"># output: 1234</span></span><br><span class=\"line\"><span class=\"comment\"># 這邊因為re.search()有符合字串，判斷為True，執行指定動作</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> re.search(<span class=\"string\">&quot;(\\d&#123;4&#125;)-(\\d&#123;3&#125;)-(\\d&#123;3&#125;)&quot;</span>, <span class=\"string\">&quot;0987654321&quot;</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"number\">456</span>)</span><br><span class=\"line\"><span class=\"comment\"># 判斷為False，不進行任何動作</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"記得前面說的分組概念嗎？在這裡就可以派上用場了。\"><a href=\"#記得前面說的分組概念嗎？在這裡就可以派上用場了。\" class=\"headerlink\" title=\"記得前面說的分組概念嗎？在這裡就可以派上用場了。\"></a>記得前面說的分組概念嗎？在這裡就可以派上用場了。</h4><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(example_re_obj.group(<span class=\"number\">0</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 你也可以指派一個變數給他，如：</span></span><br><span class=\"line\">example_re_obj = re.search(<span class=\"string\">&quot;(\\d&#123;4&#125;)-(\\d&#123;3&#125;)-(\\d&#123;3&#125;)&quot;</span>, <span class=\"string\">&quot;0987-654-321&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 記得前面說的分組概念嗎？在這裡就可以派上用場了。</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(example_re_obj.group(<span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"comment\"># output: 0987-654-321</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(example_re_obj.group(<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"comment\"># output: 0987</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(example_re_obj.group(<span class=\"number\">2</span>))</span><br><span class=\"line\"><span class=\"comment\"># output: 654</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(example_re_obj.group(<span class=\"number\">3</span>))</span><br><span class=\"line\"><span class=\"comment\"># output: 321</span></span><br></pre></td></tr></table></figure>\n\n<p>好的，大概就是這樣，完成了這步驟，接下來就可以往「替換」以及「抽取」的目的地前進了！我們明天見！</p>\n","categories":["自然語言處理"]},{"title":"【NLP】Day 5: 大風吹，吹什麼，吹...正規表達式：萃取（3）","url":"/2022/09/18/%E3%80%90NLP%E3%80%91Day-5-%E5%A4%A7%E9%A2%A8%E5%90%B9%EF%BC%8C%E5%90%B9%E4%BB%80%E9%BA%BC%EF%BC%8C%E5%90%B9-%E6%AD%A3%E8%A6%8F%E8%A1%A8%E9%81%94%E5%BC%8F%EF%BC%9A%E8%90%83%E5%8F%96%EF%BC%883%EF%BC%89/","content":"<blockquote>\n<p>我們確實曾經擁有一切，對吧？我是說，如果你仔細回想的話，就會發現確實是這麼一回事。<br><strong>《華爾街之狼》</strong></p>\n</blockquote>\n<p>轉眼間，我們也來到了正規表達式的最終章，也就是萃取啦！有些反應比較快的朋友可能會想說，萃取不就跟第一週的<code>re.search()</code>一樣嗎？如果一樣要輸出匹配的字串的話，那就直接用<code>re.search()</code>不就好了？幹嘛還要再多學這個？</p>\n<p>這是一個很好的問題，我們話不多說，就直接來看實例。這次我從維基百科的<a href=\"https://zh.m.wikipedia.org/wiki/HUNTER%C3%97HUNTER\">獵人 Hunter x Hunter</a>條目中擷取了一小段文字，然後這次的任務是需要將這段文字中，每一篇獵人的篇章所在的漫畫話數全部都取出來，<del>這樣才方便之後連載再開的時候直接找到需要複習的篇章。</del></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">蟻王篇 No.186－318（單行本18卷 - 30卷） # 我們要抓的地方</span><br><span class=\"line\">小傑與奇犽使用貪婪之島破關獎品打算去找金。但是在那裏等著的並不是金，而是凱特。凱特他們在未確認生物的探索和調查中，發現巨型嵌合蟻的殘骸，一行人前往NGL。小傑和奇犽瞭解嵌合蟻的強大，在凱特的犧牲下逃出NGL。在討伐隊還尚未準備完全之際，嵌合蟻王誕生了。王率領直屬護衛隊和一部份士兵，在背後統治東果陀共和國，打算進行『選別』（挑選）。為了防止更多的人死亡以及為凱特報仇，小傑一行人再次向嵌合蟻挑戰。</span><br><span class=\"line\">十二支會長篇 No.319－339（單行本30卷 - 32卷）# 我們要抓的地方</span><br><span class=\"line\">十二支登場，尼特羅會長於嵌合蟻之役死後，從獵人協會會長退位，開始選拔新的獵人協會會長。全世界的獵人為了新的獵人會長選舉一事紛紛開始活動；另一方面，奇犽為了拯救傷重的小傑而再度回到揍敵客家族。</span><br><span class=\"line\">暗黑大陸篇 No.340－ （單行本32卷 - ）# 我們要抓的地方</span><br><span class=\"line\">有天自稱尼特羅之子的比洋德·尼特羅在卡丁帝國支持下向全世界發表招募同伴前往暗黑大陸的影片，在十二支們接獲V5所發出的「追捕比洋德」命令的同一時間，比洋德主動前來被逮捕並要求交換條件──十二支需陪同他一起至暗黑大陸。同時雷歐力和酷拉皮卡加入十二支並接替十二支的子、亥之位置。</span><br><span class=\"line\">金以No.2身分加入比洋德雇用前往暗黑大陸的專家團隊，後來當卡丁帝國的船發出後，和其他包括12支的獵人也登上船，酷拉皮卡成為第八王妃奧伊特和第十四王子的保鑣，其目的為第四王子手上收藏的族人眼睛。船的目的地為鄰近暗黑大陸邊界的假想「新大陸」，航行過程中，王位爭奪戰正式展開。</span><br></pre></td></tr></table></figure>\n\n<p>先來看看符合字串的正規表達式怎麼寫，在本篇不會仔細說明正規表達式是如何運作的，但如果你需要詳細介紹的話，可以去看我第三天的<a href=\"https://ithelp.ithome.com.tw/articles/10293287\">文章</a>。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">No\\.\\d&#123;3&#125;－\\d&#123;0,3&#125;</span><br></pre></td></tr></table></figure>\n\n<p>稍微講解一下這段正規表達式：在<code>No</code>後面有一個小點點，要記得在前面加上逃脫字元<code>\\</code>，不然<code>.</code>是正規表達式中的黑暗大法師，他會代表所有的字元，也會把所有的字元通通都吸進去，要小心使用。後面最後之所以是<code>\\d&#123;0,3&#125;</code>是因為注意到暗黑大陸篇由於尚在連載中，所以還沒有結尾話數（我先哭）</p>\n<p>先來看看，如果這時候用前天我們所學的<code>re.search()</code>會發生什麼事。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> re <span class=\"comment\"># 要記得引入函式庫</span></span><br><span class=\"line\">test_string = <span class=\"string\">&quot;蟻王篇 No.186－318（單行本18卷 - 30卷）小傑與奇犽使用貪婪之島破關獎品打算去找金。但是在那裏等著的並不是金，而是凱特。凱特他們在未確認生物的探索和調查中，發現巨型嵌合蟻的殘骸，一行人前往NGL。小傑和奇犽瞭解嵌合蟻的強大，在凱特的犧牲下逃出NGL。在討伐隊還尚未準備完全之際，嵌合蟻王誕生了。王率領直屬護衛隊和一部份士兵，在背後統治東果陀共和國，打算進行『選別』（挑選）。為了防止更多的人死亡以及為凱特報仇，小傑一行人再次向嵌合蟻挑戰。十二支會長篇 No.319－339（單行本30卷 - 32卷）十二支登場，尼特羅會長於嵌合蟻之役死後，從獵人協會會長退位，開始選拔新的獵人協會會長。全世界的獵人為了新的獵人會長選舉一事紛紛開始活動；另一方面，奇犽為了拯救傷重的小傑而再度回到揍敵客家族。暗黑大陸篇 No.340－ （單行本32卷 - ）有天自稱尼特羅之子的比洋德·尼特羅在卡丁帝國支持下向全世界發表招募同伴前往暗黑大陸的影片，在十二支們接獲V5所發出的「追捕比洋德」命令的同一時間，比洋德主動前來被逮捕並要求交換條件──十二支需陪同他一起至暗黑大陸。同時雷歐力和酷拉皮卡加入十二支並接替十二支的子、亥之位置。金以No.2身分加入比洋德雇用前往暗黑大陸的專家團隊，後來當卡丁帝國的船發出後，和其他包括12支的獵人也登上船，酷拉皮卡成為第八王妃奧伊特和第十四王子的保鑣，其目的為第四王子手上收藏的族人眼睛。船的目的地為鄰近暗黑大陸邊界的假想「新大陸」，航行過程中，王位爭奪戰正式展開。&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(re.search(<span class=\"string\">&quot;No\\.\\d&#123;3&#125;－\\d&#123;0,3&#125;&quot;</span>, test_string))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;re.Match object; span=(4, 14), match=&#x27;No.186－318&#x27;&gt;</span><br></pre></td></tr></table></figure>\n\n<p>我們會發現，如果只有使用<code>re.search()</code>，函式只會回傳符合的第一個字串，這時該怎麼辦？因為我們想要的是<strong>所有符合的字串</strong>，只回傳符合的第一個字串，就不符合我們的需求了。這時候就要好好地利用新的函式<code>re.findall()</code>了。<code>re.findall()</code>會找出所有符合的字串，並以串列的形式回傳。話不多說，就讓我們來試試看吧！</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> re <span class=\"comment\"># 要記得引入函式庫</span></span><br><span class=\"line\">test_string = <span class=\"string\">&quot;蟻王篇 No.186－318（單行本18卷 - 30卷）小傑與奇犽使用貪婪之島破關獎品打算去找金。但是在那裏等著的並不是金，而是凱特。凱特他們在未確認生物的探索和調查中，發現巨型嵌合蟻的殘骸，一行人前往NGL。小傑和奇犽瞭解嵌合蟻的強大，在凱特的犧牲下逃出NGL。在討伐隊還尚未準備完全之際，嵌合蟻王誕生了。王率領直屬護衛隊和一部份士兵，在背後統治東果陀共和國，打算進行『選別』（挑選）。為了防止更多的人死亡以及為凱特報仇，小傑一行人再次向嵌合蟻挑戰。十二支會長篇 No.319－339（單行本30卷 - 32卷）十二支登場，尼特羅會長於嵌合蟻之役死後，從獵人協會會長退位，開始選拔新的獵人協會會長。全世界的獵人為了新的獵人會長選舉一事紛紛開始活動；另一方面，奇犽為了拯救傷重的小傑而再度回到揍敵客家族。暗黑大陸篇 No.340－ （單行本32卷 - ）有天自稱尼特羅之子的比洋德·尼特羅在卡丁帝國支持下向全世界發表招募同伴前往暗黑大陸的影片，在十二支們接獲V5所發出的「追捕比洋德」命令的同一時間，比洋德主動前來被逮捕並要求交換條件──十二支需陪同他一起至暗黑大陸。同時雷歐力和酷拉皮卡加入十二支並接替十二支的子、亥之位置。金以No.2身分加入比洋德雇用前往暗黑大陸的專家團隊，後來當卡丁帝國的船發出後，和其他包括12支的獵人也登上船，酷拉皮卡成為第八王妃奧伊特和第十四王子的保鑣，其目的為第四王子手上收藏的族人眼睛。船的目的地為鄰近暗黑大陸邊界的假想「新大陸」，航行過程中，王位爭奪戰正式展開。&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(re.findall(<span class=\"string\">&quot;No\\.\\d&#123;3&#125;－\\d&#123;0,3&#125;&quot;</span>, test_string))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">[&#x27;No.186－318&#x27;, &#x27;No.319－339&#x27;, &#x27;No.340－&#x27;]</span><br></pre></td></tr></table></figure>\n<p>我們可以發現，如果使用<code>re.findall()</code>就能把所有的話數範圍找出來了！</p>\n<p>好！那有關正規表達式的介紹就到這篇結束啦！明天開始就要進入斷詞方法了，大家抓緊好要出發了嗎？我們明天見囉！</p>\n","categories":["自然語言處理"]},{"title":"【NLP】Day 6: 斷開一切的牽連！一同探訪「斷詞」與他們的產地","url":"/2022/09/19/%E3%80%90NLP%E3%80%91Day-6-%E6%96%B7%E9%96%8B%E4%B8%80%E5%88%87%E7%9A%84%E7%89%BD%E9%80%A3%EF%BC%81%E4%B8%80%E5%90%8C%E6%8E%A2%E8%A8%AA%E3%80%8C%E6%96%B7%E8%A9%9E%E3%80%8D%E8%88%87%E4%BB%96%E5%80%91%E7%9A%84%E7%94%A2%E5%9C%B0/","content":"<blockquote>\n<p>如果沒有了這套裝備，你就什麼都不是的話，那你就更不該擁有它。<br>東尼·史塔克《蜘蛛人：返家日》</p>\n</blockquote>\n<p>在處理自然語言的時候，斷詞往往是首要的工作。其實這也很好理解，為什麼？我們可以從自然語言處理的目的開始來思考這個問題，也就是說，進行自然語言處理的目的是什麼？是要知道一篇正面或是負面評價的電影評論會用哪些詞彙嗎？還是要知道在一句話中，哪個是名詞？動詞？形容詞？或者是要知道這篇文章與其他篇文章比起來，有哪些關鍵詞？還是你今天要做一個文字雲，來搞清楚現在網路上最熱門的話題是什麼？</p>\n<p>不知道你是否有發現，以上這些自然語言處理的目的，都有一個共通的特性：<strong>詞</strong>。也就是說，大部分自然語言處理的應用上，都需要將一句完整的句子，斷成一個一個的詞，或是斷成那個語言的最小單位，端看你的目的性是什麼。處理的語言更是另一個需要考量的點。以英文為例吧！在英文中，由於詞與詞之間都已經有空格了，所以相較於中文而言，英文考量的點比中文還要再稍微少一點點；至於中文，因為每個字都是黏在一起的，有時候單一個字就可以代表一個完整的意義，但有時候卻又需要兩個字才有意義。所以中文的斷詞在技術上還有許多的難處。<del>想到我們所上的語意學教授有云：「中文就是一個很混亂的語言」。</del></p>\n<p>話說回來，通常在計算語言學中，我們會稱這一個一個的詞為<strong>token</strong>。接著再藉由著一個一個的token來完成目的。</p>\n<p>你可能已經想到了，既然這一個一個的token會被作為之後處理自然語言的主要材料，那斷詞的正確性，這時就顯得特別重要。另外，斷詞的Level還要語境也同樣不能被忽視。就拿當初來語言所面試時，現任老闆當時問我的問題為例。</p>\n<blockquote>\n<p><strong>今天我在國立政治大學語言所面試</strong></p>\n</blockquote>\n<p>想想看，如果是你，該如何斷詞才比較正確？今天如果斷詞工具將這句話斷成：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">今天/我/在/國立/政治/大學/語言/所/面試</span><br></pre></td></tr></table></figure>\n<p>這是正確的斷詞結果嗎？我們等等再回來回答這個問題。先讓我們稍微了解一下有哪些斷詞工具吧！</p>\n<h2 id=\"斷詞工具\"><a href=\"#斷詞工具\" class=\"headerlink\" title=\"斷詞工具\"></a>斷詞工具</h2><p>目前有三種大家比較常用的自然語言處理工具，分別是Jieba、CKIP，還有卓騰語言科技的Articut，以下來跟各位簡單介紹一下各個常用套件以及基本用法。</p>\n<ol>\n<li><p><strong>Jieba</strong> </br><br>Jieba（結巴）是海峽左岸的朋友所開發的斷詞套件，是現今華語界中最廣為人知的常見斷詞套件，不只是開源在GitHub上，更有在積極維護。由於是左岸朋友發明的斷詞套件，所以可想而知背後的訓練資料幾乎都是簡體中文的語料，像是人民日報等等的媒體文本，因此簡體中文的斷詞正確率較為出色。不過Jieba也有釋出專給繁體中文的斷詞工具，只是相較於簡體中文的表現下就真的稍微遜色了點，一些台灣用語或是在華語世界中只有台灣有的一些相關詞彙，Jieba的繁體中文斷詞相較簡體中文之下的表現，就相對比較沒有那麼地好。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install jieba</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> jieba</span><br><span class=\"line\">inputSTR = <span class=\"string\">&quot;我要成為海賊王&quot;</span></span><br><span class=\"line\">segment_list = jieba.cut(inputSTR)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot; &quot;</span>.join(segment_list)) <span class=\"comment\"># 這邊將斷詞過後的結巴物件用join拆開來，並用空白字元分隔</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#x27;我要 成為 海賊 王&#x27;</span><br></pre></td></tr></table></figure></li>\n<li><p><strong>CKIP</strong> （Chinese Knowledge And Information Processing）</br><br>前面有提到說，左岸朋友所開發的開源斷詞工具在繁體中文的表現上叫不如簡體中文，於是我國的中央研究院資訊所在1986年也開發了繁體中文專屬的斷詞工具：CKIP。既然是我國專為繁體中文開發，斷詞系統想必也是享負盛名，準確率也達到了97.49%（<a href=\"https://www.ithome.com.tw/news/132838\">來源</a>），但是因為模型偏大，所以運算時間相對來說也比較長。馬上一起來看看要怎麼用吧！</p>\n <figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install ckiptagger</span><br><span class=\"line\">pip install tensorflow</span><br><span class=\"line\">pip install gdown</span><br></pre></td></tr></table></figure>\n\n <figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> ckiptagger <span class=\"keyword\">import</span> data_utils</span><br><span class=\"line\">data_utils.download_data_gdown(<span class=\"string\">&quot;./&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">from</span> ckiptagger <span class=\"keyword\">import</span> WS</span><br><span class=\"line\"></span><br><span class=\"line\">inputSTR = <span class=\"string\">&#x27;我要成為海賊王&#x27;</span></span><br><span class=\"line\">ws = WS(<span class=\"string\">&quot;./data&quot;</span>)</span><br><span class=\"line\">segment_list = ws([inputSTR])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(segment_list)</span><br></pre></td></tr></table></figure>\n <figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">[[&#x27;我&#x27;, &#x27;要&#x27;, &#x27;成為&#x27;, &#x27;海賊王&#x27;]]</span><br></pre></td></tr></table></figure></li>\n<li><p><strong>Articut</strong></br><br>這是我前實習老闆的公司所開發的斷詞工具。個人認為，這是個以任何觀點來說都非常厲害的繁體中文專用斷詞工具。這是因為前面所提到的兩種工具，前者是由簡體中文所訓練而成，在繁體中文斷詞正確率上就有待加強，另外也是藉由機器學習模型打造的，後者也是透過統計方法或是機器學習模型（CRF、HMM等）佐以大量的訓練資料所打造而成的，而Articut正好可以克服前面所提到的兩個缺點，這全部都是因為同時有著語言學以及資訊工程背景的卓騰所開發的Articut，是以過去語言學家所研究的中文句法學作為理論基礎下去打造的斷詞工具，因此只要是中文的斷詞，皆有相當優秀的正確率，更因為這並非以任何統計模型訓練而成，所以運算速度也相當快。更厲害的是，也可以透過句法樹斷詞後所提供的資訊，推斷不同詞的詞性。</p>\n<p> 不過由於是透過句法樹所推斷的資訊進行的斷詞，所以有些含有專有名詞的斷詞結果需要針對不同語境透過手動的調整，以加入自定義字典的方式來進行優化，雖然麻煩了一點點，但其實這點倒也提供了處理自然語言上不少的彈性空間。</p>\n <figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install AritcutAPI</span><br></pre></td></tr></table></figure>\n <figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> ArticutAPI <span class=\"keyword\">import</span> Articut</span><br><span class=\"line\">articut = Articut()</span><br><span class=\"line\">inputSTR = <span class=\"string\">&quot;我要成為海賊王&quot;</span></span><br><span class=\"line\">articut.parse(inputSTR = inputSTR, level=<span class=\"string\">&quot;lv2&quot;</span>)</span><br></pre></td></tr></table></figure>\n <figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;&#x27;exec_time&#x27;: 0.10455012321472168,</span><br><span class=\"line\">&#x27;result_pos&#x27;: [&#x27;&lt;ENTITY_pronoun&gt;我&lt;/ENTITY_pronoun&gt;&lt;ACTION_verb&gt;要&lt;/ACTION_verb&gt;&lt;ACTION_verb&gt;成為&lt;/ACTION_verb&gt;&lt;ENTITY_nouny&gt;海&lt;/ENTITY_nouny&gt;&lt;ENTITY_nouny&gt;賊王&lt;/ENTITY_nouny&gt;&#x27;],</span><br><span class=\"line\">&#x27;result_segmentation&#x27;: &#x27;我/要/成為/海/賊王&#x27;,</span><br><span class=\"line\">&#x27;result_obj&#x27;: [[&#123;&#x27;text&#x27;: &#x27;我&#x27;, &#x27;pos&#x27;: &#x27;ENTITY_pronoun&#x27;&#125;,</span><br><span class=\"line\">&#123;&#x27;text&#x27;: &#x27;要&#x27;, &#x27;pos&#x27;: &#x27;ACTION_verb&#x27;&#125;,</span><br><span class=\"line\">&#123;&#x27;text&#x27;: &#x27;成為&#x27;, &#x27;pos&#x27;: &#x27;ACTION_verb&#x27;&#125;,</span><br><span class=\"line\">&#123;&#x27;text&#x27;: &#x27;海&#x27;, &#x27;pos&#x27;: &#x27;ENTITY_nouny&#x27;&#125;,</span><br><span class=\"line\">&#123;&#x27;text&#x27;: &#x27;賊王&#x27;, &#x27;pos&#x27;: &#x27;ENTITY_nouny&#x27;&#125;]],</span><br><span class=\"line\">&#x27;level&#x27;: &#x27;lv2&#x27;,</span><br><span class=\"line\">&#x27;version&#x27;: &#x27;v256&#x27;,</span><br><span class=\"line\">&#x27;status&#x27;: True,</span><br><span class=\"line\">&#x27;msg&#x27;: &#x27;Success!&#x27;,</span><br><span class=\"line\">&#x27;word_count_balance&#x27;: 1986,</span><br><span class=\"line\">&#x27;product&#x27;: &#x27;https://api.droidtown.co/product/&#x27;,</span><br><span class=\"line\">&#x27;document&#x27;: &#x27;https://api.droidtown.co/document/&#x27;&#125;</span><br></pre></td></tr></table></figure></li>\n</ol>\n<p>說完了，讓我們回到前面的問題，不知道你想到答案了沒有？</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">今天/我/在/國立/政治/大學/語言/所/面試</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>答案是：沒有正確答案。</p>\n</blockquote>\n<p>其實從前面就有提示了，自然語言處理的方式很吃你所要達成的目的是什麼。假如說今天對你的任務來說，台灣不同大學之間的區別很重要的話，那麼就應該在自定義字典中加入「政治大學」，但如果相反而言，大學之間的區別對你的下游任務來說並不是特別重要，只是想要知道「大學」這個高等教育在文本中所扮演的角色的話，那其實將政治大學斷開也沒有關係。</p>\n<h2 id=\"斷詞真的那麼重要嗎？\"><a href=\"#斷詞真的那麼重要嗎？\" class=\"headerlink\" title=\"斷詞真的那麼重要嗎？\"></a>斷詞真的那麼重要嗎？</h2><p>前面有說到，斷詞是一個在自然語言處理中常用的技巧，但是你要說一看到文本就必須得立刻給他斷詞嗎？倒也是粗暴言論大可不必。我們強調了目的性的重要，那麼在做自然語言處理時，更重要的應該是對於<strong>要處理的語言有充分的了解</strong>，並<strong>審慎思考最適當的斷詞方式</strong>，而不是不考慮就通通給他直接斷下去，卻連文本大概長什麼樣子都不曉得。斷詞不是萬靈丹，更不應該是唯一的自然語言處理的第一步驟。也有些人主張，除了斷詞之外，也更應該強調詞語在文本中的位置，因為位置確實也會導致意義的不同，更會引領至不一樣的結果。</p>\n<p>就像前言所提到的沒有裝備的蜘蛛人一樣，如果把斷詞當作自然語言處理中至高無上的存在，其他什麼都不管的話，那麼斷詞本身就會變得一點意義都沒有。反倒是有目的性、有理由地決定要斷詞，並且對文本有一定的熟悉程度後，靈活運用並適配最妥當的斷詞方法，才是一個真正的自然語言處理專家要克服的第一步。</p>\n"},{"title":"【NLP】Day 8: 你拿定主意的話...葛萊芬多！BOW＆TF-IDF","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-8-%E4%BD%A0%E6%8B%BF%E5%AE%9A%E4%B8%BB%E6%84%8F%E7%9A%84%E8%A9%B1-%E8%91%9B%E8%90%8A%E8%8A%AC%E5%A4%9A%EF%BC%81BOW%EF%BC%86TF-IDF/","content":"<blockquote>\n<p>「拿定主意了嗎？你能成大器，你知道，在你一念之間，史萊哲林能幫助你走向輝煌，這毫無疑問——不樂意？那好，既然你已經拿定主意——那就最好去葛來芬多吧！」<br><strong>分類帽《哈利波特：神秘的魔法石》</strong></p>\n</blockquote>\n<p>這幾天下來我們學了正規表達式、斷詞等等，這些都是屬於資料前處理的範疇。是說，剛剛在寫稿前，還被我的隊友罵說，啊我的正規表達式怎麼寫那麼少！欸不是，在這邊跟各位解釋一下，我的規劃就是簡單介紹正規表達式的三個函式，然後正規表達式常用的三個函式就是那三個，如果用到第四個再去查就可以了，<del>我甚至很懷疑有沒有第四個函式</del>，這個系列的文章就是適合我引你進門，至於修行就是要看個人啦！如果還想要更詳細的介紹文章，那我大力推薦我隊友<a href=\"https://ithelp.ithome.com.tw/users/20151687/ironman/5359\">fish_in_bed</a>以及<a href=\"https://ithelp.ithome.com.tw/users/20151689/ironman/5357\">cjon_06991</a>寫的文章，內容真的很豐富，在這邊提供傳送門給各位。</p>\n<p>OK！閒聊結束！</p>\n<p>今天漸漸要進入自然語言目前主流的機器學習方法，預計將會介紹詞袋（Bag-of-Words, BoW）、TF-IDF，應該啦，如果我寫得了這麼多的話。</p>\n<h2 id=\"語言模型（Language-Modeling）\"><a href=\"#語言模型（Language-Modeling）\" class=\"headerlink\" title=\"語言模型（Language Modeling）\"></a>語言模型（Language Modeling）</h2><p>不知道各位在第一次看到所謂「語言模型」時，想到的是什麼？是以語言學搭建的機器學習模型嗎？還是大量資料丟進去演算的人工智慧？（在這裡，我偏好使用自然語言處理）其實自然語言處理<strong>在過去的研究脈絡中，傾向於將各語言的最小單位視作一個一個的token，並透過這些token進行統計上的機率模擬及運算</strong>，因此也有些人也會稱其為<strong>統計語言模型（Statistical Language Modeling）</strong> 。</p>\n<p>像是有一篇計算語言學學生必讀的經典論文 <strong><a href=\"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\">A Neural Probabilistic Language Model</a></strong> 摘要的第一行就寫著：</p>\n<blockquote>\n<p>統計語言模型的目標是透過文字序列了解在語言中字與字之間的聯合機率。</p>\n</blockquote>\n<p>其實就是將文字轉成數字的形式讓電腦理解啦！不過若你仔細一想，倒也很好可以理解為什麼需要從統計的角度去看待自然語言處理，這是因為電腦跟人腦不同，電腦無法直接處理文字資料，但電腦非常擅長處理數值資料的演算，因此自然語言處理領域有一大半都是將文字轉成數值資料的研究。</p>\n<p>而我們接下來要介紹的就是三種經典文本轉數值的常見方法。</p>\n<h2 id=\"詞袋（Bag-of-Words）\"><a href=\"#詞袋（Bag-of-Words）\" class=\"headerlink\" title=\"詞袋（Bag-of-Words）\"></a>詞袋（Bag-of-Words）</h2><p><img src=\"https://miro.medium.com/max/1400/1*ujkZ3JrQ6ubSuEpepHE4Aw.png\"></p>\n<p>前人種樹，後人乘涼。另外一本計算語言學的必讀經典 <a href=\"https://web.stanford.edu/~jurafsky/slp3/\">Speech and Language Processing</a> 其中的一張圖就完美詮釋了詞袋（Bag of Words）的概念。我們前面已經介紹過斷詞，其實詞袋就是將斷詞過後所得到的這些詞類作為一個一個的袋子，接著將這些字一一丟進這些袋子裡面。這就是詞袋的概念，這種計算詞頻的方式也是詞袋的其中一種做法。</p>\n<p>另外一種做法，則是以獨熱編碼（One-Hot Encoding）的形式呈現<del>其實根本沒有人會用中文吧？</del> 也就是說，先找出詞袋後，詞袋中的詞序為隨機，接著對照每一個句子，若句子有詞袋中的字，則編碼為<code>1</code>，無則<code>0</code>。</p>\n<p>讓我們來看看以下例子：</p>\n<ul>\n<li><strong>斷詞後的結果</strong>  <figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">魯夫/說/他/想要/成為/海賊王</span><br><span class=\"line\">娜美/說/他/想要/描繪/世界海圖</span><br></pre></td></tr></table></figure></li>\n<li><strong>詞袋</strong>  <figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">[&#x27;魯夫&#x27;, &#x27;娜美&#x27;, &#x27;説&#x27;, &#x27;他&#x27;, &#x27;想要&#x27;, 成為&#x27;, &#x27;描繪&#x27;, &#x27;海賊王&#x27;, &#x27;世界海圖&#x27;]</span><br></pre></td></tr></table></figure></li>\n<li><strong>獨熱編碼（One-Hot Encoding）</strong>  <figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">[1, 0, 1, 1, 1, 1, 0, 1, 0]</span><br><span class=\"line\">[0, 1, 1, 1, 1, 0, 1, 0, 1]</span><br></pre></td></tr></table></figure>\n我們可以發現詞袋像是一條模板一樣，將它跟句子比對之後，接著賦值。這麼一來，每一個句子就會有屬於自己的encoding，並利用這種方式賦予每一個句子獨立的存在。但就如同前面我們不斷建立的一個觀念一樣，沒有最好或是最差的方式，只有最適合的方式。既然是最早的文字轉數值方法，一定有這種方法的缺點：</li>\n</ul>\n<ol>\n<li><strong>維度災難（Disaster of Dimensionality）</strong>：假如說今天的文本詞類高達1000多種，那輸入資料的維度也就高達1000多維，意即有1000多個欄位，這是何其驚人！如此龐大的維度其實並不利於電腦的機器學習，可能會造成<strong>維度災難（Disaster of Dimensionality）</strong>，也對電腦是一大負擔。</li>\n<li><strong>資料稀疏（Data sparsity）</strong>：當資料越來越多，詞彙也跟著越來越多時，你可能會發現說，這麼一來在編碼內的<code>0</code>也會越來越多。這種情形稱為<strong>資料的稀疏性（Data sparsity）</strong>。相對於其他數值上的機器學習，這在NLP中是一個很嚴重的問題。這是因為資料一旦稀疏，就難以讓演算法判別不同句子之間的差異性（記得之前提到BoW的一個很大重點是句子之間的差異嗎？）更不用說語言具有無限可能。若出現其他新的模型沒看過的資料，BoW就比較沒辦法處理了。</li>\n</ol>\n<p>那麼後人為了解決這些問題，TF-IDF以及Word2Vec出現了。</p>\n<h2 id=\"TF-IDF\"><a href=\"#TF-IDF\" class=\"headerlink\" title=\"TF-IDF\"></a>TF-IDF</h2><p>TF-IDF，是一種在一個大語料庫中，找出每一篇文章關鍵字的計算方式。顧名思義，包含了字詞頻率（Term Frequency）以及逆向文件頻率(Inversed Document Frequency)，並將兩者相乘，所得的分數稱為TF-IDF。</p>\n<p><img src=\"https://i.imgur.com/62G3jIE.png\"></p>\n<h3 id=\"Term-frequency-（TF）\"><a href=\"#Term-frequency-（TF）\" class=\"headerlink\" title=\"Term frequency （TF）\"></a>Term frequency （TF）</h3><p>記得前面的斷詞所分成的token嗎？TF就是這些token在文本中的出現頻率，再除以那篇文章的字數，這是因為文本的篇幅都有長有短，一篇落落長的文章，詞彙出現的次數當然就可能會比較多囉，為了避免這種狀況，通常都會除上文章字數，這樣子的過程叫做將文本<strong>正規化</strong>。</p>\n<p><img src=\"https://i.imgur.com/Apsf9ei.png\"></p>\n<p>檔案<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=d_%7B%7Bj%7D%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=d_%7B%7Bj%7D%7D\">中共有$k$個詞語<br><img src=\"https://chart.googleapis.com/chart?cht=tx&chl=%7B%5Cdisplaystyle%20n_%7Bk,j%7D%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=%7B%5Cdisplaystyle%20n_%7Bk%2Cj%7D%7D\">：<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=%7B%5Cdisplaystyle%20t_%7Bk%7D%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=%7B%5Cdisplaystyle%20t_%7Bk%7D%7D\">在檔案$d_$中出現的次數<br><img src=\"https://chart.googleapis.com/chart?cht=tx&chl=n_%7B%7Bi,j%7D%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=n_%7B%7Bi%2Cj%7D%7D\">：該詞在檔案$d_$中的出現次數<br><img src=\"https://chart.googleapis.com/chart?cht=tx&chl=%7B%5Csum%20_%7Bk%7Dn_%7B%7Bk,j%7D%7D%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=%7B%5Csum%20_%7Bk%7Dn_%7B%7Bk%2Cj%7D%7D%7D\">：在檔案<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=d_%7B%7Bj%7D%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=d_%7B%7Bj%7D%7D\">中所有字詞的出現次數之和。</p>\n<h3 id=\"Inversed-Document-frequency-（IDF）\"><a href=\"#Inversed-Document-frequency-（IDF）\" class=\"headerlink\" title=\"Inversed Document frequency （IDF）\"></a>Inversed Document frequency （IDF）</h3><p>那為了要減少term frequency在某些文章中過高，或者是有些字在文章中雖然很常出現，但是卻不代表任何意義的情形（e.g. a, an, the），所以也要乘上一個數，可以使其值減低。對機器學習有一點概念的朋友，我認為可以把IDF想像成Penalty。Inversed Document frequency計算的則是以語料庫中的文本數除以文本中含有token的文本數，接著再取自然對數。白話的意思就是，若<strong>某些字集中出現在某幾篇文本中，代表這些字在這些文本中越重要，idf就越高</strong>，反之，<strong>若某些字在幾乎所有文本中都有出現，代表這些字其實並不那麼重要，idf就越低。</strong></p>\n<p><img src=\"https://i.imgur.com/xCnZlRe.png\"></p>\n<p><img src=\"https://chart.googleapis.com/chart?cht=tx&chl=%7CD%7C\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=%7CD%7C\">：語料庫中的檔案總數<br><img src=\"https://chart.googleapis.com/chart?cht=tx&chl=%7C%5C%7Bj:t_%7B%7Bi%7D%7D%5Cin%20d_%7B%7Bj%7D%7D%5C%7D%7C\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=%7C%5C%7Bj%3At_%7B%7Bi%7D%7D%5Cin%20d_%7B%7Bj%7D%7D%5C%7D%7C\">：包含詞語<img src=\"https://chart.googleapis.com/chart?cht=tx&chl=t_%7B%7Bi%7D%7D\" alt=\"https://chart.googleapis.com/chart?cht=tx&amp;chl=t_%7B%7Bi%7D%7D\">的檔案數目</p>\n<p>下面提供簡單程式碼：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> feature_extraction  </span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.feature_extraction.text <span class=\"keyword\">import</span> TfidfTransformer  </span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.feature_extraction.text <span class=\"keyword\">import</span> CountVectorizer  </span><br><span class=\"line\">corpus = [<span class=\"string\">&quot;魯夫 是 想要 成為 海賊王 的 男人&quot;</span>, </span><br><span class=\"line\">        <span class=\"string\">&quot;索隆 是 想要 成為 世界 最強 劍士 的 男人&quot;</span>, </span><br><span class=\"line\">        <span class=\"string\">&quot;娜美 想要 畫出 全 世界 的 海圖&quot;</span>, </span><br><span class=\"line\">        <span class=\"string\">&quot;香吉士 想要 找到 ALL_BLUE&quot;</span>, </span><br><span class=\"line\">        <span class=\"string\">&quot;騙人布 想要 成為 勇敢 的 海上戰士&quot;</span>, </span><br><span class=\"line\">        <span class=\"string\">&quot;喬巴 想要 成為 萬靈藥&quot;</span>, </span><br><span class=\"line\">        <span class=\"string\">&quot;羅賓 想要 找到 所有 歷史本文&quot;</span>, </span><br><span class=\"line\">        <span class=\"string\">&quot;佛朗基 想要 千陽號 航向 世界 盡頭&quot;</span>, </span><br><span class=\"line\">        <span class=\"string\">&quot;布魯克 想要 環繞 世界 重逢 拉布&quot;</span>]</span><br><span class=\"line\">vectorizer=CountVectorizer() </span><br><span class=\"line\">transformer=TfidfTransformer()  </span><br><span class=\"line\">tfidf=transformer.fit_transform(vectorizer.fit_transform(corpus))</span><br><span class=\"line\">word=vectorizer.get_feature_names()</span><br><span class=\"line\">weight=tfidf.toarray() </span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(weight)):  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">u&quot;-------這裡輸出第&quot;</span>,i,<span class=\"string\">u&quot;類文字的詞語tf-idf權重------&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(word)):  </span><br><span class=\"line\">        <span class=\"built_in\">print</span>(word[j],weight[i][j]  )</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">all_blue 0.0</span></span><br><span class=\"line\"><span class=\"string\">世界 0.0</span></span><br><span class=\"line\"><span class=\"string\">佛朗基 0.0</span></span><br><span class=\"line\"><span class=\"string\">劍士 0.0</span></span><br><span class=\"line\"><span class=\"string\">勇敢 0.0</span></span><br><span class=\"line\"><span class=\"string\">千陽號 0.0</span></span><br><span class=\"line\"><span class=\"string\">喬巴 0.0</span></span><br><span class=\"line\"><span class=\"string\">娜美 0.0</span></span><br><span class=\"line\"><span class=\"string\">布魯克 0.0</span></span><br><span class=\"line\"><span class=\"string\">想要 0.21155991248018197</span></span><br><span class=\"line\"><span class=\"string\">成為 0.3582020693353289</span></span><br><span class=\"line\"><span class=\"string\">所有 0.0</span></span><br><span class=\"line\"><span class=\"string\">找到 0.0</span></span><br><span class=\"line\"><span class=\"string\">拉布 0.0</span></span><br><span class=\"line\"><span class=\"string\">最強 0.0</span></span><br><span class=\"line\"><span class=\"string\">歷史本文 0.0</span></span><br><span class=\"line\"><span class=\"string\">海上戰士 0.0</span></span><br><span class=\"line\"><span class=\"string\">海圖 0.0</span></span><br><span class=\"line\"><span class=\"string\">海賊王 0.552052456377027</span></span><br><span class=\"line\"><span class=\"string\">環繞 0.0</span></span><br><span class=\"line\"><span class=\"string\">男人 0.4662722935918962</span></span><br><span class=\"line\"><span class=\"string\">畫出 0.0</span></span><br><span class=\"line\"><span class=\"string\">盡頭 0.0</span></span><br><span class=\"line\"><span class=\"string\">索隆 0.0</span></span><br><span class=\"line\"><span class=\"string\">羅賓 0.0</span></span><br><span class=\"line\"><span class=\"string\">航向 0.0</span></span><br><span class=\"line\"><span class=\"string\">萬靈藥 0.0</span></span><br><span class=\"line\"><span class=\"string\">重逢 0.0</span></span><br><span class=\"line\"><span class=\"string\">香吉士 0.0</span></span><br><span class=\"line\"><span class=\"string\">騙人布 0.0</span></span><br><span class=\"line\"><span class=\"string\">魯夫 0.552052456377027</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"問題與討論\"><a href=\"#問題與討論\" class=\"headerlink\" title=\"問題與討論\"></a>問題與討論</h2><p>被台灣教育荼毒過的對這五個字一定很熟悉，但很重要！不免俗，我們來討論一下TF-IDF可能需要特別注意哪些地方，以及缺點。</p>\n<ul>\n<li><strong>語料庫的選擇很重要</strong><br>前面有提到這些TF-IDF的計算方法高度依賴語料庫中其他文本的詞類，因此當你要找關鍵字時，<strong>詞庫與詞庫間、文本檔案與檔案間的對應要特別留意</strong>，假如你要找的是某領域語料庫的關鍵字有哪些，那就不能用相同領域的對應語料庫（referent corpus），否則關鍵字就浮不出來了；同理，如果你要找的是某特定文本在某個語料庫中的關鍵字，那就要以「自己比自己」的方式，方可找到你要的結果。孰好孰壞？老話一句，<strong>看你的目的為何</strong>。</li>\n<li><strong>放大檢視優缺點</strong><br>優點就是簡單快速容易理解，但缺點的話，由於詞與詞之間都被切開來，單純以詞頻的角度去看待文章，這麼做反倒會忽略詞在文章中的位置（我們得要知道同一個詞在文章中的不同位置，可能代表不同的意義）另外，很少見的生澀詞彙可能會爆冷門變成關鍵詞，但重要的人名地名等等反倒不會出現在關鍵詞的前幾名中。</li>\n</ul>\n<p>參考資料：</p>\n<ol>\n<li><a href=\"https://medium.com/@derekliao_62575/nlp%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%9F%B7%E8%A1%8C%E6%AD%A5%E9%A9%9F-ii-bag-of-words-%E8%A9%9E%E8%A2%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B-3b670a0c7009\">NLP的基本執行步驟(II) — Bag of Words 詞袋語言模型</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/274720/why-is-it-a-big-problem-to-have-sparsity-issues-in-natural-language-processing\">Why is it a big problem to have sparsity issues in Natural Language processing (NLP)?</a></li>\n<li><a href=\"https://shangzhi-huang.gitbook.io/workspace/nlp-zhi-guan-jian-ci-ti-qu/gai-jin-tfidf-suan-fa\">改进TF-IDF算法</a></li>\n<li><a href=\"https://www.796t.com/content/1546773727.html\">[python] 使用scikit-learn工具計算文字TF-IDF值</a></li>\n</ol>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"【NLP】Day 9: 又是國王皇后的例子啦！Word2Vec、N-Gram","url":"/2023/01/08/%E3%80%90NLP%E3%80%91Day-9-%E5%8F%88%E6%98%AF%E5%9C%8B%E7%8E%8B%E7%9A%87%E5%90%8E%E7%9A%84%E4%BE%8B%E5%AD%90%E5%95%A6%EF%BC%81Word2Vec%E3%80%81N-Gram/","content":"<blockquote>\n<p>「模型有什麼盲點，反映其創造者的判斷，也反映創造這重視哪些東西。雖然模型據說是公正的，他們其實反映某些人的目標和意識形態。」<br>凱西・歐尼爾《大數據的傲慢與偏見》</p>\n</blockquote>\n<p>我們前面提到了語言模型（Language modeling），是以統計機率的角度來看待語言（是否合理？我可能要先打個問號），所以為了要進行統計，過去通常會將語言以數值的方式呈現。昨天提到了幾種方法，首先是詞袋，只是有幾個缺點，第一是維度災難，太高的維度會給予電腦太高的負擔；第二是資料太稀疏，模型無法判別出資料的差異。所以自然語言處理專家後續又再發展出了幾種方法來改善這些缺點，像是昨天介紹的TF-IDF，以及今天要介紹的N-Gram以及Word2Vec。</p>\n<p>不過什麼叫做以機率的角度來看待語言，我們可以先來看看以下例子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">1. 魯夫說他想要成為海賊王</span></span><br><span class=\"line\"><span class=\"string\">2. 說海賊王成為想要魯夫他</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>\n<p>今天在我們所說的那些自然語言中，第一句出現的機率絕對會比第二句高吧？也就是說，為了要「預測」後面會出現什麼句子或是詞語，或是在進行語音辨識、拼字矯正、文字校正這些較不清楚的資料輸入時，每個詞出現的機率在這時就變得特別重要。而N-Gram就是建立在這個基礎之上所訓練而成的模型方法。</p>\n<h2 id=\"N-Gram\"><a href=\"#N-Gram\" class=\"headerlink\" title=\"N-Gram\"></a>N-Gram</h2><p>N-gram代表由N個字所組成的字串，一個字叫unigram，兩個字叫bigram，三個字叫trigram。以前面海賊王的句子為例，bigram在例句一中，就會以 <em>魯夫&#x2F;夫說&#x2F;說他…</em> 的形式呈現，並賦予每一個bigram可能的出現機率。這種方式相較於最近頂尖流行的神經網路、BERT，是一個相對單純的建模方式外，也是了解語言建模基礎概念的重要基礎工具。以下我們將以bigram為例，來為您介紹N-gram的概念。</p>\n<p>Bigram在這邊要做的，就是計算以前面的字為前提之下，後面的字會出現的機率是多少。比如說，今天我們有「海」這個字，後面是「賊」的機率是多少，因為前面是「海」的時候，後面接的也有可能是「洋」、「底」、「馬」。這該怎麼進行計算呢？</p>\n<center> \n\n<p><img src=\"https://i.imgur.com/NGv9y0f.png\"> </p>\n</center>\n\n<p>各位可千萬不要被這些數學符號嚇到了，其實就是我們高中所學的條件機率！同樣是以前面「海」為例的話，分母其實就是所有以「海」為開頭的詞語，其出現次數；至於分子就是我們要計算的「海賊」，其出現的次數。那假如今天要建立一個簡單的bigram模型，該怎麼做？讓我們來看看以下例子，有個簡易的範例語料庫：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">&lt;s&gt;魯夫是海賊王&lt;/s&gt;</span><br><span class=\"line\">&lt;s&gt;魯夫戴草帽&lt;/s&gt;</span><br><span class=\"line\">&lt;s&gt;索隆砍敵人&lt;/s&gt;</span><br></pre></td></tr></table></figure>\n<p>要計算這些bigram機率的話，可以這麼計算：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">P(魯|&lt;s&gt;)= 2/3 P(夫|魯)= 2/2 P(是|夫)= 1/2</span><br></pre></td></tr></table></figure>\n<p>這邊就不把所有計算都列出來的，相信聰明的各位一定可以舉一反三吧！</p>\n<p>透過這種方式所訓練出來的模型，就可以得到其訓練資料中的機率分配，而透過這些機率分配，就可以做到很多事情，比如說今天我們要運用這個模型來自動生成句子，那模型就可以從這些機率分配中抽出具有較高機率出現的字，也就比較不可能產生機率較低的字。</p>\n<center>\n\n<p><img src=\"https://i.imgur.com/9ROanJL.png\"></p>\n</center>\n\n<h3 id=\"特性及優缺點\"><a href=\"#特性及優缺點\" class=\"headerlink\" title=\"特性及優缺點\"></a>特性及優缺點</h3><ol>\n<li>n越大，模型出來的效果越好（但相對的運算消耗量也越大）</li>\n<li>內容高度依賴訓練資料集，所以測試的資料跟訓練的資料，其domain相去甚遠時就不太有用，因此選擇的訓練資料要按照task的內容</li>\n<li>完全無法處理訓練資料集中沒看過的資料（OOV, Out-of-voculary）</li>\n</ol>\n<h2 id=\"Word2Vec\"><a href=\"#Word2Vec\" class=\"headerlink\" title=\"Word2Vec\"></a>Word2Vec</h2><p>跟前面學習下一個字出現的機率完全不同的概念，我們在這邊將不會計算文字w在文字x出現的次數，而是訓練一個分類器（classifier），來預測文字w是否有可能會出現在文字x附近，也就是把所有訓練資料壓在一個向量空間中，而這個分類器所得的權重，也就是所謂的<strong>詞嵌入（word embedding）</strong>。最後訓練出來的這些詞嵌入，會將不同的詞以向量的形式呈現，並透過計算的方式來理解詞與詞之間的關係。</p>\n<center>\n\n<p><img src=\"https://i.imgur.com/NBabxjX.png\"></p>\n</center>\n\n<p>以這張圖來舉例的話，機器學習模型就可以藉由男人（Man）與女人（Woman）的embedding來理解國王（King）以及皇后（Queen）之間的關係。當我們可以將詞轉換成向量時，就可以進行許多跟相似度有關的應用，比如說文本相似度、辭意相似度。</p>\n<p>大家是否還記得Wordle，當時也有人透過Google所訓練的Word2Vec，做了一個猜字網站<a href=\"https://semantle.com/\">Semantle</a>，大家有興趣可以去玩玩看，其實漸漸就可以發現，這不一定是我們所認知的近義詞，而是依照訓練資料的內容所得的「近義詞」。</p>\n<h3 id=\"特性及優缺點-1\"><a href=\"#特性及優缺點-1\" class=\"headerlink\" title=\"特性及優缺點\"></a>特性及優缺點</h3><p>看起來這麼厲害的詞向量，也是有它的優缺點，而按照慣例，我們也要來了解一下詞向量跟詞嵌入有什麼優缺點，以及有什麼特性需要我們去注意的。</p>\n<h4 id=\"刻板印象\"><a href=\"#刻板印象\" class=\"headerlink\" title=\"刻板印象\"></a>刻板印象</h4><p>由於詞向量是透過過去的大量文本資料所訓練而得的，所以其實某種程度上這些向量也隱含著人類過去隱含在文本中的一些暗示及毛病，也就是說，這也高度依賴著訓練資料。這些訓練資料若是包含對性別的歧視或刻板印象，那麼訓練出來的詞向量，也會有這種含義。例如：訓練出來「男人」相對於「程式工程師」，此時再輸入「女人」，帶有偏見的詞向量就會回答「家管」。這樣的訓練結果並不合適，因為假如程式設計公司今天要招募工程師，並用自然語言處理模型替履歷進行分類時，就可能因此將所有女性姓名全部放到「不合適」的類別，反倒更加深了性別刻板印象。</p>\n<h4 id=\"代表性問題\"><a href=\"#代表性問題\" class=\"headerlink\" title=\"代表性問題\"></a>代表性問題</h4><p>同理，過去的這些「有毒的資料」也會導致模型對族群有錯誤的連結。就有研究顯示，在Word2Vec中，非裔美國人的名字較容易與負面詞彙連結，而歐裔美國人較容易與正面詞彙進行連結；或者是，男人較容易與數學、工程等詞進行連結，而女人與藝術、文學的詞向量距離也比較近。</p>\n<h2 id=\"結語\"><a href=\"#結語\" class=\"headerlink\" title=\"結語\"></a>結語</h2><p>說了這麼多，其實就是要了解到，在人工智慧越來越進步的時代，更多的是要探討這些模型 <strong>對人類社會可能造成的影響。</strong> 也就是說，<strong>人工智慧應該是幫助人類越來越進步的工具，是相輔相成的存在，我們不應該因為人工智慧，就放棄了身而為人所應該堅持的基本價值。</strong> 該怎麼做？ <strong>人工智慧、機器學習模型的運作必須是透明的，訓練資料來源必須是透明的、產生結果以及原因必須是透明的</strong> ，這也就是我認為人工智慧應該以加強解釋力的方向持續努力的原因，更是人類在大數據時代實踐公平以及負責任所應該要學習的議題。</p>\n"},{"title":"【NLP】Day 1: 前往NLP的偉大航道！一起成為我的夥伴吧！","url":"/2022/09/01/%E3%80%90NLP%E3%80%91%E6%88%91%E6%89%80%E7%9F%A5%E9%81%93%E7%9A%84%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86/","content":"<blockquote>\n<p>擁有財富、名聲、勢力，擁有整個世界的海賊王，哥爾・D・羅傑，他在臨刑前的一句話讓人們趨之若鶩，奔向大海。<br>「想要我的財寶嗎？想要的話可以全部給你，去找吧，我把所有財寶都放在這裡」<br>於是所有男子漢航向偉大的航道追逐夢想，世界開始迎接大海賊時代的來臨。<br>《海賊王》第一話〈Romance Dawn 冒險的黎明〉</p>\n</blockquote>\n<h2 id=\"NLP？那是什麼？可以吃嗎\"><a href=\"#NLP？那是什麼？可以吃嗎\" class=\"headerlink\" title=\"NLP？那是什麼？可以吃嗎\"></a>NLP？那是什麼？可以吃嗎</h2><p>筆者自從就讀語言所這一年多以來，很多人都會問我：「咦？Milan，你讀語言所，啊你是唸什麼的呀？」，我都會說：「喔！我是唸計算語言學，做自然語言處理的。NLP啦！」只是往往這麼回答朋友還是親戚們，他們臉上都還是會掛著大大的問號。好啦，有些人可能會知道NLP是什麼，他們會說：「喔喔喔！我知道！就是神經語言處理啦！欸，我覺得那個很有用耶，可以幫助我們用另一個角度來看待事情。」<br></br><br>事實上，自然語言處理（Natural Language Processing）跟神經語言處理（Neural Language Processing）是兩件完全不同的領域。<br></br><br>自然語言處理，根據維基百科的定義，是<strong>人工智慧</strong>跟<strong>語言學</strong>的跨領域學科，藉由<em>讓電腦將輸入語言轉換成有意義的符號和關係</em>，或是將語言轉換成電腦可以理解的數據，並透過機器學習模型來解決各種問題，例如<em>文本分類</em>、<em>文本生成</em>、<em>情感分析</em>以及<em>自然語言理解</em>、<em>聊天機器人</em>等等族繁不及備載的應用方式，其中的技術包含認知、理解、生成等三大部分。討論到人工智慧，大家可能也會想到先前Google所開發，聲稱已具有「人格意識」的語言人工智慧LaMDA，其實這也是自然語言處理的一環，但至於其是否真的有「自我意識」，可能就還有待商榷。<br></br><br>嗯？你問說那神經語言處理是什麼，我覺得這部分對我來說有點玄，<del>可能要問一下洪蘭教授，或者是問天了。</del><br></br><br>說了這麼多冠冕堂皇的定義解釋，大家可能也還不是很理解自然語言處理到底是在做什麼。想問一下各位是否曾經被「祖」過？臉書的演算法都會藉由偵測人們分享在社群中的貼文是否包含仇恨言論，並且決定後續的動作，如果貼文被刪除並被禁文三十天，就是被「祖」了。YouTube也會透過演算法判斷影片內容是否適合廣告商投放，YouTube也會判斷題材內容是否適合提供大眾觀賞等等，若不適合則投以黃標；另外，像是最近很紅的假新聞議題，如何透過程式語言去偵測，藉此去預防假新聞的過度氾濫。以上種種，都是需要運用自然語言處理的技術來解決的。<br></br><br>可是這項技術到目前為止的發展足夠成熟嗎？目前的人工智慧真的有這麼厲害嗎？想先問問大家是否曾經有過、或是看過別人有過被演算法耽誤的類似經驗，比如說明明只是想跟朋友分享某部動漫給了很多「殺必死」，但卻因為「殺」跟「死」被臉書判斷成帶有仇恨言論，莫名其妙地被禁言三十天，何其無辜？或者是作為影音創作者，在影片中可能也不太知道說了或做了什麼事，就被黃標。類似的演算法錯誤判斷的案例層出不窮。這也是為什麼到最後其實Meta跟YouTube到後來也逐漸改回用人工的方式進行審查，畢竟就現有的技術來說，人工審查的正確比例還是比機器高的許多。只是人工審查又可能有標準不一的問題，不同的人判斷事情有不同的標準；對於演算法來說，也同樣會有不同標準的議題需要討論，但這又是另一個層面的問題了。<br></br><br>綜上所述，其實我們可以發現，計算語言學、自然語言處理、人工智慧還有非常非常大進步的空間。誇張一點，在我們開發出像哆啦A夢或是《銀翼殺手 Blade Runner》裡面的仿生人、《西部世界 Westworld》裡面的接待員之前，永遠都還有非常大的發展餘地。就像是《Bleach》裡面，涅繭利對上第八十刃・薩爾阿波羅所說：「我討厭完美，一旦完美就再沒有進步的餘地，也沒有創造的空間。這代表智慧與才能都將無用武之地了，對我們科學家來說，完美就是絕望。」（這比喻也太宅…）這也是人類逐漸走向進步與未來所必備的進程之一，而我們都只是這巨大的洪流中的一隻隻小蝦米而已。<br></br><br><img src=\"https://hips.hearstapps.com/hmg-prod/images/screen-shot-2019-07-24-at-1-29-59-pm-1563989459.png\"></p>\n<p>圖一、《銀翼殺手》中最後羅伊・巴蒂對戴克所說的雨中獨白，探討何謂「意識」、何謂「人」</p>\n</br>\n\n<p><img src=\"https://www.tvinsider.com/wp-content/uploads/2022/08/evan-rachel-wood_-westworld-season-4-finale.jpg\"></p>\n<p>圖二、《西部世界》中，希望不管是人或是接待員都能擁有真正自由的迪樂芮</p>\n</br>\n扯遠了。是說，雖然計算語言學/自然語言處理可以大至Google的LaMDA模型，小至假新聞、臉書貼文審查。這麼一說，自然語言處理真的離我們這麼遠嗎？在我踏進這艘賊船的這兩三年，我發現好像其實也不是這麼一回事。在迎接人工智慧的時代來臨，有些人會擔心人工智慧是否會取代人類；我是認為，好好地了解這些平常在我們的日常生活中無處不在的人工智慧，還有他們的運作方式，才不會被人工智慧所駕馭，就如同唐鳳所說，AI應該是輔助式智慧，幫助人類相輔相成。更不該因為是AI，就放棄了這個社會所堅持的價值。\n</br>\n\n<h2 id=\"踏上偉大航道前的必備：紀錄指針\"><a href=\"#踏上偉大航道前的必備：紀錄指針\" class=\"headerlink\" title=\"踏上偉大航道前的必備：紀錄指針\"></a>踏上偉大航道前的必備：紀錄指針</h2><p>在踏入這領域的這兩三年，其實學習自然語言處理的路上，就跟魯夫一樣，一路上找尋夥伴，在一座座的島嶼冒險，過程有哭有笑，<del>可能還死哥哥（都過幾年，魯夫都已經move on了，這不算爆雷吧？）</del>，最終抵達拉乎德爾，找到魯夫心目中真正的大秘寶。自然語言處理亦然，在偉大航路前半段的樂園，先逐漸了解基礎文本處理、慢慢接觸到模型後，漸漸地體認到這個世界有多大，直到進入偉大航路後半段的新世界，已經可以熟用霸氣後，學會應用各種技能，並開始認識自然語言處理中的巨人等等，一路雖然辛苦，但我想會很值得。<br></br><br>因此，我在這三十天的鐵人賽中，將會介紹一些成為語言資料科學家常會用到的技能，也當作自己學習路上的紀錄。俗話說：「師父領進門，修行在個人。」在接下來的文章中，將會嘗試著用最粗淺的語言來解釋現今文本處理的入門技術及應用，且將會使用Python作為主要的語言，希望可以作為讀者的紀錄指針，將各位引領至計算語言學的偉大航道。內容將包含：</p>\n<ul>\n<li>文本處理的東方藍：資料預處理<ul>\n<li>基礎Python</li>\n<li>正規表達式（regular expression）:檢查、搜尋、抽取</li>\n<li>斷詞（segmentation）: Articut、CKIP、Jieba</li>\n<li>斷詞層級與處理（segmentation）: tokenization、lemmatization、stemming</li>\n</ul>\n</li>\n<li>開始踏上偉大航道：文本計算<ul>\n<li>Bag of Words、TF-IDF、N-Gram</li>\n<li>Word2Vec</li>\n<li>機器學習：貝氏分類器</li>\n<li>機器學習：羅吉斯迴歸</li>\n</ul>\n</li>\n<li>這裡就是新世界了！<ul>\n<li>深度學習！<ul>\n<li>人工神經網路（Artificial Neural Network, ANN）</li>\n<li>遞迴神經網路（Recurrent Neural Network, RNN）</li>\n<li>長短期記憶模型（Long Short-Term Memory, LSTM）</li>\n<li>長短期記憶模型（Bidirectional Long Short-Term Memory, BiLSTM）</li>\n</ul>\n</li>\n<li>特別篇：Loki</li>\n</ul>\n</li>\n<li>冥王雷利補習班：實戰篇<ul>\n<li>網路爬蟲</li>\n<li>用Flask架出你的第一個網頁</li>\n<li>利用Docker在虛擬環境佈署專案</li>\n<li>資料存儲：資料庫建置與應用</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"環境建置\"><a href=\"#環境建置\" class=\"headerlink\" title=\"環境建置\"></a>環境建置</h2><h3 id=\"Python\"><a href=\"#Python\" class=\"headerlink\" title=\"Python\"></a>Python</h3><p>因為在接下來的文章中，將會使用Python作為主要的程式語言。因此在一起踏上旅程前，要請你先把Python的環境建置起來。這部分因為網路上也已經有族繁不及備載的教學文章，甚至影片，所以在這邊也不再贅述。請讀者按照系統到所屬的網頁建置環境：</p>\n<ul>\n<li><a href=\"https://docs.microsoft.com/zh-tw/windows/python/beginners\">Windows</a></li>\n<li><a href=\"https://docs.python.org/zh-tw/3/using/mac.html\">OS X</a></li>\n<li><a href=\"https://ubunlog.com/zh-TW/python-3-9-como-instalar-en-ubuntu-20-04/?_gl=1*xg80z2*_ga*YW1wLWk0TmRyYml2cTFVUVAtWjdGVEVHRDJkRk5kT3NyQUxtR0RIQmJ4TjFob2NmSVA0U1Atczk1cEhPNGs0Ukl6S3c\">Ubuntu</a>: 都已經用烏班圖了，應該也會建置環境了吧？</li>\n</ul>\n<h3 id=\"編譯器\"><a href=\"#編譯器\" class=\"headerlink\" title=\"編譯器\"></a>編譯器</h3><p>如果你是初學者，你可以使用Jupyter Notebook，或者是Google Colaboratory；但學習到了一定階段，都會需要轉移到原始碼編輯器上運行，才會得到完整的開發體驗。在寫程式時，每個人都會有習慣的原始碼編輯器，各自有各自的優點，你可以選擇自己喜歡的原始碼編輯器做使用。</p>\n<ul>\n<li><a href=\"https://code.visualstudio.com/\">VSCODE</a>: 微軟開發的編輯器，很多外掛跟好用的介面，是我自己常用的原始碼編輯器。</li>\n<li><a href=\"https://wingware.com/\">WingPro</a>: 專為Python打造的編輯器。之前實習公司前老闆的愛用編輯器。</li>\n<li><a href=\"https://www.sublimetext.com/\">Sublime Text</a>: 簡單好用，畫面介面簡潔不複雜。在學校的現任老闆的愛用編輯器</li>\n<li><a href=\"https://notepad-plus-plus.org/\">NotePad++</a>: 台灣工程師開發的編輯器，開源好用，且為世界的不公不義做出了許多行動打抱不平。</li>\n</ul>\n","categories":["自然語言處理"],"tags":["自然語言處理","NLP"]},{"title":"【Other】Day 29: 除了自然語言處理，我們還要...？處理資料時，你可能需要會的這些那些","url":"/2023/01/08/%E3%80%90Other%E3%80%91Day-29-%E9%99%A4%E4%BA%86%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%EF%BC%8C%E6%88%91%E5%80%91%E9%82%84%E8%A6%81-%EF%BC%9F%E8%99%95%E7%90%86%E8%B3%87%E6%96%99%E6%99%82%EF%BC%8C%E4%BD%A0%E5%8F%AF%E8%83%BD%E9%9C%80%E8%A6%81%E6%9C%83%E7%9A%84%E9%80%99%E4%BA%9B%E9%82%A3%E4%BA%9B/","content":"<p>終於快要到鐵人賽的尾聲，在處理 NLP 任務的時候，除了要瞭解不同語言模型的內部架構、功能，還有這些模型適合的不同任務內容，有時候也會需要學習其他技能，來幫助你執行 NLP 的任務。所謂的其他技能，除了最基本的程式語言之外（不一定是 Python），我們昨天以及前天所說的<strong>網路爬蟲</strong>，就是除了模型以外，處理自然語言處理任務上，可能會需要學習的。那由於時間的關係，再加上這些主題有太多內容可以講，所以今天的主題來簡單介紹一下在進行自然語言處理的時候，可能需要學習，但還來不及詳細介紹的技能。</p>\n<h2 id=\"資料庫基本概念（Database）＆-SQL\"><a href=\"#資料庫基本概念（Database）＆-SQL\" class=\"headerlink\" title=\"資料庫基本概念（Database）＆ SQL\"></a>資料庫基本概念（Database）＆ SQL</h2><p>身為一個資料處理人員，資料的儲存問題將會是一個需要考量的點之一。你可能會想問說：</p>\n<blockquote>\n<p>我們為什麼不直接像先前所說的，利用網路爬蟲爬下來儲存成 <code>json</code> 檔之後，再將這些資料儲存在本機端就好了？</p>\n</blockquote>\n<p>有一點需要先釐清，通常在進行程式開發時，基本上都是採取多人共同開發模式。如果說只單純利用前面所說的資料儲存方式，在多人協作上就會產生很多的不便以及困難點，畢竟不可能大家都在那邊上傳 Google 雲端，然後還要上傳下載等等等之類的，過多的繁雜程序會造成任務處理的時間增加，反而沒有效率 <del>畢竟工程師都希望時間越快越好</del> 。另外，一旦處理的任務變得越來越複雜，資料與資料之間產生相依性的時候，複雜的資料結構也會造成困難（例如：增加新進人員理解資料結構的速度）。</p>\n<p>我們可以思考一下，假如今天有家百貨公司需要建構資料庫，我們會需要哪些資料？可能有：</p>\n<ol>\n<li>會員基本資料：姓名、手機、地址、Email etc.</li>\n<li>顧客購買紀錄</li>\n<li>商品資料</li>\n<li>樓層資料</li>\n</ol>\n<p>因為我們沒有辦法在同一個表格同時包含所有我們需要的資料，意思就是，沒有辦法在同一個表格同時包含會員的基本資料、又涵蓋所有顧客購買紀錄；那所謂的相依性，就是我們可以<strong>透過共同欄位的資料，來連結不同相對應的表格</strong>。例如：在會員基本資料以及顧客購買紀錄的表格中，有個共同欄位「姓名」（通常會用 ID 但這裡以姓名為例）。那麼就可以透過「姓名」欄位來「連結」兩個表格，也就是：「紀錄了王小明在百貨公司購買哪些商品的表格」。</p>\n<p>回到主題，如果我們同時需要：</p>\n<ol>\n<li>多人共同存取資料</li>\n<li>處理相依性問題</li>\n</ol>\n<p>那這時就會需要使用資料庫，來提升開發的速度。</p>\n<p>存取資料庫，會利用另外一個程式語言 SQL ，來提取資料庫中的資料。那基本架構大概會是長這樣：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> [回傳欄位]</span><br><span class=\"line\"><span class=\"keyword\">FROM</span> [資料庫表格]</span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> [條件式篩選]</span><br><span class=\"line\"><span class=\"keyword\">GROUP</span> <span class=\"keyword\">BY</span> [組合欄位]</span><br><span class=\"line\"><span class=\"keyword\">HAVING</span> [組合後進行的條件式篩選] <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> [需要排序的欄位] </span><br></pre></td></tr></table></figure>\n\n<p>那所謂的資料庫有以下這些：</p>\n<ul>\n<li>Oracle </li>\n<li>MySQL </li>\n<li>MS SQL Server </li>\n<li>PostgreSQL </li>\n<li>Amazon Redshift </li>\n<li>IBM DB2 </li>\n<li>MS Access </li>\n<li>SQLite </li>\n<li>Snowflake</li>\n</ul>\n<p>大家如果有興趣可以再去查，因為這又完全是另外一個大主題了。</p>\n<h2 id=\"用-Python-架網頁以及架設-API：Flask\"><a href=\"#用-Python-架網頁以及架設-API：Flask\" class=\"headerlink\" title=\"用 Python 架網頁以及架設 API：Flask\"></a>用 Python 架網頁以及架設 API：Flask</h2><p>前面說到，html + CSS + Javascript。另外，也可以用 Python 來建立網站。這邊提供非常簡單的介紹，通常若要用 Flask 來搭建網站，會需要以下程式碼：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> flask <span class=\"keyword\">import</span> Flask</span><br><span class=\"line\"></span><br><span class=\"line\">app = Flask(__name__)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@app.route(<span class=\"params\"><span class=\"string\">&quot;/&quot;</span></span>)</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">home</span>():</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&quot;&quot;&quot;&lt;!DOCTYPE html&gt;</span></span><br><span class=\"line\"><span class=\"string\">&lt;html lang=&quot;en&quot;&gt;</span></span><br><span class=\"line\"><span class=\"string\">&lt;head&gt;</span></span><br><span class=\"line\"><span class=\"string\">    &lt;meta charset=&quot;UTF-8&quot;&gt;</span></span><br><span class=\"line\"><span class=\"string\">    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt;</span></span><br><span class=\"line\"><span class=\"string\">    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;</span></span><br><span class=\"line\"><span class=\"string\">    &lt;title&gt;Document&lt;/title&gt;</span></span><br><span class=\"line\"><span class=\"string\">&lt;/head&gt;</span></span><br><span class=\"line\"><span class=\"string\">&lt;body&gt;</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">&lt;/body&gt;</span></span><br><span class=\"line\"><span class=\"string\">&lt;/html&gt;&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    app.run()</span><br></pre></td></tr></table></figure>\n\n<p>接著直接運行這個 Python 檔，就可以用 Python 運行你的第一個網頁喔！另外，Flask 也很常會用來部署 API。所謂的 API，也就是 Application Platform Interface。記得先前我們在利用 Tensorflow 進行機器學習的時候，透過別人已經訓練好的模型來進行下游的自然語言處理任務，Tensorflow 就是 API 的其中一種。Flask 就可以用來架設 API。</p>\n<h2 id=\"Git-amp-GitHub\"><a href=\"#Git-amp-GitHub\" class=\"headerlink\" title=\"Git &amp; GitHub\"></a>Git &amp; GitHub</h2><p>前面說到，通常開發流程都會是多人共同進行的，這時多人同時存取程式碼也就成為一個需求，就會需要同時有很多分支讓開發團隊同時進行程式碼的撰寫；另外，也難保有人會寫錯程式碼，或者是有時候會發生昨天還可以跑，今天就不行跑了 <del>相信我，這很常發生，不然台灣的乖乖文化哪來的？</del>。Git 套件就可以幫助你在發生這些狀況的時候，讓他回復到尚未出問題之前的狀態。所謂「回復」、「分支」稱為「版本管理」。</p>\n<p>除了以上功能，也會需要一個儲存程式碼的地方，稱為程式庫（repository）。存放這些程式庫的地方有很多，最為人所用的是微軟開發的 GitHub，另外其他的程式庫存放處，像是 GitKraken、GitLab。</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\"></p>\n<p>明天邁入最後一天囉！我明天會說明模型解釋力的重要性，最後做個結尾，感謝各位一路以來的支持。</p>\n","categories":["自然語言管理","資料庫管理"],"tags":["自然語言處理","資料庫管理"]},{"title":"【影評】親愛的房客","url":"/2022/08/30/%E3%80%90%E5%BD%B1%E8%A9%95%E3%80%91%E8%A6%AA%E6%84%9B%E7%9A%84%E6%88%BF%E5%AE%A2/","content":"<p>合歡山上的雨聲，隨著鋼琴聲悄悄地將觀眾帶進健一的世界。一座一座綿延不絕的合歡山山脈，團團白霧彷彿預示著健一、悠宇與阿嬤三人之間不可言說的複雜關係，時不時晃動的鏡頭，代表著三人的未知、不穩定的羈絆，也揭露了內心的種種不安。<br></br><br>沒有撕心裂肺地哭喊，也沒有死心塌地依戀，在電影中的每個人，好像就只能默默地慢慢向前走，突然發覺自己走不了的時候就稍微停下來休息一下，好好沈浸在過去幸福的片刻，覺得自己準備好了，再繼續往前走。<br></br><br>時刻都在處理「分離」的我們，即使內心充滿著疑問，即使知道傷口只是結痂，也只能隨時準備出發，因為不知道心裡那道缺口有沒有被填平的一刻，因為清楚無論如何，時間還是在走，日子還是得過，我們終究得抬起頭，緊緊抓住身邊所有。</p>\n<p><img src=\"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/%E8%8E%AB%E5%AD%90%E5%84%80%E8%88%87%E7%99%BD%E6%BD%A4%E9%9F%B3%E5%9C%A8-%E8%A6%AA%E6%84%9B%E7%9A%84%E6%88%BF%E5%AE%A2-%E6%9C%89%E4%B8%8D%E5%B0%91%E6%83%85%E6%84%9F%E5%90%83%E9%87%8D%E7%9A%84%E5%B0%8D%E6%89%8B%E6%88%B2-1604032024.jpg\"></p>\n<p>就像是被叔叔強帶著去看諮商師的悠宇一樣，心裡明明知道誰才是真正對自己好的人，小小的心靈卻仍充滿著對這位「爸爸二號」的疑問，但又不知道該如何表達內心的情緒，甚至是該如何開口。出來後，在基隆港沿岸，只能默默地向前走，任由這位「爸爸二號」在後面苦苦追著，直到知道自己再也受不了時，抬起頭轉身，問道：<br></br><br>「你到底是誰？」<br></br><br>這位「爸爸二號」也想知道自己為愛能夠跟這個社會對抗到什麼程度吧。為了能夠面對過去的自己，也要坦然面對阿嬤、悠宇跟叔叔的不諒解，同時為過去拆散對方家庭的自己贖罪，所以無怨無悔地為摯愛的母親換藥、為摯愛的兒子扣上釦子，撐起這家庭的一切。蒙上了陰影的愛情，該有多苦？緬懷著與立維的過去，看著扣著制服扣子的悠宇的現在，看向霧茫茫的未來，只能抓住現在身邊所有吧。畢竟在這荒蕪的世界中，我們都是人，我們都會犯錯。<br></br><br>「沒有我，你應該比較輕鬆吧？」<br>「我有你，我會比較快樂啊！」<br></br><br>這大概是健一活在這世上唯一能抓住的快樂，另一方面也是想把立維的愛留在身邊吧，就好像立維還在身邊一樣；渴望得到立維的愛，也渴望能獲得悠宇的認同。所以當健一聽到悠宇認他做「爸爸二號」時，或許在片刻，內心的缺口多少也暫時被填補了一些，才會忍不住哽咽。</p>\n<p><img src=\"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/%E8%8E%AB%E5%AD%90%E5%84%80%E8%88%87%E5%A7%9A%E6%B7%B3%E8%80%80%E5%9C%A8-%E8%A6%AA%E6%84%9B%E7%9A%84%E6%88%BF%E5%AE%A2-%E4%B8%AD%E6%9C%89%E4%B8%80%E6%AE%B5%E6%83%85-1604032024.jpg\"></p>\n<p>所以無論發生什麼事，他都很清楚自己必須盡其所能保護好悠宇與阿嬤，但他也知道年紀還小的悠宇有權必須知道這一切，至少在被警察逮捕之前，知道他是誰。於是就帶著悠宇前往那座當年失去摯愛的山上，本想著在不被打擾的情況下，跟悠宇、跟那個人、跟這座山一起度過好幾晚，卻因警察的追捕，只能抬起頭，繼續往前走。<br></br><br>內心也早有數的阿嬤，在面對著這位認為自己害死別人兒子的健一，大概也都看在眼裡了。兒子的死很痛，雖然內心深處大概也知道這不是任何人的錯，卻也還是有過不去的坎。最令人痛苦的，並不是過不去，而是明知自己必須過去，卻始終過不去，就像是《我們與惡的距離》宋喬安在電影院外的崩潰哭泣一般，過不去的，無論如何就是過不去。所以對阿嬤而言，也是需要很長一大段時間才有辦法療癒的痛吧，所以當她對健一說：<br></br><br>「你實在也是長得很緣投，難怪我兒子會被你煞到。」<br>「…」<br>「我…其實早就已經不怨你了。」<br></br><br>健一對自己跟悠宇的好，阿嬤都看在眼裡。只是要說出這句話，需要多少夜晚的掙扎、心痛？需要多少時間的療傷？需要多寬廣的慈悲？那些必須被放下的，始終放下了；那些說不出的悲戚，也逐漸隨著健一的愛被淡化了。</p>\n<p><img src=\"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/67df3920-181e-11eb-9f8f-2f29c6e143f7-1604032020.jpeg\"></p>\n<p>面對這一切，健一都默默地吞下去了。他知道這些都是為了保護阿嬤、也保護悠宇，所以在帳篷裡，在被警察拘捕之前的片刻，寫了封信給悠宇，並告訴他：<br></br><br>「你長大以後，也許會遇到很多很討厭、很莫名其妙的事。但無論發生什麼事，都不是你的錯。」<br></br><br>短短的一句話，其實是想告訴悠宇：阿嬤的死，不是你的錯；健一被捕，不是你的錯；未來那些種種的不合理，要記得那些都不是你的錯。<br></br><br>最後，讀了那封向自己認罪的坦誠之信，悠宇也決定坦承。因為他也感受到，原來健一把拔的苦，就好像覺得自己害死阿嬤一樣，那既然健一都鼓起勇氣了，所以悠宇才願意說出實話，為彼此承擔一切，一起抬起頭，一起背負著罪過，一起往前走。即使結局始終要面對著分離，只是內心的罪惡感都能稍微減輕一點了，只要有彼此在身邊，面對重重高山，就能蹽溪過嶺，就能有勇氣面對那些很討厭很莫名其妙的事。<br></br><br>電影的結尾讓我想起這學期選修讀的小說，石黑一雄的《別讓我走》（Never Let Me Go）最後Tommy對Kathy說的話：<br></br><br>「我一直在想像著某個畫面：有條河流，水流的非常湍急，有兩個人就站在河中央，死命地緊緊抱住彼此。但最後水流實在太強，這兩個人始終還是得放手，他們只好放開彼此，各自漂流。」<br></br><br><img src=\"https://images.squarespace-cdn.com/content/v1/57e05e534402434aa0f846c2/1485210458244-NRFPGVTKT14E5S2AYDZS/never-let-me-go-deferral.jpg\"><br></br><br>雖然小說講的是愛情，甚至是某種超越愛情的情感，但對健一與悠宇這對毫無血緣關係的父子又何嘗不是呢？即使社會如何亟力拆散兩人，始終還是得學會放手。而現在不再擁有悠宇監護權的健一，眼前所能擁有的，就是極力抓住那些過去珍貴的回憶，使其永不消褪，那也是健一現在唯一剩下的美好了。</p>\n","categories":["電影評論"]},{"title":"【影評】《A.I.人工智慧》&《模仿遊戲》：何謂自我？何謂真實？","url":"/2023/01/08/%E3%80%90%E8%80%81%E5%BD%B1%E8%A9%95%E3%80%91%E3%80%8AA-I-%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7%E3%80%8B-%E3%80%8A%E6%A8%A1%E4%BB%BF%E9%81%8A%E6%88%B2%E3%80%8B%EF%BC%9A%E4%BD%95%E8%AC%82%E8%87%AA%E6%88%91%EF%BC%9F%E4%BD%95%E8%AC%82%E7%9C%9F%E5%AF%A6%EF%BC%9F/","content":"<p><img src=\"https://www.portablepress.com/wp-content/uploads/2016/06/Artificial-Intellegence.jpg\"></p>\n<p>逐漸發展出成熟人工智慧的人類，藉由電影、小說、文本揮灑著人類的想像，描繪出在後人類時代，身為萬物之靈的我們，該如何自處的觀點與逐漸對未來失去控制的焦慮。在電影《A.I.人工智慧》中，為人類未來描繪出了後末日的想像，在全球暖化後的社會，海平面的上升淹沒了現今仍富有繁榮的曼哈頓，會生老病死的人類也開發出了有感情、同時也超越人類極限的機器人，以彌補內心無法逾越的匱乏，同時卻又害怕機器人影響人類在社會上的生存地位，即秉持著造物主的傲慢，將機器人趕盡殺絕。在電影《模仿遊戲》中，則是回到人工智慧的發展剛伸出幼苗的二戰時期，藉由電腦之父圖靈的眼中，探討人工智慧是否有著思考的能力，以及為即將步入後人類時代的我們，了解人工智慧的起源。而兩部電影皆可以回溯到一個關於人工智慧最基本的核心命題：人工智慧是否會思考？何謂真實？何謂自我？</p>\n<p><img src=\"https://api.time.com/wp-content/uploads/2014/11/imitation-game.jpg\"></p>\n<p>在電影《模仿遊戲》中，藉由圖靈的引領，走進了名為人工智慧的世界。在電影的某一個片段，圖靈被警探問到的一個問題「機器能像人一樣思考嗎？」，圖靈回答：「不，機器不能思考。然而，就因為機器不能像人一樣思考，就證明機器不會思考嗎？」人類往往害怕自己無法掌控又無法理解的事物，例如會思考的機器，害怕機器有一天會反噬人類的存在以及在這社會的重要性。又例如「同性戀」。在當時的年代（甚至是現代），社會並無法接受同性戀的存在，或許是因為不了解，又或許是因為不願意不想去接納不同的存在，當人類害怕某件事物時，總是會想辦法去摧毀它，因為解決提出問題的人遠比解決問題本身還要容易且省時許多。因此圖靈問道「我們究竟是人？還是機器？」我們常常自詡是萬物之靈，擁有著智慧的我們可以統御世間一切事物，但是自古以來卻一直無情地自相殘殺，納粹希特勒仍然殺了無數猶太人以及同志族群，破壞了許多人的家庭，在這個由人類組成的社會，明明應充滿著仁慈的光輝與人性的溫暖，卻仍然如同機器一般無情地吞噬這世界上與大多數不同的人。究竟是人，還是機器，仍然那麼重要嗎；去探究兩者之間的界線，去釐清何謂真實，何謂虛構，仍然那麼重要嗎？<br>後現代觀點的哲學家布希亞所提出的「超真實」觀點，當原本生活型態被「仿真體」(simulation)遮蓋並被佔據時，「原真實」已經被「超真實」(hyperreality)給覆蓋過去，而布希亞則舉了迪士尼樂園作為例子，說明作為擬仿物的迪士尼樂園逐漸取代了美國與美國夢，使得大家逐漸誤認迪士尼樂園即為美國夢，擬仿物對原真實產生了反噬。同理，在電影《A.I.人工智慧》前半段，導演史蒂芬史匹柏透過莫妮卡、其親生兒子馬丁、人工智慧機器人大衛之間的三方對話，淋漓盡致地發揮了布希亞的「超真實」觀點。莫妮卡的親生兒子馬丁對仿真體與真實的斷裂做出了詮釋：當馬丁拿著資本主義下大量生產的直升機對大衛說：「這種玩具要把它打破才比較有意思。」這不正是一種對原形不再懷舊的表現嗎？正是因為不再懷舊，才使得無限的生產得以可能，正是因為圓形的大量複製與製造，才使得每件物品皆成為了彼此的等價物，打破的並不僅僅是直升機的形體，而是真實與擬仿物之間的界線。</p>\n<blockquote>\n<p>You don’t look like a toy, you are like an ordinary person.</p>\n</blockquote>\n<p>大衛的媽媽莫妮卡則是一個誤認「超真實」為「真實」的案例。起初莫妮卡對於其丈夫亨利帶回具有感情與同理心的大衛回家藉此替代其瀕死的兒子馬丁的行為感到無法接受與理解，但隨著大衛的種種的行為，例如叫莫妮卡「媽咪」，以及種種渴望愛的行為，讓身為「仿真體」的大衛逐漸遮蓋並佔據了莫妮卡的「原真實」，使得「原真實」被「超真實」壓扁，對於本應是仿真的機器人產生了對人的感情，仿真體與真實間產生了斷裂，讓原本的真實更為真實，也因莫妮卡的同情心，使得馬丁得以脫離原本必須面臨機器人屠宰場命運。</p>\n<p><img src=\"https://media.timeout.com/images/101619595/image.jpg\"></p>\n<p>另外，該如何從哲學觀點去判斷誰是人類、誰是機器人呢？我們可以在電影中發現一個有趣的現象：每個機器人都知道自己的存在意義，而且完全不為外在環境所動。被製造出來就是要滿足莫妮卡愛與隸屬需求的大衛，對於母愛的渴求貫穿了整部電影；裘德洛飾演的性̷愛̷愛情機器人即使在被誣陷殺人後，還是保有著極欲求愛的積極性（這是堪入耳的說法）；在大衛準備要被抓到機器人屠宰場前碰到的保母機器人，無論受到什麼樣的威脅，還是告訴著大衛不要害怕，一切會好起來，仍善盡著自己身為保母機器人的職責。<br>這正說明了人類與機器人最關鍵的差異，也就是存在主義的基本命題：存在先於本質（Existence precedes essence）。沙特認為，人在一開始並沒有任何先天的本質，而關於我們自身的一切，例如地位、身份等皆是存在後才被賦予的，因此首先必須先肯定人的存在，再去賦予存在意義，並且甚至超越自己所創造的價值。對於一個人造物而言，在被製造之前，必定會先知道其用途所在。小學美勞課，我們必定都是在腦中對作品先有個雛形，接著再動手去創造。同理，這些機器人在被人類創造出來之前，必定是先有其目的與用途，接著才去製作這些機器人。<br>身為一個人，我們時時刻刻都在改變甚至超越著自我的本質，前一秒的你跟後一秒的你很可能就代表著截然不同的意義。但在電影《A.I.人工智慧》中的機器人卻始終無法在面對快速變化的世界隨時改變自我價值，還是不斷地重複著自我的使命，直到故障為止。相反地，在《銀翼殺手2049》中，殺手Ｋ卻是在劇情的推動之下，自己眼中的世界觀不斷地翻轉，甚至是對自我價值的挑戰，之後再寫一篇文章詳細介紹。<br>順帶一提，沙特也為「存在先於本質」一說做出結論，人會害怕在眼前的過多選擇中做出錯誤判斷，因而不願意為自己的選擇負責任。對此，沙特說，你是自由的（是不是聽起來很熟悉？），所以自己做出選擇吧，唯一要記得的就是要為自己的選擇負責。</p>\n<p><img src=\"https://dp9a3tyzxd5qs.cloudfront.net/a-i-artificial-intelligence-2001-1.jpg\"></p>\n<p>話說回來，故事在大衛找到自己的造物主後，發現自己只不過是許多被創造出來的「眾多大衛中的其中一個大衛」，另一個用來滿足造物主的內心缺乏 - - 已故兒子的替代品罷了，此時的大衛已經無法在自我異化中認出自我，自我衝突與矛盾的大衛最終選擇了跳下被海水淹沒的曼哈頓，卻又在彌留之際，在水中看見了自己不斷尋找的藍仙子的雕像，於是此時的大衛，從「滿足人類情感需求的眾多大衛中的大衛」意識到自己仍然是那個「渴求莫妮卡母愛的大衛」，再次地在行動中認出自己。只是事與願違，虛構的童話終究是虛構，大衛也將虛擬的童話誤認成為真實，一心祈禱了兩千年，希望藍仙子可以讓他變成真正的人類。<br>諷刺地，過了兩千年之後，高度進化的人工智慧是存留在世上的唯一「生命」物種，人類早已自取滅絕。而這些人工智慧物種找到大衛並了解他的故事之後，透過時空軌跡以及仍保留在泰迪身上的莫妮卡的頭髮，藉由DNA以及時空代碼重現莫妮卡的樣貌與記憶。此時導演彷彿也暗示了人類的存在本身也是如同機器人一般，是藉由並不代表任何意義的代碼（人類：ＡＴＣＧ、機器人：二進位）所模擬的真實，因此布希亞也指出，人類當代社會已經不再是先有真實，接著擬仿物掩蓋了真實，而是先有擬仿物，而這些擬仿物建構了我們的真實。當真實可以藉由擬仿物不斷建構並複製，那麼真實還是真實嗎？又或者，擬仿物即是真實？<br>這些對於未來科技的想像，以及回頭看向過去的反省，人類可以透過文本的建構、哲學的探討，百花齊放的結果提醒了我們必須做好準備去迎向未來將會更迅速變化的差異性，並且以更開放的胸襟去接受這些與我們的不同。正因二戰時期的人類不願理解同志族群的權益，而痛失了可能讓人類社會向前一大步的電腦巨擘；正因我們對於自己所認知的機器人的不真實感到不信任，而錯過了與機器人共同和平相處的可能未來。或許，圖靈想要的只是安靜地一頭埋進自己感興趣的事物，並供他自由揮灑才華的空間；或許，大衛想要的只是自然地被人類所看待，愛人以及被愛，並且能與莫妮卡永遠在一起。就是因為這些種種的故事與想像，才得以讓即將跨入後人類時代的我們一管窺天地看向未來，並為即將到來的世界做好一切的準備。</p>\n<p>後記：重弄了新的部落格，以及會有這篇文是因為這學期修了人工智慧導論，一開始老師就介紹了兩部他心目中最棒的人工智慧主題的電影，要我們回家寫心得。起初想說，還有什麼人工智慧相關的電影是我沒看過的，所以導致我一開始看A.I.人工智慧的時候還有點不耐煩，但看著看著卻想到了文學批評課上所學到的理論，就想說，不然來寫寫看好了！又討厭以前那個部落格，就弄了個新的，把舊的黑歷史刪掉。就這樣，也不知道會不會有下一篇，但應該不會再刪了哈哈哈。</p>\n<p><strong>2023&#x2F;01&#x2F;08 更新</strong></p>\n<p>這是大五上學期修的人工智慧導論的其中一份作業，有點掉書袋的影評，當時主要是想學超級歪那種文本分析的方法，很酷！結果被教授特別挑出來討論，說跨領域就是要這樣！</p>\n<p>這門課讓我印象很深刻的是當時老師 demo 了一個推薦系統，結果發現那個推薦系統要 XP 的 IE 才能跑（也就是最新的 IE 也不行跑，更別提當時 IE 也已經苟延殘喘了）沒想表達什麼，只是覺得很酷。最近在整合過去到現在寫過的長文到這個個人網站，想說這篇也跟人工智慧還有影評有關，也符合這個部落格的主題，這篇才又被我翻出來，現在碩班的我再看…啊，真是掉書袋呀！</p>\n"}]