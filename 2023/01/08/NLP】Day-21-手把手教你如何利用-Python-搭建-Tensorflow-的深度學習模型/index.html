<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/icon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/icon.png">
  <link rel="mask-icon" href="/images/icon.png" color="#222">
  <meta name="google-site-verification" content="DMyRve2tDXDJqmJpSS4BaBWT1R0ZCH9n_EsCnROTiM0">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha256-xejo6yLi6vGtAjcMIsY8BHdKsLg7QynVlFMzdQgUuy8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"milanochuang.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.12.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜尋...","empty":"我們無法找到任何有關 ${query} 的搜索結果","hits_time":"${hits} 找到 ${time} 個結果","hits":"找到 ${hits} 個結果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="啊！好久沒有實作了，不曉得各位還記不記得怎麼寫 Python 呢？今天的這篇文章將主要以實作為主，簡單地介紹如何搭建出我們在過去幾天的旅程中，學到的那些形形色色的深度學習模型：LSTM、GRU，以及兩者的雙向版本，也就是BiLSTM、以及BiGRU。不過，你可能已經發現，咦？那單純的 RNN 呢？事實上，在自然語言處理的實務中，由於先前提過的那些缺失，如記性不好等問題，已經很少人使用單純的 RNN">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP】Day 21: 手把手教你如何利用 Python 搭建 Tensorflow 的深度學習模型 ">
<meta property="og:url" content="https://milanochuang.github.io/2023/01/08/NLP%E3%80%91Day-21-%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8-Python-%E6%90%AD%E5%BB%BA-Tensorflow-%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="米蘭牛角尖">
<meta property="og:description" content="啊！好久沒有實作了，不曉得各位還記不記得怎麼寫 Python 呢？今天的這篇文章將主要以實作為主，簡單地介紹如何搭建出我們在過去幾天的旅程中，學到的那些形形色色的深度學習模型：LSTM、GRU，以及兩者的雙向版本，也就是BiLSTM、以及BiGRU。不過，你可能已經發現，咦？那單純的 RNN 呢？事實上，在自然語言處理的實務中，由於先前提過的那些缺失，如記性不好等問題，已經很少人使用單純的 RNN">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://i.imgur.com/x3YOaph.jpg">
<meta property="og:image" content="https://i.redd.it/1qskl89lxzs31.png">
<meta property="article:published_time" content="2023-01-08T11:56:00.000Z">
<meta property="article:modified_time" content="2023-01-08T11:56:50.014Z">
<meta property="article:author" content="Hao Yun Chuang (Milan)">
<meta property="article:tag" content="自然語言處理">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/x3YOaph.jpg">


<link rel="canonical" href="https://milanochuang.github.io/2023/01/08/NLP%E3%80%91Day-21-%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8-Python-%E6%90%AD%E5%BB%BA-Tensorflow-%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-TW","comments":true,"permalink":"https://milanochuang.github.io/2023/01/08/NLP%E3%80%91Day-21-%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8-Python-%E6%90%AD%E5%BB%BA-Tensorflow-%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B/","path":"2023/01/08/NLP】Day-21-手把手教你如何利用-Python-搭建-Tensorflow-的深度學習模型/","title":"NLP】Day 21: 手把手教你如何利用 Python 搭建 Tensorflow 的深度學習模型 "}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>NLP】Day 21: 手把手教你如何利用 Python 搭建 Tensorflow 的深度學習模型  | 米蘭牛角尖</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">米蘭牛角尖</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">計算語言學的學習紀錄，偶爾可能會出現野生的電影評論？</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首頁</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>關於</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>標籤</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分類</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>所有文章</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8A%E5%A4%A9%E8%A6%81%E4%BD%BF%E7%94%A8%E7%9A%84%E5%B7%A5%E5%85%B7%E7%AE%B1"><span class="nav-number">1.</span> <span class="nav-text">今天要使用的工具箱</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensorflow"><span class="nav-number">1.1.</span> <span class="nav-text">Tensorflow</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM"><span class="nav-number">2.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BiLSTM"><span class="nav-number">3.</span> <span class="nav-text">BiLSTM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GRU"><span class="nav-number">4.</span> <span class="nav-text">GRU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B5%90%E8%AA%9E"><span class="nav-number">5.</span> <span class="nav-text">結語</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hao Yun Chuang (Milan)"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Hao Yun Chuang (Milan)</p>
  <div class="site-description" itemprop="description">目前就讀於政大語言所，研究興趣為計算語言學<br/></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分類</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/milanochuang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;milanochuang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:milanochuang@gmail.com" title="E-Mail → mailto:milanochuang@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/milanochuang" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;milanochuang" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/milanochuang" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;milanochuang" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/milanochuang" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;milanochuang" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i></a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="回到頂端">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/milanochuang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://milanochuang.github.io/2023/01/08/NLP%E3%80%91Day-21-%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8-Python-%E6%90%AD%E5%BB%BA-Tensorflow-%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Hao Yun Chuang (Milan)">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="米蘭牛角尖">
      <meta itemprop="description" content="目前就讀於政大語言所，研究興趣為計算語言學<br/>">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="NLP】Day 21: 手把手教你如何利用 Python 搭建 Tensorflow 的深度學習模型  | 米蘭牛角尖">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP】Day 21: 手把手教你如何利用 Python 搭建 Tensorflow 的深度學習模型 
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>
      

      <time title="創建時間：2023-01-08 19:56:00 / 修改時間：19:56:50" itemprop="dateCreated datePublished" datetime="2023-01-08T19:56:00+08:00">2023-01-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/natural-language-processing/" itemprop="url" rel="index"><span itemprop="name">自然語言處理</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>啊！好久沒有實作了，不曉得各位還記不記得怎麼寫 Python 呢？今天的這篇文章將主要以實作為主，簡單地介紹如何搭建出我們在過去幾天的旅程中，學到的那些形形色色的深度學習模型：LSTM、GRU，以及兩者的雙向版本，也就是BiLSTM、以及BiGRU。不過，你可能已經發現，咦？那單純的 RNN 呢？事實上，在自然語言處理的實務中，由於先前提過的那些缺失，如記性不好等問題，已經很少人使用單純的 RNN 來完成任務了。欸！別人都是一篇一個神經網路，我直接一篇全部寫完，超值吧！還不趕快再點一下我其他文章！</p>
<p>不過在開始弄髒你的手實際上工之前，我們得先了解今天要用的工具有哪些。</p>
<h2 id="今天要使用的工具箱"><a href="#今天要使用的工具箱" class="headerlink" title="今天要使用的工具箱"></a>今天要使用的工具箱</h2><p>對於一些可能從未寫過程式的朋友，可能會有一些疑慮，擔心說「我們該不會要從零開始搭神經網路吧？先前介紹過的那些輸出層、輸入層、隱藏層殺毀的，我們都要從頭開始做起嗎？別吧？」接著擺出了 No No No 的手勢。</p>
<p><img src="https://i.imgur.com/x3YOaph.jpg"></p>
<p>事實上，像這種這麼常用的深度學習工具，不可能沒有前人寫過。所謂前人種樹，後人乘涼，在過去就已經有程式大神代替我們將這些深度學習以及神經網路整理成 API 了。什麼？你說什麼是API？沒事，你只需要理解因為某邪惡大神的建樹，我們只需要簡單地引入（<code>import</code>）工具箱後，這些神經網路就可以隨取隨用了。我想，最多只需要調參數、以及疊各種隱藏層吧？而我今天就會一步一步地仔細帶領你撰寫搭建模型的程式，並逐句介紹這些程式語言在做什麼。至於是哪個大神如此邪惡，擁有幾乎全世界所有網頁的資料，又有極大量的運算資源？其實也沒有別人了，就是：</p>
<p><img src="https://i.redd.it/1qskl89lxzs31.png"></p>
<h3 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h3><p>Tensorflow 是一個開源的深度學習程式庫，並利用機器學習來完成各種人工智慧的下游任務，例如：圖像辨識，還有我們的重點，自然語言處理。你現在所用的幾乎所有 Google 的服務，舉凡如 Google 搜尋、Google 翻譯（乙定要有的吧！）、GOogle 語音辨識、Google 地圖、Google 相片，都是來自於利用 Tensorflow 的深度學習框架所打造而成的。換句話說，我們其實就是站在 Google 打造好的立足點來解決自然語言處理的應用。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>首先，在程式檔的頂端，我們得先引用搭建神經網路所需要的工具箱，也就是 Tensorflow。今天，我們會直接使用 tensorflow 內建的 imdb 資料集。<del>資料科學的前輩對imdb資料一定又愛又恨吧！</del></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> imdb <span class="comment"># 引入資料集</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential <span class="comment"># 引入序列模型，記得我們在先前提到的 sequence model嗎？</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense <span class="comment"># 一層全連接的神經網路</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> LSTM <span class="comment"># 長短期記憶（Long Short-term memory）</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Embedding <span class="comment"># 使模型理解語意的 Embedding</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing <span class="keyword">import</span> sequence </span><br><span class="line"><span class="comment"># 在前處理會需要用到，將長短不一的資料補齊或截斷成相同長度（記得我們先前所說神經網路的長度可以彈性調整嗎？就是透過這裡將序列轉換成相同長度）</span></span><br><span class="line">tf.random.set_seed(<span class="number">7</span>)  <span class="comment"># 設定隨機種子</span></span><br></pre></td></tr></table></figure>
<p>再來得將資料先處理好，我們在這邊先將資料處理好之後，後續在搭建無論是 GRU，甚至是BiLSTM、BiGRU，都會直接用這裡的資料，就不會再重複這裡的動作啦！</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 設定資料中字頻前五千的字</span></span><br><span class="line">top_words = <span class="number">5000</span> </span><br><span class="line"><span class="comment"># 取得資料後，分成訓練資料集以及測試資料集</span></span><br><span class="line">(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words) </span><br><span class="line"><span class="comment"># 在這裡就是要截斷或補齊輸入模型的句子，以保持所有句子長度相同。</span></span><br><span class="line">max_review_length = <span class="number">500</span></span><br><span class="line">X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)</span><br><span class="line">X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)</span><br></pre></td></tr></table></figure>
<p>接著要先搭建模型，搭建完模型之後才會開始訓練。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">embedding_vector_length = <span class="number">32</span></span><br><span class="line"><span class="comment"># 首先設定模型為序列模型</span></span><br><span class="line">lstm_model = Sequential()</span><br><span class="line"><span class="comment"># 將已經訓練好的 Embedding 加入神經網路模型中</span></span><br><span class="line">lstm_model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))</span><br><span class="line"><span class="comment"># LSTM函數中的值則是代表你要在模型中放入多少 LSTM 的神經元（有門閥的那個）</span></span><br><span class="line">lstm_model.add(LSTM(<span class="number">100</span>))</span><br><span class="line"><span class="comment"># 全連接的神經網路隱藏層，可以選擇想要的激勵函數</span></span><br><span class="line">lstm_model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"><span class="comment"># 這裡的compile是在將模型組合起來。</span></span><br><span class="line">lstm_model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="comment"># 可以一起來看看模型長怎樣</span></span><br><span class="line"><span class="built_in">print</span>(lstm_model.summary())</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Model: &quot;sequential&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line"> Layer (type)                Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line"> embedding (Embedding)       (None, 500, 32)           160000    </span><br><span class="line">                                                                 </span><br><span class="line"> lstm (LSTM)                 (None, 100)               53200     </span><br><span class="line">                                                                 </span><br><span class="line"> dense (Dense)               (None, 1)                 101       </span><br><span class="line">                                                                 </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 213,301</span><br><span class="line">Trainable params: 213,301</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">None</span><br></pre></td></tr></table></figure>
<p>我們從<code>params</code>中可以看到模型總共有多少個參數。另外，在 LSTM 中的參數決定的是神經元數量，沒有一定的固定數字，但是有個模式可循。首先，RNN 的寬度代表特徵的多寡，由輸入以及篩選器的數量決定；深度代表的則是特徵的豐富度，由神經網路層數及步驟數決定的。若不需要從原資料產出大量特徵，那麼一般來說寬度會減少。若資料相對單純，那麼深度則需要調整的淺一點，這麼做可以節省訓練時間及資源。(<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/59072728/what-is-the-rule-to-know-how-many-lstm-cells-and-how-many-units-in-each-lstm-cel">source</a>)</p>
<p>確認沒問題之後，就可以開始訓練模型了！這裡可能會需要跑比較久時間，像我試跑就跑了十分多鐘，所以才說深度學習模型的一個缺點就是速度比較慢一點。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, y_train, epochs=<span class="number">3</span>, batch_size=<span class="number">64</span>) <span class="comment"># 訓練模型</span></span><br><span class="line">scores = model.evaluate(X_test, y_test, verbose=<span class="number">0</span>) <span class="comment"># 測試模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: %.2f%%&quot;</span> % (scores[<span class="number">1</span>]*<span class="number">100</span>)) </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/3</span><br><span class="line">391/391 [==============================] - 207s 525ms/step - loss: 0.4316 - accuracy: 0.7884</span><br><span class="line">Epoch 2/3</span><br><span class="line">391/391 [==============================] - 205s 525ms/step - loss: 0.2653 - accuracy: 0.8937</span><br><span class="line">Epoch 3/3</span><br><span class="line">391/391 [==============================] - 204s 522ms/step - loss: 0.2377 - accuracy: 0.9065</span><br><span class="line">Accuracy: 86.72%</span><br></pre></td></tr></table></figure>
<h2 id="BiLSTM"><a href="#BiLSTM" class="headerlink" title="BiLSTM"></a>BiLSTM</h2><p>其實雙向的 BiLSTM 也是大同小異，我們只需要關注模型的搭建上就可以了。資料一樣沿用在上面所整理好的訓練以及測試資料集。別忘了要先引進雙向的套件，至於其他的套件，在前面就已經引進過，就不需要再引進了。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Bidirectional</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bilstm_model = Sequential()</span><br><span class="line">bilstm_model.add(Embedding(top_words, embedding_vetcor_length, input_length=max_review_length))</span><br><span class="line">bilstm_model.add(Bidirectional(LSTM(<span class="number">100</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>)))</span><br><span class="line">bilstm_model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">bilstm_model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(bilstm_model.summary()) <span class="comment"># 同樣先來看模型參數</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Model: &quot;sequential_1&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line"> Layer (type)                Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line"> embedding_1 (Embedding)     (None, 500, 32)           160000    </span><br><span class="line">                                                                 </span><br><span class="line"> bidirectional (Bidirectiona  (None, 200)              106400    </span><br><span class="line"> l)                                                              </span><br><span class="line">                                                                 </span><br><span class="line"> dense_1 (Dense)             (None, 1)                 201       </span><br><span class="line">                                                                 </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 266,601</span><br><span class="line">Trainable params: 266,601</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">None</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bilstm_model.fit(X_train, y_train, epochs=<span class="number">3</span>, batch_size=<span class="number">64</span>)</span><br><span class="line">scores = bilstm_model.evaluate(X_test, y_test, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: %.2f%%&quot;</span> % (scores[<span class="number">1</span>]*<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/3</span><br><span class="line">391/391 [==============================] - 805s 2s/step - loss: 0.4870 - accuracy: 0.7579</span><br><span class="line">Epoch 2/3</span><br><span class="line">391/391 [==============================] - 803s 2s/step - loss: 0.3148 - accuracy: 0.8726</span><br><span class="line">Epoch 3/3</span><br><span class="line">391/391 [==============================] - 799s 2s/step - loss: 0.2637 - accuracy: 0.8944</span><br><span class="line">Accuracy: 87.89%</span><br></pre></td></tr></table></figure>
<p>BiLSTM 因為參數更多，所以訓練時間又很明顯地變更久了。我在這裡花了快20多分鐘在訓練模型。</p>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">gru_model = Sequential()</span><br><span class="line">gru_model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))</span><br><span class="line">gru_model.add(GRU(<span class="number">32</span>))</span><br><span class="line">gru_model.add(Dense(<span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">gru_model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">gru_model.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(gru_model.summary())</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Model: &quot;sequential_2&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line"> Layer (type)                Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line"> embedding_2 (Embedding)     (None, 500, 32)           160000    </span><br><span class="line">                                                                 </span><br><span class="line"> gru (GRU)                   (None, 32)                6336      </span><br><span class="line">                                                                 </span><br><span class="line"> dense_2 (Dense)             (None, 10)                330       </span><br><span class="line">                                                                 </span><br><span class="line"> dense_3 (Dense)             (None, 1)                 11        </span><br><span class="line">                                                                 </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 166,677</span><br><span class="line">Trainable params: 166,677</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gru_model.fit(X_train, y_train, epochs=<span class="number">3</span>, batch_size=<span class="number">64</span>)</span><br><span class="line">scores = gru_model.evaluate(X_test, y_test, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: %.2f%%&quot;</span> % (scores[<span class="number">1</span>]*<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/3</span><br><span class="line">391/391 [==============================] - 89s 222ms/step - loss: 0.4658 - accuracy: 0.7620</span><br><span class="line">Epoch 2/3</span><br><span class="line">391/391 [==============================] - 88s 226ms/step - loss: 0.2631 - accuracy: 0.8943</span><br><span class="line">Epoch 3/3</span><br><span class="line">391/391 [==============================] - 88s 224ms/step - loss: 0.2215 - accuracy: 0.9149</span><br><span class="line">Accuracy: 87.74%</span><br></pre></td></tr></table></figure>
<p>GRU 模型當初有說，其實設計架構上與 LSTM 差不了多少，但是由於 GRU 的參數相對比較少的緣故，所以運算時間還有資源都比原本的 LSTM 還要少。在準確度皆為接近 90% 的前提下，我在前面訓練 LSTM 的時間為十一分鐘左右，而這裡 GRU 我卻只需要五分多鐘就結束訓練了。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bigru_model = Sequential()</span><br><span class="line">bigru_model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))</span><br><span class="line">bigru_model.add(Bidirectional(GRU(<span class="number">32</span>)))</span><br><span class="line">bigru_model.add(Dense(<span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">bigru_model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">bigru_model.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">bigru_model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Model: &quot;sequential_3&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line"> Layer (type)                Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line"> embedding_3 (Embedding)     (None, 500, 32)           160000    </span><br><span class="line">                                                                 </span><br><span class="line"> bidirectional_1 (Bidirectio  (None, 64)               12672     </span><br><span class="line"> nal)                                                            </span><br><span class="line">                                                                 </span><br><span class="line"> dense_4 (Dense)             (None, 10)                650       </span><br><span class="line">                                                                 </span><br><span class="line"> dense_5 (Dense)             (None, 1)                 11        </span><br><span class="line">                                                                 </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 173,333</span><br><span class="line">Trainable params: 173,333</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bigru_model.fit(X_train, y_train, epochs=<span class="number">3</span>, batch_size=<span class="number">64</span>)</span><br><span class="line">scores = bigru_model.evaluate(X_test, y_test, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: %.2f%%&quot;</span> % (scores[<span class="number">1</span>]*<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">1</span>/<span class="number">3</span></span><br><span class="line"><span class="number">391</span>/<span class="number">391</span> [==============================] - 166s 414ms/step - loss: <span class="number">0.4397</span> - accuracy: <span class="number">0.7790</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">3</span></span><br><span class="line"><span class="number">391</span>/<span class="number">391</span> [==============================] - 160s 408ms/step - loss: <span class="number">0.2599</span> - accuracy: <span class="number">0.8975</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">3</span></span><br><span class="line"><span class="number">391</span>/<span class="number">391</span> [==============================] - 159s 407ms/step - loss: <span class="number">0.2170</span> - accuracy: <span class="number">0.9153</span></span><br><span class="line">Accuracy: <span class="number">87.74</span>%</span><br></pre></td></tr></table></figure>
<p>BiGRU 的模型訓練時間跟 BiLSTM 相比就更明顯了，BiLSTM 我當初花了 40 分鐘左右才訓練完成，反觀 BiGRU，只需要 9 分鐘左右就結束了，模型表現卻也都差不多。</p>
<h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>我們最後來看一下結果，可以發現四種表現都沒有差距太大，但訓練時間就有明顯的差異，也就是 GRU 的時間還要少於 LSTM；另外，雙向的神經網路模型表現也比單向還要好一些。</p>
<table>
<thead>
<tr>
<th></th>
<th>LSTM</th>
<th>BiLSTM</th>
<th>GRU</th>
<th>BiGRU</th>
</tr>
</thead>
<tbody><tr>
<td>準確度 <strong>(%)</strong></td>
<td>86.72</td>
<td>87.89</td>
<td>87.74</td>
<td>87.74</td>
</tr>
<tr>
<td>時間 <strong>(min)</strong></td>
<td>11</td>
<td>19</td>
<td>5.5</td>
<td>9</td>
</tr>
</tbody></table>
<p>這也印證了在先前 <a target="_blank" rel="noopener" href="https://ithelp.ithome.com.tw/articles/10302765">【NLP】Day 17: 每天成為更好的自己！神經網路也是！深度學習模型 GRU</a> 的文章中所說，參數的多寡會大大影響著運算時間以及資源。好，神經網路的實作就到這邊，是不是比想像中的還要簡單很多呢？明天開始就要進入 BERT 了喔！</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86/" rel="tag"># 自然語言處理</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/01/08/%E3%80%90NLP%E3%80%91Day-20-%E6%94%BE%E9%BB%9E%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%9C%A8%E5%A4%9A%E9%A0%AD%E4%B8%8A%EF%BC%88NLP%E4%B9%9F%E6%9C%89%E5%A4%9A%E9%A0%AD%E5%95%8A%EF%BC%81%EF%BC%89%EF%BC%9ATransformer%EF%BC%88%E4%B8%8B%EF%BC%89/" rel="prev" title="【NLP】Day 20: 放點注意力在多頭上（NLP也有多頭啊！）：Transformer（下）">
                  <i class="fa fa-chevron-left"></i> 【NLP】Day 20: 放點注意力在多頭上（NLP也有多頭啊！）：Transformer（下）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/01/08/%E3%80%90NLP%E3%80%91Day-22-%E5%B9%AB%E4%BD%A0%E8%A7%A3%E6%B1%BA%E5%90%84%E9%A1%9ENLP%E4%BB%BB%E5%8B%99%E7%9A%84BERT%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E5%9C%A8%E8%8A%9D%E9%BA%BB%E8%A1%97%E7%9A%84%E5%A5%BD%E6%8D%A7%E6%B2%B9%E5%80%91%EF%BC%88%E4%B8%8A%EF%BC%89/" rel="next" title="【NLP】Day 22: 幫你解決各類NLP任務的BERT，以及其他在芝麻街的好捧油們（上）">
                  【NLP】Day 22: 幫你解決各類NLP任務的BERT，以及其他在芝麻街的好捧油們（上） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa-solid fa-code"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hao Yun Chuang (Milan)</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  





<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"display":{"superSample":2,"width":250,"height":500,"position":"right","hOffset":-30,"vOffset":-130},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
