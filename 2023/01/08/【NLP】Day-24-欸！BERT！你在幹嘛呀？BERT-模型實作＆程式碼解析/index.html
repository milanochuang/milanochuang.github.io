<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/icon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/icon.png">
  <link rel="mask-icon" href="/images/icon.png" color="#222">
  <meta name="google-site-verification" content="DMyRve2tDXDJqmJpSS4BaBWT1R0ZCH9n_EsCnROTiM0">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha256-xejo6yLi6vGtAjcMIsY8BHdKsLg7QynVlFMzdQgUuy8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"milanochuang.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.12.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜尋...","empty":"我們無法找到任何有關 ${query} 的搜索結果","hits_time":"${hits} 找到 ${time} 個結果","hits":"找到 ${hits} 個結果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="別想太多，做就對了！《捍衛戰士：獨行俠》  前兩天我們已經了解 BERT 的內部運作，還有 BERT 在進行語言處理上的一些缺陷。今天不聊理論，我們來簡單一一解析 Tensorflow 的官方教學 沒錯，又是 Tensorflow 了喔，並結合前兩天所學到的知識，來對教學中程式碼進行剖析。由於 BERT 的應用實在太多了，所以今天只會提到最簡單的情緒文本分析，也就是先前所說的多對一文本分類，若各">
<meta property="og:type" content="article">
<meta property="og:title" content="【NLP】Day 24: 欸！BERT！你在幹嘛呀？BERT 模型實作＆程式碼解析">
<meta property="og:url" content="https://milanochuang.github.io/2023/01/08/%E3%80%90NLP%E3%80%91Day-24-%E6%AC%B8%EF%BC%81BERT%EF%BC%81%E4%BD%A0%E5%9C%A8%E5%B9%B9%E5%98%9B%E5%91%80%EF%BC%9FBERT-%E6%A8%A1%E5%9E%8B%E5%AF%A6%E4%BD%9C%EF%BC%86%E7%A8%8B%E5%BC%8F%E7%A2%BC%E8%A7%A3%E6%9E%90/index.html">
<meta property="og:site_name" content="米蘭牛角尖">
<meta property="og:description" content="別想太多，做就對了！《捍衛戰士：獨行俠》  前兩天我們已經了解 BERT 的內部運作，還有 BERT 在進行語言處理上的一些缺陷。今天不聊理論，我們來簡單一一解析 Tensorflow 的官方教學 沒錯，又是 Tensorflow 了喔，並結合前兩天所學到的知識，來對教學中程式碼進行剖析。由於 BERT 的應用實在太多了，所以今天只會提到最簡單的情緒文本分析，也就是先前所說的多對一文本分類，若各">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://www.tensorflow.org/static/text/tutorials/classify_text_with_bert_files/output_0EmzyHZXKIpm_0.png">
<meta property="article:published_time" content="2023-01-08T12:00:05.000Z">
<meta property="article:modified_time" content="2023-01-08T12:00:21.846Z">
<meta property="article:author" content="Hao Yun Chuang (Milan)">
<meta property="article:tag" content="自然語言處理, 計算語言學, 機器學習, 人工智慧, 電影">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.tensorflow.org/static/text/tutorials/classify_text_with_bert_files/output_0EmzyHZXKIpm_0.png">


<link rel="canonical" href="https://milanochuang.github.io/2023/01/08/%E3%80%90NLP%E3%80%91Day-24-%E6%AC%B8%EF%BC%81BERT%EF%BC%81%E4%BD%A0%E5%9C%A8%E5%B9%B9%E5%98%9B%E5%91%80%EF%BC%9FBERT-%E6%A8%A1%E5%9E%8B%E5%AF%A6%E4%BD%9C%EF%BC%86%E7%A8%8B%E5%BC%8F%E7%A2%BC%E8%A7%A3%E6%9E%90/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-TW","comments":true,"permalink":"https://milanochuang.github.io/2023/01/08/%E3%80%90NLP%E3%80%91Day-24-%E6%AC%B8%EF%BC%81BERT%EF%BC%81%E4%BD%A0%E5%9C%A8%E5%B9%B9%E5%98%9B%E5%91%80%EF%BC%9FBERT-%E6%A8%A1%E5%9E%8B%E5%AF%A6%E4%BD%9C%EF%BC%86%E7%A8%8B%E5%BC%8F%E7%A2%BC%E8%A7%A3%E6%9E%90/","path":"2023/01/08/【NLP】Day-24-欸！BERT！你在幹嘛呀？BERT-模型實作＆程式碼解析/","title":"【NLP】Day 24: 欸！BERT！你在幹嘛呀？BERT 模型實作＆程式碼解析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>【NLP】Day 24: 欸！BERT！你在幹嘛呀？BERT 模型實作＆程式碼解析 | 米蘭牛角尖</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">米蘭牛角尖</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">計算語言學的學習紀錄，偶爾可能會出現野生的電影評論？</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首頁</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>關於</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>標籤</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分類</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>所有文章</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%8F%E9%81%8E-BERT-%E9%80%B2%E8%A1%8C%E6%96%87%E6%9C%AC%E5%88%86%E9%A1%9E"><span class="nav-number">1.</span> <span class="nav-text">透過 BERT 進行文本分類</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%92%B0%E5%A2%83%E5%BB%BA%E7%BD%AE"><span class="nav-number">1.1.</span> <span class="nav-text">環境建置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B5%90%E8%AA%9E"><span class="nav-number">2.</span> <span class="nav-text">結語</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hao Yun Chuang (Milan)"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Hao Yun Chuang (Milan)</p>
  <div class="site-description" itemprop="description">目前就讀於政大語言所，研究興趣為計算語言學<br/></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分類</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/milanochuang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;milanochuang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:milanochuang@gmail.com" title="E-Mail → mailto:milanochuang@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/milanochuang" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;milanochuang" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/milanochuang" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;milanochuang" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/milanochuang" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;milanochuang" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i></a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="回到頂端">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/milanochuang" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://milanochuang.github.io/2023/01/08/%E3%80%90NLP%E3%80%91Day-24-%E6%AC%B8%EF%BC%81BERT%EF%BC%81%E4%BD%A0%E5%9C%A8%E5%B9%B9%E5%98%9B%E5%91%80%EF%BC%9FBERT-%E6%A8%A1%E5%9E%8B%E5%AF%A6%E4%BD%9C%EF%BC%86%E7%A8%8B%E5%BC%8F%E7%A2%BC%E8%A7%A3%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Hao Yun Chuang (Milan)">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="米蘭牛角尖">
      <meta itemprop="description" content="目前就讀於政大語言所，研究興趣為計算語言學<br/>">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="【NLP】Day 24: 欸！BERT！你在幹嘛呀？BERT 模型實作＆程式碼解析 | 米蘭牛角尖">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【NLP】Day 24: 欸！BERT！你在幹嘛呀？BERT 模型實作＆程式碼解析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>
      

      <time title="創建時間：2023-01-08 20:00:05 / 修改時間：20:00:21" itemprop="dateCreated datePublished" datetime="2023-01-08T20:00:05+08:00">2023-01-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <blockquote>
<p>別想太多，做就對了！<br><strong>《捍衛戰士：獨行俠》</strong></p>
</blockquote>
<p>前兩天我們已經了解 BERT 的內部運作，還有 BERT 在進行語言處理上的一些缺陷。今天不聊理論，我們來簡單一一解析 Tensorflow 的官方教學 <del>沒錯，又是 Tensorflow 了喔</del>，並結合前兩天所學到的知識，來對教學中程式碼進行剖析。由於 BERT 的應用實在太多了，所以今天只會提到最簡單的情緒文本分析，也就是先前所說的多對一文本分類，若各位有興趣，或許我們明年還可以相見？在下面留言：）</p>
<p>今天的程式碼將會全部取自於 <a target="_blank" rel="noopener" href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert">Tensorflow 官方教學文件</a>。</p>
<h2 id="透過-BERT-進行文本分類"><a href="#透過-BERT-進行文本分類" class="headerlink" title="透過 BERT 進行文本分類"></a>透過 BERT 進行文本分類</h2><p>在開始進入正題之前，我們得先在 <a target="_blank" rel="noopener" href="https://colab.research.google.com/">Colab</a> 中建置環境。Colab 是線上的編譯工具，也是機器學習常用的程式平台，更是初學者剛學習 Python 的最佳途徑。之所以 ML 常用，是因為在上面可以調用 Google 分配的運算資源，讓運算速度可以比較快一點。在開始之前，讓我們先來建置一下環境。</p>
<h3 id="環境建置"><a href="#環境建置" class="headerlink" title="環境建置"></a>環境建置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># A dependency of the preprocessing for BERT inputs</span><br><span class="line">pip install -q -U &quot;tensorflow-text==2.8.*&quot;</span><br><span class="line">pip install -q tf-models-official==2.7.0</span><br></pre></td></tr></table></figure>

<p>搭建 BERT 通常會需要兩個步驟：資料前處理、模型建置。在這裡的資料前處理跟先前我們所學利用正規表達式來清理資料的那種前處理有一點差別。還記得我們先前學習正規表達式的時候，是為了要將文本中不需要的詞以及符號等等的刪除，只留下我們所需要的資料即可；但在搭建 BERT 時，所謂的前處理就不再是指清理資料了，而是<strong>將資料轉換成 BERT 可以理解的格式</strong>。</p>
<p>接著我們在這邊將需要的套件引入程式中。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil </span><br><span class="line"><span class="comment"># 這兩個套件是為了要將訓練資料的路徑讀取工具也引進來</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># 就是 Tensorflow 不用多說</span></span><br><span class="line"><span class="keyword">import</span> tensorflow_hub <span class="keyword">as</span> hub <span class="comment"># Tensorflow bert 社群</span></span><br><span class="line"><span class="keyword">import</span> tensorflow_text <span class="keyword">as</span> text <span class="comment"># 前處理</span></span><br><span class="line"><span class="keyword">from</span> official.nlp <span class="keyword">import</span> optimization  <span class="comment"># 優化工具</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 畫圖用的套件</span></span><br><span class="line"></span><br><span class="line">tf.get_logger().setLevel(<span class="string">&#x27;ERROR&#x27;</span>) <span class="comment"># Debug的工具，也可以不寫</span></span><br></pre></td></tr></table></figure>

<p>接下來就是要引入訓練的資料。我們在這邊引用網路上常用的 imdb 情感分析資料。在這邊可以看到套件 <code>os</code> 是為了要將路徑整合，使模型可以更方便讀取我們下載的訓練資料。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 這個是資料集的下載連結，如果你複製這段 url 到瀏覽器上，也可以下載的到</span></span><br><span class="line">url = <span class="string">&#x27;https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&#x27;</span></span><br><span class="line"></span><br><span class="line">dataset = tf.keras.utils.get_file(<span class="string">&#x27;aclImdb_v1.tar.gz&#x27;</span>, url,</span><br><span class="line">                                  untar=<span class="literal">True</span>, cache_dir=<span class="string">&#x27;.&#x27;</span>,</span><br><span class="line">                                  cache_subdir=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所謂的 os.path.join 就是把兩個路徑結合起來的意思，如果路徑是 ./data/，join之後就會變成 ./data/aclImdb，而以下所做的，就是要整合資料路徑，方便之後模型讀取</span></span><br><span class="line">dataset_dir = os.path.join(os.path.dirname(dataset), <span class="string">&#x27;aclImdb&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_dir = os.path.join(dataset_dir, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove unused folders to make it easier to load the data</span></span><br><span class="line">remove_dir = os.path.join(train_dir, <span class="string">&#x27;unsup&#x27;</span>)</span><br><span class="line">shutil.rmtree(remove_dir)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</span><br><span class="line">84131840/84125825 [==============================] - 7s 0us/step</span><br><span class="line">84140032/84125825 [==============================] - 7s 0us/step</span><br></pre></td></tr></table></figure>

<p>接下來就是把資料轉換成訓練資料集以及測試資料集，在這裡我們將資料以 8:2 的形式，將資料進行分割。官方文件是透過 keras 的 <code>text_dataset_from_directory</code> 函式中的一個參數來分割資料。至於所謂的 <code>batch_size</code> 就是每一批放進模型訓練的批量。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">AUTOTUNE = tf.data.AUTOTUNE</span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">seed = <span class="number">42</span></span><br><span class="line"></span><br><span class="line">raw_train_ds = tf.keras.utils.text_dataset_from_directory(</span><br><span class="line">    <span class="string">&#x27;aclImdb/train&#x27;</span>,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    validation_split=<span class="number">0.2</span>,</span><br><span class="line">    subset=<span class="string">&#x27;training&#x27;</span>,</span><br><span class="line">    seed=seed)</span><br><span class="line"></span><br><span class="line">class_names = raw_train_ds.class_names</span><br><span class="line">train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><br><span class="line"></span><br><span class="line">val_ds = tf.keras.utils.text_dataset_from_directory(</span><br><span class="line">    <span class="string">&#x27;aclImdb/train&#x27;</span>,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    validation_split=<span class="number">0.2</span>,</span><br><span class="line">    subset=<span class="string">&#x27;validation&#x27;</span>,</span><br><span class="line">    seed=seed)</span><br><span class="line"></span><br><span class="line">val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><br><span class="line"></span><br><span class="line">test_ds = tf.keras.utils.text_dataset_from_directory(</span><br><span class="line">    <span class="string">&#x27;aclImdb/test&#x27;</span>,</span><br><span class="line">    batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Found 25000 files belonging to 2 classes.</span><br><span class="line">Using 20000 files for training.</span><br><span class="line">Found 25000 files belonging to 2 classes.</span><br><span class="line">Using 5000 files for validation.</span><br><span class="line">Found 25000 files belonging to 2 classes.</span><br></pre></td></tr></table></figure>

<p>最後我們可以得到訓練資料集 <code>train_ds</code> 以及驗證資料集 <code>val_ds</code>，以及最後的測試資料集 <code>test_ds</code>。如果我們看一下裡面的資料長什麼樣子，就會發現<code>0</code>代表負面影評，<code>1</code>代表正面影評。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> text_batch, label_batch <span class="keyword">in</span> train_ds.take(<span class="number">1</span>):</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">f&#x27;Review: <span class="subst">&#123;text_batch.numpy()[<span class="number">0</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">   label = label_batch.numpy()[i]</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">f&#x27;Label : <span class="subst">&#123;label&#125;</span> (<span class="subst">&#123;class_names[label]&#125;</span>)&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>這邊就不印出三份資料了，印出第一份讓大家來有點概念即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Review: b&#x27;&quot;Pandemonium&quot; is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. &quot;Airplane&quot;, &quot;The Naked Gun&quot; trilogy, &quot;Blazing Saddles&quot;, &quot;High Anxiety&quot;, and &quot;Spaceballs&quot; are some of my favorite comedies that spoof a particular genre. &quot;Pandemonium&quot; is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\&#x27;t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\&#x27;s all this film has going for it. Geez, &quot;Scream&quot; had more laughs than this film and that was more of a horror film. How bizarre is that?&lt;br /&gt;&lt;br /&gt;*1/2 (out of four)&#x27;</span><br><span class="line">Label : 0 (neg)</span><br></pre></td></tr></table></figure>

<p>記得我們先前所說的遷移學習（Transfer Learning）嗎？我們可以取用別人已經訓練好的預訓練模型，再針對預訓練模型加上我們自己的資料，讓模型找出特徵之後，來解決自然語言處理的下游任務。那該要到哪裡去下載別人已經預訓練好的模型呢？網路上有兩個常用的平台，一個是我們現在用的 Tensorflow 的 hub ，另一個則是大家更常用的抱臉怪（huggingface），在抱臉怪上有各式各樣別人從各領域的文本所訓練的預訓練模型，而大家可以在上面任意取用符合自己需求的預訓練模型。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/">Huggingface</a></li>
<li><a target="_blank" rel="noopener" href="https://www.tensorflow.org/hub?hl=zh-tw">Tensorflow Hub</a></li>
</ul>
<p>而以下所做的，就是取用別人已經訓練好的模型，並把它用在我們的任務當中。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tfhub_handle_encoder = <span class="string">&quot;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1&quot;</span> <span class="comment"># 這是預訓練模型</span></span><br><span class="line">tfhub_handle_preprocess = <span class="string">&quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;</span> <span class="comment"># 這是 BERT 前處理需要用的模型</span></span><br></pre></td></tr></table></figure>

<p>以下就可以來研究看看輸入進去的資料會有哪些。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) <span class="comment"># 呼叫前處理模型</span></span><br><span class="line">text_test = [<span class="string">&#x27;this is such an amazing movie!&#x27;</span>] <span class="comment"># 先輸入簡單的句子看看會變成什麼樣子</span></span><br><span class="line">text_preprocessed = bert_preprocess_model(text_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Keys       : <span class="subst">&#123;<span class="built_in">list</span>(text_preprocessed.keys())&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Shape      : <span class="subst">&#123;text_preprocessed[<span class="string">&quot;input_word_ids&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Word Ids   : <span class="subst">&#123;text_preprocessed[<span class="string">&quot;input_word_ids&quot;</span>][<span class="number">0</span>, :<span class="number">12</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Input Mask : <span class="subst">&#123;text_preprocessed[<span class="string">&quot;input_mask&quot;</span>][<span class="number">0</span>, :<span class="number">12</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Type Ids   : <span class="subst">&#123;text_preprocessed[<span class="string">&quot;input_type_ids&quot;</span>][<span class="number">0</span>, :<span class="number">12</span>]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Keys       : [&#x27;input_type_ids&#x27;, &#x27;input_word_ids&#x27;, &#x27;input_mask&#x27;]</span><br><span class="line">Shape      : (1, 128)</span><br><span class="line">Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]</span><br><span class="line">Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]</span><br><span class="line">Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]</span><br></pre></td></tr></table></figure>

<p>我們可以看到輸出總共分成 <code>input_word_ids</code>、<code>input_mask</code>、以及 <code>input_type_ids</code>。其中<code>input_word_ids</code>就是每個文字分配的識別碼，可以讓模型分別對應回去的數字。 <code>input_mask</code> 則是在進行 Masked 的程序（我的理解，若有錯還請不吝賜教），而<code>input_type_ids</code>則是會依照不同句子給予不同的id，也就是識別句子的功能。這邊是因為測試的句子只有一句，所以只會有 0 這個數字。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bert_model = hub.KerasLayer(tfhub_handle_encoder)</span><br><span class="line">bert_results = bert_model(text_preprocessed)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Loaded BERT: <span class="subst">&#123;tfhub_handle_encoder&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Pooled Outputs Shape:<span class="subst">&#123;bert_results[<span class="string">&quot;pooled_output&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Pooled Outputs Values:<span class="subst">&#123;bert_results[<span class="string">&quot;pooled_output&quot;</span>][<span class="number">0</span>, :<span class="number">12</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Sequence Outputs Shape:<span class="subst">&#123;bert_results[<span class="string">&quot;sequence_output&quot;</span>].shape&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Sequence Outputs Values:<span class="subst">&#123;bert_results[<span class="string">&quot;sequence_output&quot;</span>][<span class="number">0</span>, :<span class="number">12</span>]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1</span><br><span class="line">Pooled Outputs Shape:(1, 512)</span><br><span class="line">Pooled Outputs Values:[ 0.76262873  0.99280983 -0.1861186   0.36673835  0.15233682  0.65504444</span><br><span class="line">  0.9681154  -0.9486272   0.00216158 -0.9877732   0.0684272  -0.9763061 ]</span><br><span class="line">Sequence Outputs Shape:(1, 128, 512)</span><br><span class="line">Sequence Outputs Values:[[-0.28946388  0.3432126   0.33231565 ...  0.21300787  0.7102078</span><br><span class="line">  -0.05771166]</span><br><span class="line"> [-0.28742015  0.31981024 -0.2301858  ...  0.58455074 -0.21329722</span><br><span class="line">   0.7269209 ]</span><br><span class="line"> [-0.66157013  0.6887685  -0.87432927 ...  0.10877253 -0.26173282</span><br><span class="line">   0.47855264]</span><br><span class="line"> ...</span><br><span class="line"> [-0.2256118  -0.28925604 -0.07064401 ...  0.4756601   0.8327715</span><br><span class="line">   0.40025353]</span><br><span class="line"> [-0.29824278 -0.27473143 -0.05450511 ...  0.48849759  1.0955356</span><br><span class="line">   0.18163344]</span><br><span class="line"> [-0.44378197  0.00930723  0.07223766 ...  0.1729009   1.1833246</span><br><span class="line">   0.07897988]]</span><br></pre></td></tr></table></figure>

<p>在輸出中，總共有三個不同需要注意的欄位：<code>pooled_output</code>、<code>sequence_output</code>、<code>encoder_outputs</code>。</p>
<ul>
<li><code>pooled_output</code>：代表所有資料經由 BERT 之後所取出的 embedding。在這裡代表的就是電影評論資料整體的 embedding。</li>
<li><code>sequence_output</code>：代表的是每一個 token 在上下文中的 embedding。</li>
</ul>
<p>透過 BERT 取得了需要的 embedding 之後，接下來就是要進行下游任務，在這裡就是進行文本分類。</p>
<p>Tutorial 在這裡用一個函式來定義模型搭建：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_classifier_model</span>():</span><br><span class="line">  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=<span class="string">&#x27;text&#x27;</span>)</span><br><span class="line">  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=<span class="string">&#x27;preprocessing&#x27;</span>)</span><br><span class="line">  encoder_inputs = preprocessing_layer(text_input)</span><br><span class="line">  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=<span class="literal">True</span>, name=<span class="string">&#x27;BERT_encoder&#x27;</span>)</span><br><span class="line">  outputs = encoder(encoder_inputs)</span><br><span class="line">  net = outputs[<span class="string">&#x27;pooled_output&#x27;</span>]</span><br><span class="line">  net = tf.keras.layers.Dropout(<span class="number">0.1</span>)(net)</span><br><span class="line">  net = tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="literal">None</span>, name=<span class="string">&#x27;classifier&#x27;</span>)(net)</span><br><span class="line">  <span class="keyword">return</span> tf.keras.Model(text_input, net)</span><br></pre></td></tr></table></figure>

<p>模型架構如下：<br><img src="https://www.tensorflow.org/static/text/tutorials/classify_text_with_bert_files/output_0EmzyHZXKIpm_0.png"><br>Source: <a target="_blank" rel="noopener" href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert">Tensorflow</a></p>
<p>接下來就是要用 Cross entropy 來計算損失函數，前面沒有介紹到損失函數。簡單的說，損失函數就是在進行梯度下降時，模型在評估與正確答案的相近程度時所計算的最小化最佳解問題，即為負對數似然（negative log-likelihood）。</p>
<p>接著設定後面的模型參數。Learning rate 則採取原論文建議的參數：3e-5。原論文建議三種參數，分別為5e-5、3e-5、2e-5。至於 epoch、batch、以及 learning rate，大家可以來看這位前輩寫的<a target="_blank" rel="noopener" href="https://medium.com/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7-%E5%80%92%E5%BA%95%E6%9C%89%E5%A4%9A%E6%99%BA%E6%85%A7/epoch-batch-size-iteration-learning-rate-b62bf6334c49">文章</a>。最後在這裡將優化參數加進 <code>create_optimizer</code> 函式中，以利後續在編譯時對模型進行優化。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">metrics = tf.metrics.BinaryAccuracy()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()</span><br><span class="line">num_train_steps = steps_per_epoch * epochs</span><br><span class="line">num_warmup_steps = <span class="built_in">int</span>(<span class="number">0.1</span>*num_train_steps)</span><br><span class="line"></span><br><span class="line">init_lr = <span class="number">3e-5</span></span><br><span class="line">optimizer = optimization.create_optimizer(init_lr=init_lr,</span><br><span class="line">                                          num_train_steps=num_train_steps,</span><br><span class="line">                                          num_warmup_steps=num_warmup_steps,</span><br><span class="line">                                          optimizer_type=<span class="string">&#x27;adamw&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>最後就是開始訓練：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">classifier_model.<span class="built_in">compile</span>(optimizer=optimizer,</span><br><span class="line">                         loss=loss,</span><br><span class="line">                         metrics=metrics)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Training model with <span class="subst">&#123;tfhub_handle_encoder&#125;</span>&#x27;</span>)</span><br><span class="line">history = classifier_model.fit(x=train_ds,</span><br><span class="line">                               validation_data=val_ds,</span><br><span class="line">                               epochs=epochs)                         </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1</span><br><span class="line">Epoch 1/5</span><br><span class="line">625/625 [==============================] - 90s 136ms/step - loss: 0.4784 - binary_accuracy: 0.7528 - val_loss: 0.3713 - val_binary_accuracy: 0.8350</span><br><span class="line">Epoch 2/5</span><br><span class="line">625/625 [==============================] - 83s 133ms/step - loss: 0.3295 - binary_accuracy: 0.8525 - val_loss: 0.3675 - val_binary_accuracy: 0.8472</span><br><span class="line">Epoch 3/5</span><br><span class="line">625/625 [==============================] - 83s 133ms/step - loss: 0.2503 - binary_accuracy: 0.8963 - val_loss: 0.3905 - val_binary_accuracy: 0.8470</span><br><span class="line">Epoch 4/5</span><br><span class="line">625/625 [==============================] - 83s 133ms/step - loss: 0.1930 - binary_accuracy: 0.9240 - val_loss: 0.4566 - val_binary_accuracy: 0.8506</span><br><span class="line">Epoch 5/5</span><br><span class="line">625/625 [==============================] - 83s 133ms/step - loss: 0.1526 - binary_accuracy: 0.9429 - val_loss: 0.4813 - val_binary_accuracy: 0.8468</span><br></pre></td></tr></table></figure>

<p>接著就可以來看模型表現如何了！</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss, accuracy = classifier_model.evaluate(test_ds)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Loss: <span class="subst">&#123;loss&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">782/782 [==============================] - 59s 75ms/step - loss: 0.4568 - binary_accuracy: 0.8557</span><br><span class="line">Loss: 0.45678260922431946</span><br><span class="line">Accuracy: 0.855679988861084</span><br></pre></td></tr></table></figure>

<p>我們在這裡可以得知模型的準確率約為 0.86。</p>
<h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>其實若你實作過一遍，就會知道 BERT 的訓練時間，相較於前面所介紹過的任何一種模型，像是 LSTM、RNN等這些深度學習模型來說，體感上有非常明顯的差距。BERT 光是這麼一點點資料，就可能會需要訓練到快一個小時，資料一多，甚至可能要跑到一週兩週都是家常便飯，所以這也是 BERT 一個最大的缺點之一。</p>
<p>另外，我國的中研院所開發的 CKIP 繁體中文預訓練模型，也發佈在 Huggingface 上了，可以<a target="_blank" rel="noopener" href="https://huggingface.co/ckiplab">點我</a> 進去看看，CKIP透過 BERT 完成了命名實體辨識、詞性標註等任務，我們也可以基於模型，再訓練符合下游任務的模型。除此之外，Huggingface 上面有很多好玩的<a target="_blank" rel="noopener" href="https://huggingface.co/spaces">模型實作</a>，大家有興趣也可以去玩玩看，比如說像是文本生成、QA等等的 demo，或許大家可以對自然語言處理有更多的想像空間，這樣的社群需要你我一起來建構與發揮。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/01/08/%E3%80%90NLP%E3%80%91Day-23-%E5%B9%AB%E4%BD%A0%E8%A7%A3%E6%B1%BA%E5%90%84%E9%A1%9ENLP%E4%BB%BB%E5%8B%99%E7%9A%84BERT%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E5%9C%A8%E8%8A%9D%E9%BA%BB%E8%A1%97%E7%9A%84%E5%A5%BD%E6%8D%A7%E6%B2%B9%E5%80%91%EF%BC%88%E4%B8%8B%EF%BC%89/" rel="prev" title="【NLP】Day 23: 幫你解決各類NLP任務的BERT，以及其他在芝麻街的好捧油們（下）">
                  <i class="fa fa-chevron-left"></i> 【NLP】Day 23: 幫你解決各類NLP任務的BERT，以及其他在芝麻街的好捧油們（下）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/01/08/%E3%80%90NLP%E3%80%91Day-25-%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E7%9A%84%E5%8F%A6%E5%A4%96%E4%B8%80%E7%A8%AE%E6%83%B3%E5%83%8F%EF%BC%81Articut%E3%80%81Loki-%E4%BB%A5%E5%8F%8A%E4%BB%96%E7%9A%84%E5%A5%BD%E5%8A%A9%E6%89%8B%E5%80%91%EF%BC%81%EF%BC%88%E4%B8%8A%EF%BC%89/" rel="next" title="【NLP】Day 25: 自然語言處理的另外一種想像！Articut、Loki 以及他的好助手們！（上）">
                  【NLP】Day 25: 自然語言處理的另外一種想像！Articut、Loki 以及他的好助手們！（上） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa-solid fa-code"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hao Yun Chuang (Milan)</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  





<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"display":{"superSample":2,"width":250,"height":500,"position":"right","hOffset":-30,"vOffset":-130},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
